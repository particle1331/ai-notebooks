{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f6b29b-9794-4d11-8567-1a649baa1977",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a4741-de9d-445e-8502-df6ebcfa69e8",
   "metadata": {},
   "source": [
    "**Remark.** Let $\\mathbf{Z}_{i+1} = f(\\mathbf{Z}_i; \\mathbf{W}_i)$ be a custom layer. From the equations, it sufficies to *specify* (e.g. by manual calculation) local gradients $\\frac{\\partial{f}}{\\partial{\\mathbf{Z}_{i}}}$ and $\\frac{\\partial{f}}{\\partial{\\mathbf{W}_{i}}}.$ This locality allows neural network layers to be moduler, i.e. composed arbitrarily, and new layers (or generally, tensor operations) to be easily integrated into the library. Note that if the custom layer can be expressed in terms of existing ops and layers, then this is not strictly necessary, although it may be desirable for efficiency reasons.\n",
    "\n",
    "The form `out = f(in, params)` is general\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-notebooks",
   "language": "python",
   "name": "ai-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
