<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Softmax Regression – particle1331/ai-notebooks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../topics/deep/02.html" rel="next">
<link href="../../topics/deep/index.html" rel="prev">
<link href="../../assets/robot.jpg" rel="icon" type="image/jpeg">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2cd88b842f3556f70e7132508a5c4eca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2cd88b842f3556f70e7132508a5c4eca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2cd88b842f3556f70e7132508a5c4eca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ba00ac99bbea9e7b2b28265043683411.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-f79dbffa86626eb872d8c5a00245cbfc.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ba00ac99bbea9e7b2b28265043683411.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../assets/styles.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">particle1331/ai-notebooks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">README</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-topics">    
        <li>
    <a class="dropdown-item" href="../../topics/deep/index.html">
 <span class="dropdown-text">Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../topics/tooling/index.html">
 <span class="dropdown-text">Tooling</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/particle1331/ai-notebooks" title="GitHub" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../topics/deep/index.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="../../topics/deep/01.html">Softmax Regression</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../topics/deep/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../topics/deep/01.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Softmax Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../topics/deep/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../topics/deep/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Automatic Differentiation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../topics/deep/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gradient-Based Optimization</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#ml-as-data-driven-programming" id="toc-ml-as-data-driven-programming" class="nav-link active" data-scroll-target="#ml-as-data-driven-programming">ML as data-driven programming</a></li>
  <li><a href="#three-elements-of-a-learning-algorithm" id="toc-three-elements-of-a-learning-algorithm" class="nav-link" data-scroll-target="#three-elements-of-a-learning-algorithm">Three elements of a learning algorithm</a></li>
  <li><a href="#multi-class-classification" id="toc-multi-class-classification" class="nav-link" data-scroll-target="#multi-class-classification">Multi-class classification</a></li>
  <li><a href="#linear-hypothesis-class" id="toc-linear-hypothesis-class" class="nav-link" data-scroll-target="#linear-hypothesis-class">Linear hypothesis class</a></li>
  <li><a href="#softmax-and-cross-entropy" id="toc-softmax-and-cross-entropy" class="nav-link" data-scroll-target="#softmax-and-cross-entropy">Softmax and cross-entropy</a></li>
  <li><a href="#optimization-problem" id="toc-optimization-problem" class="nav-link" data-scroll-target="#optimization-problem">Optimization problem</a>
  <ul class="collapse">
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a></li>
  <li><a href="#stochastic-gradient-descent-sgd" id="toc-stochastic-gradient-descent-sgd" class="nav-link" data-scroll-target="#stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</a></li>
  <li><a href="#gradient-of-cross-entropy" id="toc-gradient-of-cross-entropy" class="nav-link" data-scroll-target="#gradient-of-cross-entropy">Gradient of cross-entropy</a></li>
  </ul></li>
  <li><a href="#code-implementation" id="toc-code-implementation" class="nav-link" data-scroll-target="#code-implementation">Code implementation</a></li>
  <li><a href="#appendix-model-complexity" id="toc-appendix-model-complexity" class="nav-link" data-scroll-target="#appendix-model-complexity">Appendix: Model complexity</a></li>
  <li><a href="#appendix-maximum-likelihood-estimation-mle" id="toc-appendix-maximum-likelihood-estimation-mle" class="nav-link" data-scroll-target="#appendix-maximum-likelihood-estimation-mle">Appendix: Maximum likelihood estimation (MLE)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../topics/deep/index.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="../../topics/deep/01.html">Softmax Regression</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Softmax Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Suppose you want to write a program that will classify handwritten drawing of digits into their appropriate category: <strong>0</strong>, <strong>1</strong>, <strong>2</strong>, …, <strong>9</strong>. You could, think hard about the nature of digits, try to determine the logic of what indicates what kind of digit, and write a program to codify this logic. Or you could take advantage of the <strong>statistics</strong> of the data, e.g.&nbsp;pixel intensity in a 28 x 28 grid, as discriminative features of each instance. For this task we will use the MNIST dataset:</p>
<p><img src="img/02-1.png" width="100%" title="Title: An elephant"></p>
<section id="ml-as-data-driven-programming" class="level2">
<h2 class="anchored" data-anchor-id="ml-as-data-driven-programming">ML as data-driven programming</h2>
<p><strong>Machine learning approach.</strong> Collect a training set of images with known labels and feed these into a machine learning algorithm, which, if done well, will automatically produce a “program” that solves this task. The said program will consist of a large number of <a href="https://en.wikipedia.org/wiki/Magic_number_(programming)">magic numbers</a>, but it nonetheless performs a sequence of computations to determine the output class:</p>
<center>
<img src="img/02-2.png" width="100%">
</center>
<p>^The latter input is different from training data. In practice, we want the models to perform well on unlabeled data.</p>
</section>
<section id="three-elements-of-a-learning-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="three-elements-of-a-learning-algorithm">Three elements of a learning algorithm</h2>
<p>Every machine learning algorithm has the ff.&nbsp;elements:</p>
<table class="caption-top table">
<caption>Elements of a learning algorithm</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 20%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Element</th>
<th style="text-align: left;">Object</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Hypothesis class</td>
<td style="text-align: left;"><span class="math inline">\mathcal{H}</span></td>
<td style="text-align: left;">This defines the “program structure”. In deep learning, the hypothesis class is parameterized via a set of parameters <span class="math inline">\Theta.</span> The parameters describe how we map <strong>inputs</strong> (images of digits) to <strong>outputs</strong> (labels, or probabilities for each class). Formally, <span class="math inline">\mathcal{H} = \{h_\Theta \mid \Theta \in \mathbb{R}^d \}</span> where <span class="math inline">h_\Theta(\mathbf{x}) = \hat{y}</span> or <span class="math inline">\hat{\mathbf{p}}</span> for an input <span class="math inline">\mathbf{x}.</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Loss function</td>
<td style="text-align: left;"><span class="math inline">\mathcal{L}, \ell</span></td>
<td style="text-align: left;">A function that specifies how “well” a given <strong>hypothesis</strong> <span class="math inline">h_\Theta</span> (i.e.&nbsp;a choice of parameters) performs on the task of interest. The loss is expressed in terms of a pointwise <strong>loss function</strong> <span class="math inline">\ell</span> such that: <span class="math display">\mathcal{L}_{\mathcal{D}}(\Theta) = \frac{1}{N}\sum_{i=1}^N \ell(h_\Theta(\mathbf{x}_i), y_i)</span> where <span class="math inline">\ell \geq 0</span> and <span class="math inline">\ell \to 0</span> whenever the predictions are accurate, and <span class="math inline">\ell \to \infty</span> as the predictions become increasingly worse.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Optimizer</td>
<td style="text-align: left;">e.g.&nbsp;<br> <a href="https://docs.pytorch.org/docs/stable/optim.html#how-to-use-an-optimizer"><code>torch.optim.*</code></a>, <br> <a href="https://docs.pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"><code>LRScheduler</code></a></td>
<td style="text-align: left;">Procedure for determining a set of parameters that (approximately) minimizes the training loss. More precisely, the optimizer handles state and the calculation required to find optimal parameters <span class="math display">\Theta^* \approx {\text{argmin}}_{\Theta} \; \mathcal{L}_\mathcal{D}(\Theta).</span> For deep learning, this is typically an iterative approach via SGD and its variants.</td>
</tr>
</tbody>
</table>
</section>
<section id="multi-class-classification" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification">Multi-class classification</h2>
<p>For multi-class classification, the <strong>training dataset</strong> <span class="math inline">\mathcal{D}</span> consists of input-output pairs <span class="math inline">\mathcal{D} = (\mathbf{x}_i, y_i)_{i=1}^N</span> where <span class="math inline">\mathbf{x}_i \in \mathbb{R}^d</span> with <span class="math inline">d</span> is the <strong>input dimensionality</strong> and <span class="math inline">y_i \in [1, K] \subset \mathbb{Z}</span> where <span class="math inline">K</span> is the <strong>number of classes</strong>. Here <span class="math inline">N = | \mathcal{D} |</span> is the size of the dataset. In this setting, a <strong>hypothesis function</strong> <span class="math inline">h</span> maps input vectors <span class="math inline">\mathbf{x}</span> to <span class="math inline">K</span>-dimensional vectors: <span class="math inline">h \colon \; \mathbb{R}^d \to \mathbb{R}^K.</span> The output <span class="math inline">h_j(\mathbf{x})</span> indicates some measure of “belief” in how much likely the label is to be class <span class="math inline">j</span>. That is, the most likely class for an input <span class="math inline">\mathbf{x}</span> is predicted as the coordinate <span class="math inline">\hat{j} = \text{argmax}_j \; h_j(\mathbf{x})</span>.</p>
<p><strong>Example.</strong> For MNIST, <span class="math inline">d = 28 \times 28 = 784</span>, <span class="math inline">K = 10</span> and <span class="math inline">M = 60,000.</span></p>
</section>
<section id="linear-hypothesis-class" class="level2">
<h2 class="anchored" data-anchor-id="linear-hypothesis-class">Linear hypothesis class</h2>
<p>A <strong>linear hypothesis function</strong> has matrix multiplication as core operation: <span class="math inline">h_\Theta(\mathbf{x}) = \Theta^\top \mathbf{x}</span> for parameters <span class="math inline">\Theta \in \mathbb{R}^{d \times K}.</span> In practice, we usually write this using matrix-batch notation since we process inputs in parallel as a matrix:</p>
<p><span class="math display">
h_\Theta(\mathbf{X}) =  \underbrace{\mathbf{X}}_{\mathbb{R}^{M \times d}} \; \underbrace{\Theta}_{\mathbb{R}^{d \times K}} \in \mathbb{R}^{M \times K}
</span></p>
<p>where the inputs are laid out as row vectors inside the matrix:</p>
<p><span class="math display">
\begin{equation}
\mathbf{X}=\left[\begin{array}{c}
-\, \mathbf{x}^{(1)\top}- \\
\vdots \\
-\, \mathbf{x}^{(M)\top}-
\end{array}\right] \in \mathbb{R}^{M \times d}
\end{equation}.
</span></p>
<p><strong>Remark.</strong> Geometrically each <span class="math inline">\Theta_j = \Theta_{[:, j]} \in \mathbb{R}^d</span> defines a separating hyperplane for class <span class="math inline">j \in [K].</span> So a linear hypothesis class is able to learn to separate linearly separable data points in <span class="math inline">\mathbb{R}^d</span> using <span class="math inline">K</span> separating hyperplanes by assigning a score <span class="math inline">s_j = \Theta_j^\top \mathbf{x} \in \mathbb{R}</span> proportional to its weighted distance from the hyperplane, and the “weight” of that hyperplane <span class="math inline">\lVert\Theta_j\rVert.</span></p>
</section>
<section id="softmax-and-cross-entropy" class="level2">
<h2 class="anchored" data-anchor-id="softmax-and-cross-entropy">Softmax and cross-entropy</h2>
<p>The simplest loss is just the <strong>zero-one loss</strong>: <span class="math inline">\ell = 0</span> if <span class="math inline">\operatorname{argmax}_i h_i(\mathbf{x}) = y</span>, otherwise <span class="math inline">\ell = 1.</span> This is just the classification error. Unfortunately, the error is unsuitable for optimization, simply because it is not differentiable. That is, we can smoothly adjust the parameters without seeing a change in <span class="math inline">\ell</span>, or it transitions abruptly from <span class="math inline">0</span> to <span class="math inline">1.</span></p>
<p>Instead, we look at the probabilities assigned by the model to each class. To do this, we have to convert the class scores to probabilities exponentiating and normalizing its entries (i.e.&nbsp;making <span class="math inline">\sum_j p = 1</span> s.t. <span class="math inline">p_j \geq 0</span>). Class scores <span class="math inline">h_j(\mathbf{x}) = \Theta_j^\top \mathbf{x}</span> are exponentiated before normalizing:</p>
<p><span class="math display">
p_j = \frac{\exp(h_j(\mathbf{x}))}{\sum_l \exp(h_l(\mathbf{x}))} \eqqcolon \text{Softmax} (h(\mathbf{x}))_j.
</span></p>
<p>We can let <span class="math inline">\hat{\mathbf{y}}</span> be equal to <span class="math inline">\mathbf{p} = \text{Softmax} (h(\mathbf{x}))</span> since the softmax approximates the one-hot vector <span class="math inline">\mathbf{y}</span> of the target label <span class="math inline">y</span> (see remark below). Then, what is left is to define a loss function that captures the difference between the model probability vector and the true one. For this, we use the <strong>cross-entropy loss</strong> given by the negative log of the probability of the true class <span class="math inline">y</span>:</p>
<p><span class="math display">
\boxed{
    \begin{aligned}
    \ell_{\text{CE}}(h(\mathbf{x}), y)
    &amp;= -\log \hat{\mathbf{y}} \odot \mathbf{y}\\[0.8em]
    &amp;= -\log \text{Softmax} (h(\mathbf{x}))_y \\
    &amp;= -h_y(\mathbf{x})+\log \sum_{j=1}^K \exp \left(h_j(\mathbf{x})\right).
    \end{aligned}
}
</span></p>
<p>Here <span class="math inline">p_y = 1</span> implies <span class="math inline">\ell = -\log 1 = 0</span> while <span class="math inline">p_y = 0</span> implies <span class="math inline">-\log 0 = +\infty.</span> Hence, the model is penalized the more it becomes unconfident in the true class. Note taking the negative log of the true class suffices to penalize high score in the other classes since we have the constraint <span class="math inline">\sum p_j = 1.</span> Also, looking at the gradient, the logarithm has the nice property that it does not saturate as it approaches perfect prediction, and it explodes with really bad predictions:</p>
<p><span class="math display">\frac{\partial\ell}{\partial p_y} = -\frac{1}{p_y}.</span></p>
<div id="25780e85-c7fe-4664-bd8d-d4aeba4f8323" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_formats <span class="op">=</span> [<span class="st">'svg'</span>] </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.linspace(<span class="dv">0</span> <span class="op">+</span> eps, <span class="dv">1</span>, <span class="dv">10000</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">4</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.plot(p, <span class="op">-</span>np.log(p), linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"-log(p)"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.grid(alpha<span class="op">=</span><span class="fl">0.6</span>, linestyle<span class="op">=</span><span class="st">"dashed"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"p"</span>)<span class="op">;</span> plt.ylabel(<span class="st">"loss"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="01_files/figure-html/cell-2-output-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="01_files/figure-html/cell-2-output-1.svg" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>To further visualize the cross-entropy loss, let’s incorporate this graph in the evaluation process. The <strong>baseline loss</strong> is given by <span class="math inline">-\log 0.1 \approx 2.30</span> assuming uniform prediction across 10 classes. Since the loss decrease more going from <span class="math inline">p= 0.1</span> to <span class="math inline">0.2</span> compared to <span class="math inline">p = 0.5</span> to <span class="math inline">0.6</span> for the true class, then the optimizer will focus on examples with bad predictions during training. This is desirable behavior.</p>
<div id="10c4897a" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="01_files/figure-html/cell-3-output-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="01_files/figure-html/cell-3-output-1.svg" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p><strong>Remark.</strong> The softmax can be made to be numerically stable. To see this, notice that</p>
<p><span class="math display">
p_j = \frac{\exp(\Delta h_j(\mathbf{x}))}{\sum_l \exp(\Delta h_l(\mathbf{x}))}
</span></p>
<p>where <span class="math inline">\Delta h_l(\mathbf{x}) = h_l(\mathbf{x}) - \max_{m} h_m(\mathbf{x}).</span> Thus log-softmax becomes <span class="math inline">\log (1 + \sum a_j)</span> where <span class="math inline">0 &lt; a_j \leq 1</span>, preventing both underflow and overflow. Moreover, it shows that the individual scores scale exponentially with the difference from the largest score. Hence, this transformation is sometimes called soft-<em>arg</em>max since it tends to pick out the largest entry.</p>
<div id="2a87d20c" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]).<span class="bu">float</span>()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].bar(<span class="bu">range</span>(<span class="dv">3</span>), z.numpy(), label<span class="op">=</span><span class="st">"z"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].bar(<span class="bu">range</span>(<span class="dv">3</span>), z.exp().numpy(), color<span class="op">=</span><span class="st">"C1"</span>, label<span class="op">=</span><span class="st">"exp(z)"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].bar(<span class="bu">range</span>(<span class="dv">3</span>), z.exp().numpy() <span class="op">/</span> z.exp().numpy().<span class="bu">sum</span>(), color<span class="op">=</span><span class="st">"C2"</span>, label<span class="op">=</span><span class="st">"Softmax(z)"</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()<span class="op">;</span> ax[<span class="dv">1</span>].legend()<span class="op">;</span> ax[<span class="dv">2</span>].legend()<span class="op">;</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="01_files/figure-html/cell-4-output-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="01_files/figure-html/cell-4-output-1.svg" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="optimization-problem" class="level2">
<h2 class="anchored" data-anchor-id="optimization-problem">Optimization problem</h2>
<p>The third ingredient of a learning algorithm is a method for solving the associated optimization problem, i.e.&nbsp;the problem of minimizing the average loss on the training set:</p>
<p><span class="math display">
\hat{\Theta} = \underset{\Theta}{\operatorname{min}} \frac{1}{N} \sum_{i=1}^N \ell_{\text{CE}} (h_\Theta(\mathbf{x}_i), y_i)
</span></p>
<p>How do we find an optimal set of parameters <span class="math inline">\hat{\Theta}</span>? It turns out that an iterative approach is the most practical.</p>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<p>For a matrix-input, scalar-output function <span class="math inline">f\colon \mathbb{R}^{d \times k} \to \mathbb{R}</span> the <strong>gradient</strong> <span class="math inline">\nabla_\Theta f(\Theta)</span> is defined as the matrix of partial derivatives:</p>
<p><span class="math display">
\nabla_\Theta f(\Theta) =\left[\begin{array}{ccc}
\frac{\partial f(\Theta)}{\partial \Theta_{11}} &amp; \cdots &amp; \frac{\partial f(\Theta)}{\partial \Theta_{1 k}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f(\Theta)}{\partial \Theta_{d 1}} &amp; \cdots &amp; \frac{\partial f(\Theta)}{\partial \Theta_{d k}}
\end{array}\right] \in \mathbb{R}^{d \times k}
</span></p>
<p><strong>NOTE:</strong> <span class="math inline">\nabla_\Theta f</span> always has the same shape as <span class="math inline">\Theta</span> when <span class="math inline">f</span> is a scalar.</p>
<p>From the multivariate Taylor expansion,</p>
<p><span class="math display">
f(\Theta + \Delta \Theta) \approx f(\Theta) + \nabla_\Theta f(\Theta) \cdot \Delta\Theta + \mathcal{O}(\Delta\Theta^2).
</span></p>
<p>So that the gradient locally points in the direction that most increases <span class="math inline">f</span>, i.e.&nbsp;to first order. Hence, to minimize <span class="math inline">f</span>, we iteratively update the weight by <span class="math inline">-\nabla_\Theta f</span> at each point in the surface defined by <span class="math inline">f</span>:</p>
<p><span class="math display">
\Theta_{t + 1} = \Theta_t - \alpha \cdot \nabla_\Theta f (\Theta_t)
</span></p>
<p>where <span class="math inline">\alpha &gt; 0</span> is called the <strong>learning rate</strong>. GD is naturally sensitive to the scale of the learning rate and the initial point <span class="math inline">\Theta_0.</span></p>
<p><img src="./img/02-3.png"></p>
</section>
<section id="stochastic-gradient-descent-sgd" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h3>
<p>In practice, computing the gradient over the entire dataset for a single parameter update is often prohibitively expensive, especially when <span class="math inline">N</span> is large (typical in deep learning). Instead, we perform many gradient steps, each using a randomly sampled subset <span class="math inline">\mathcal{B} \subset \mathcal{D}</span> called a <strong>mini-batch</strong>, where the <strong>batch size</strong> <span class="math inline">B = |\mathcal{B}| \ll N</span>. At each training step:</p>
<ol type="1">
<li><p>Randomly sample <span class="math inline">\mathcal{B} \subset \mathcal{D}</span> so that we get <span class="math inline">\mathbf{X}_\mathcal{B} \in \mathbb{R}^{B \times d}</span> and <span class="math inline">\mathbf{Y}_{\mathcal{B}} \in [K]^B.</span></p></li>
<li><p>Update parameters: <span class="math display">\begin{aligned}\Theta_{t + 1}
= \Theta_t - \frac{\alpha}{B} \, \sum_{b \in I_\mathcal{B}} \nabla_\Theta \ell (h_{\Theta_t}(\mathbf{x}_b), y_b).\end{aligned}</span></p></li>
</ol>
<p>It follows that the sample dataset varies at each training step. Unlike the previous case where <span class="math inline">\mathcal{D}</span> is fixed. This mechanism of SGD reduces overfitting by <strong>implicit regularization</strong> of the gradient, i.e.&nbsp;adding noise in the training process.</p>
<div id="78cb702a" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="01_files/figure-html/cell-5-output-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="01_files/figure-html/cell-5-output-1.svg" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gradient-of-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-cross-entropy">Gradient of cross-entropy</h3>
<p>How do we actually compute <span class="math inline">\mathcal{L}_{\text{CE}}</span>? This can be done using the chain rule and tracking functional dependencies. Recall:</p>
<p><span class="math display">
\ell_{\text{CE}}(h_\Theta(\mathbf{x}), y) = -h_\Theta(\mathbf{x})_y + \log \sum_{j=1}^K \exp \left(h_\Theta(\mathbf{x})_j\right).
</span></p>
<p>Let’s start by deriving the gradient of the softmax loss itself. For a vector <span class="math inline">\mathbf{h} \in \mathbb{R}^K</span>:</p>
<p><span class="math display">
\frac{\partial \ell_{\text{CE}}}{\partial h_j} = - \delta_{yj} + \frac{\exp h_j}{\sum_{l=1}^K \exp h_l} = - \delta_{yj} + p_j.
</span></p>
<p>In vector form, <span class="math inline">\nabla_{\mathbf{h}} \ell_{\text{CE}} = \mathbf{p} - \mathbf{y}</span> where <span class="math inline">\mathbf{y}</span> is a one-hot vector with 1 on index <span class="math inline">y.</span> Next, to calculate the derivative with respect to <span class="math inline">\Theta</span>, we use the chain rule:</p>
<p><span class="math display">
\frac{\partial \ell_{\text{CE}}}{\partial \Theta_{ul}} =  \frac{\partial \ell_{\text{CE}}}{\partial h_j} \frac{{\partial h_j}}{\partial \Theta_{ul}} = \underbrace{(p_l - \delta_{yl})}_{K-\text{dim}} \; \underbrace{\vphantom{(}x_u}_{d-\text{dim}}.
</span></p>
<p>For the dimensions to make sense, <span class="math inline">\frac{\partial \ell_{\text{CE}}}{\partial \Theta} = \mathbf{x}(\hat{\mathbf{y}} - \mathbf{y})^\top</span> in matrix form. Recall that our vectors are column vectors and <span class="math inline">\hat{\mathbf{y}} = \mathbf{p}.</span> Here the product reverses since we traverse the dependence backwards from the loss.</p>
<p><strong>Batch form.</strong> The same process works for a batch of inputs, except that we have an additional batch index which we sum over since <span class="math inline">\ell</span> depends on all input instances. The contribution of each input is matched and aggregated using matrix multiplication:</p>
<p><span class="math display">
\frac{\partial \mathcal{L}_{\text{CE}}}{\partial \Theta} = \frac{1}{B}\,\underbrace{\vphantom{(}\mathbf{X}^\top}_{d \times M} \;\; \underbrace{(\hat{\mathbf{Y}} - \mathbf{Y})}_{M \times K}.
</span></p>
<p>Here the transposes switched since <span class="math inline">\mathbf{X}</span> is constructed such that it has rows of <span class="math inline">\mathbf{x}^\top,</span> hence we internally get a double transpose. Putting it all together, we can write the SGD update rule for softmax regression as follows:</p>
<p><span class="math display">
\Theta_{t + 1} = \Theta_t - \frac{\alpha}{B} \; \mathbf{X}^\top (\hat{\mathbf{Y}} - \mathbf{Y}).
</span></p>
<p>Here we have <span class="math inline">\frac{1}{B}</span> since <span class="math inline">\mathcal{L} = \frac{1}{B}\sum_b { \ell}_b.</span> Also it makes sense to scale down since the sum grows with batch size <span class="math inline">B.</span> Finally, notice that the step size becomes smaller as <span class="math inline">\| \hat{\mathbf{Y}} - \mathbf{Y} \| \to 0.</span> To recap, we have the following equations:</p>
<p><span class="math display">
\boxed{
\begin{aligned}
\frac{\partial \mathcal{L}_{\text{CE}}}{\partial \mathbf{H}} &amp;= \frac{1}{B}  (\hat{\mathbf{Y}} - \mathbf{Y}) \\[0.75em]
\frac{\partial \mathcal{L}_{\text{CE}}}{\partial \Theta} &amp;= \frac{1}{B} \mathbf{X}^\top  (\hat{\mathbf{Y}} - \mathbf{Y}) \\[0.60em]
\Theta_{t + 1} &amp;= \Theta_t - \frac{\alpha}{B} \, \mathbf{X}^\top (\hat{\mathbf{Y}} - \mathbf{Y})
\end{aligned}
}
</span></p>
</section>
</section>
<section id="code-implementation" class="level2">
<h2 class="anchored" data-anchor-id="code-implementation">Code implementation</h2>
<p>Since shuffling a large dataset at each training step is expensive, in practice, we shuffle it <em>once</em> and then iteratively draw <span class="math inline">B</span>-sized slices (batches) at each step—effectively sampling without replacement. A full pass through <span class="math inline">\mathcal{D}</span> under this process is called an <strong>epoch</strong>. After each epoch, we reshuffle the dataset for the next one. Consequently, the total number of SGD steps scales as <span class="math inline">\mathcal{O}(Ne / B)</span>, where <span class="math inline">e</span> is the number of epochs.</p>
<p>In PyTorch, this procedure is handled by the <a href="https://docs.pytorch.org/docs/stable/data.html#data-loading-order-and-sampler"><code>DataLoader</code> object</a>:</p>
<div id="34bc6849" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>NUM_EPOCHS <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">10</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(x, batch_size<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(NUM_EPOCHS):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> train_loader:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(batch.numpy())</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[9 2 7]
[6 1 0]
[5 4 3]

[0 3 2]
[4 1 9]
[7 6 5]
</code></pre>
</div>
</div>
<p>We will train a classification model based on the SGD update rule above:</p>
<div id="b2064832-9190-42dd-b46e-3cc7dfeb5aa2" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.1307</span>,), (<span class="fl">0.3081</span>,))  <span class="co"># MNIST mean and std</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.MNIST(<span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>,  transform<span class="op">=</span>transform)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>valid_dataset <span class="op">=</span> datasets.MNIST(<span class="st">'./data'</span>, train<span class="op">=</span><span class="va">False</span>, transform<span class="op">=</span>transform)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Defining the linear model:</p>
<div id="b983e078-806c-41cf-80c5-5682d3dc861c" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Remark.</strong> The linear model can be extended to have a bias vector <span class="math inline">\beta</span>, so that <span class="math inline">h_\Theta = \mathbf{X}\Theta + \beta</span> where <span class="math inline">\beta \in \mathbb{R}^K.</span> But it turns out there is a “bias trick” in deep learning where an additional dimension containing only <span class="math inline">+1</span> is added so that the input becomes of shape <span class="math inline">(N, d + 1).</span> For simplicity, we stick with no bias.</p>
<div id="3fc822b8-4eb2-4862-9947-9c7d5a91b9e3" class="cell" data-scrolled="true" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>loss_train <span class="op">=</span> []</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>NUM_EPOCHS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>ALPHA <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(x, y):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">784</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> model(x)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> theta <span class="kw">in</span> model.parameters():</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> F.softmax(h, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        e <span class="op">=</span> F.one_hot(y, num_classes<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> x.T <span class="op">@</span> (p <span class="op">-</span> e) <span class="op">/</span> B</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> ALPHA <span class="op">*</span> g.T</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>p[torch.arange(B), y].log().mean()</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item()</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>B, shuffle<span class="op">=</span><span class="va">True</span>)  <span class="co"># SGD</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(NUM_EPOCHS)):</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> train_loader:</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> train_step(x, y)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        loss_train.append(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 5/5 [00:20&lt;00:00,  4.02s/it]</code></pre>
</div>
</div>
<p><strong>Remark.</strong> Pytorch <code>nn.Linear</code> computes <code>x @ θ.T + b</code>. Hence, we take <code>g.T</code> before updating the parameter <code>θ</code>.</p>
<div id="5a2328e4-6a3a-4fd5-8317-9d597cb48738" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>plt.plot(np.array(loss_train).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">10</span>).mean(<span class="dv">1</span>), label<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"step"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"loss"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">"dashed"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="01_files/figure-html/cell-10-output-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="01_files/figure-html/cell-10-output-1.svg" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Sample label predictions:</p>
<div id="f21ec709-1f28-4717-943f-796c7b5c2467" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>dv <span class="op">=</span> DataLoader(valid_dataset, batch_size<span class="op">=</span>B, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dv))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model(x.reshape(B, <span class="op">-</span><span class="dv">1</span>)).argmax(<span class="dv">1</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> (out <span class="op">==</span> y).<span class="bu">float</span>()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Batch acc:"</span>, <span class="ss">f"</span><span class="sc">{</span>acc<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">.</span><span class="bu">int</span>()<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>B<span class="sc">}</span><span class="ss">"</span>, <span class="ss">f"(</span><span class="sc">{</span>acc<span class="sc">.</span>mean()<span class="sc">.</span>item() <span class="op">*</span> <span class="dv">100</span><span class="sc">}</span><span class="ss">%)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batch acc: 15/16 (93.75%)</code></pre>
</div>
</div>
<div id="aa81b08f" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    a, b <span class="op">=</span> <span class="bu">divmod</span>(i, <span class="dv">4</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    ax[a, b].imshow(x[i].reshape(<span class="dv">28</span>, <span class="dv">28</span>), cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> <span class="st">"black"</span> <span class="cf">if</span> out[i] <span class="op">==</span> y[i] <span class="cf">else</span> <span class="st">"red"</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    ax[a, b].set_title(<span class="ss">f"</span><span class="sc">{</span>out[i]<span class="sc">}</span><span class="ss">"</span>, color<span class="op">=</span>color)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    ax[a, b].axis(<span class="st">"off"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="01_files/figure-html/cell-12-output-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="01_files/figure-html/cell-12-output-1.svg" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p><strong>Evals.</strong> The practical goal of training is not actually to minimize <span class="math inline">\mathcal{L}_\mathcal{D}(\Theta).</span> But to minimize the loss for samples outside of the training dataset. That is, the model should be accurate on <strong>test data</strong>. If the model does not <a href="https://en.wikipedia.org/wiki/Overfitting">overfit</a> and the test distribution does not drift too far from the training distribution, then we should be good.</p>
<div id="37070e9e-4837-4906-b21f-21b05c6263e4" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>tot <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, y <span class="kw">in</span> dv:</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> model(x.reshape(B, <span class="op">-</span><span class="dv">1</span>)).argmax(<span class="dv">1</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> (out <span class="op">==</span> y).<span class="bu">float</span>()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    tot <span class="op">+=</span> <span class="bu">len</span>(y)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">+=</span> correct.<span class="bu">sum</span>()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>acc <span class="op">/</span> tot <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test acc: 92.31%</code></pre>
</div>
</div>
</section>
<section id="appendix-model-complexity" class="level2">
<h2 class="anchored" data-anchor-id="appendix-model-complexity">Appendix: Model complexity</h2>
<p>Since SGD relies on a stochastic process, the performance of the resulting trained model varies. How are we sure that we aren’t just lucky for this particular run / random seed? What is the variance of the trained model performance over multiple runs? Is there a way to control this? This issue is at the core of learning theory and precisely what the <a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html">Bias-Variance Tradeoff</a> addresses.</p>
<p>In practice, the crucial parameter to control is <strong>model complexity</strong>. Here <strong>model capacity</strong> or <strong>complexity</strong> is a measure of how complicated a pattern or relationship a model architecture can express. Let <span class="math inline">f</span> be the true function that underlies the task. If model capacity is sufficiently large, the <strong>model class</strong> <span class="math inline">\mathcal{F} = \{f_{\Theta} \mid \Theta \in \mathbb{R}^d \}</span> contains an approximation <span class="math inline">\hat{f} \in \mathcal{F}</span> such that <span class="math inline">\| f - \hat{f} \| &lt; \epsilon</span> for a small enough <span class="math inline">\epsilon &gt; 0.</span></p>
<p>The capacity of a model class can be controlled, for example, by the number of learnable parameters in practical architectures. It can also be constrained directly by applying <strong>regularization</strong> or certain <strong>prior knowledge</strong> such as invariances. This biases the model towards certain solutions, so these constraints are sometimes referred to as <strong>inductive biases</strong> — such knowledge is bias in the sense that it makes some solutions more likely, and others less likely. The tradeoff is that the model are steered to biased solutions more efficiently.</p>
</section>
<section id="appendix-maximum-likelihood-estimation-mle" class="level2">
<h2 class="anchored" data-anchor-id="appendix-maximum-likelihood-estimation-mle">Appendix: Maximum likelihood estimation (MLE)</h2>
<p>We derive a loss function based on the principle of <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> (MLE), i.e.&nbsp;finding optimal parameters <span class="math inline">\hat{\Theta}</span> such that the dataset is assigned the highest probability under <span class="math inline">h_{\hat{\Theta}}.</span> Consider a parametric model of the target denoted by <span class="math inline">p_{\Theta}(y \mid \mathbf{x}).</span> The <strong>likelihood</strong> of the <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">i.i.d.</a> sample <span class="math inline">\mathcal{D} = (\mathbf{x}_i, y_i)_{i=1}^N</span> can be defined as</p>
<p><span class="math display">
\begin{aligned}
{L}(\Theta)
&amp;= \prod_{i=1}^N p_{\Theta}(\mathbf{x}_i, y_i) \\
&amp;= {\prod_{i=1}^N {p_{\Theta}(y_i \mid \mathbf{x}_i)}} \cdot p_{\Theta}(\mathbf{x}_i).
\end{aligned}
</span></p>
<p>This is proportional to the probability assigned by the parametric model with parameters <span class="math inline">\Theta</span> on the sample <span class="math inline">\mathcal{D}.</span> The i.i.d. assumption is important: maximizing the likelihood results in a model that focuses more on inputs that are better represented in the sample, i.e.&nbsp;more probable since they are sampled more. This makes sense, but should always be kept in mind, especially if we are concerned with getting accurate inferences for underrepresented segments of the dataset.</p>
<p>Going back, probabilities are small numbers in <span class="math inline">[0, 1]</span>, so applying the logarithm to convert the large product to a sum is a good idea. Moreover, it doesn’t affect our optimization objective since <span class="math inline">\log</span> is monotonic. Then,</p>
<p><span class="math display">
\begin{aligned}
\log {L}(\Theta)
&amp;= \sum_{i=1}^N \log p_{\Theta}(y_i \mid \mathbf{x}_i) + \sum_{i=1}^N \log p_{\Theta}(\mathbf{x}_i).
\end{aligned}
</span></p>
<p>MLE then maximizes the log-likelihood with respect to the parameters <span class="math inline">\Theta</span> to get a model that makes training data more probable. The latter term is independent of <span class="math inline">\Theta,</span> hence can be dropped. It is customary in machine learning to convert this to a <strong>minimization problem</strong>. Finally, we multiply by <span class="math inline">\frac{1}{N}</span> to scale the sum with the number of examples which doesn’t affect the solution. The following then becomes our optimization problem:</p>
<p><span class="math display">\boxed{
\hat{\Theta} = \underset{\Theta}{\text{argmin}}\, -\frac{1}{N}\sum_{i=1}^N \log p_{\Theta}(y_i \mid \mathbf{x}_i).
}
</span></p>
<p>Hence, MLE is <strong>equivalent</strong> to minimizing cross-entropy by writing</p>
<p><span class="math display">p_{\Theta}(y \mid \mathbf{x}) = \text{Softmax}(h_\Theta(\mathbf{x}))_y.</span></p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/particle1331\.github\.io\/ai-notebooks\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../topics/deep/index.html" class="pagination-link" aria-label="Deep Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Deep Learning</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../topics/deep/02.html" class="pagination-link" aria-label="Neural Networks">
        <span class="nav-page-text">Neural Networks</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>