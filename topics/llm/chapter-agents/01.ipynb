{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c665168",
   "metadata": {},
   "source": [
    "# Building Effective Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a3b4a",
   "metadata": {},
   "source": [
    "This is based on [this article](https://www.anthropic.com/engineering/building-effective-agents) by [Anthropic](https://www.anthropic.com/). We will explore common patterns for building effective LLM agentic systems using pure Python around LLM APIs. In particular, we use the [Python SDK](https://github.com/openai/openai-python/tree/main) for the [OpenAI API](https://platform.openai.com/docs/api-reference/introduction). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ab857",
   "metadata": {},
   "source": [
    "## OpenAI API client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339c546",
   "metadata": {},
   "source": [
    "First, we need to load the API key in the environmental variables. The client expects the variable name `OPENAI_API_KEY` which we load from the `.env` file. This is easy to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bfff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def load_dotenv(verbose=False):\n",
      "    with open(\".env\") as f:\n",
      "        for line in f.readlines():\n",
      "            k, v = line.split(\"=\")\n",
      "            os.environ[k] = v.strip().strip('\"')\n",
      "            if verbose:\n",
      "                print(f\"Loaded env variable: {k}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from notebooks.utils import load_dotenv\n",
    "print(inspect.getsource(load_dotenv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d77b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded env variable: OPENAI_API_KEY\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f34dc",
   "metadata": {},
   "source": [
    "Then the API key is automatically read by the **client**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424d6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You're a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me about the history of GPT in a single paragraph.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "response_text = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd2ad0",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "We can have **multiple completions** of the same prompt. This allows choosing between the responses.\n",
    "For example, we can set `temperature=0.9` to get more varied outputs, so that choosing becomes nontrivial. \n",
    "By default only 1 completion by default, hence `[0]`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055bb4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Generative Pre-trained Transformer (GPT) is a series of language models '\n",
      " 'developed by OpenAI, beginning with the release of GPT in 2018. This model '\n",
      " 'introduced the concept of leveraging unsupervised learning to pre-train a '\n",
      " 'transformer-based architecture on a large corpus of text, which could then '\n",
      " 'be fine-tuned for specific tasks. Successive iterations like GPT-2, released '\n",
      " 'in 2019, showcased significant improvements in generating coherent and '\n",
      " 'contextually relevant text, though its release was initially limited due to '\n",
      " 'concerns over misuse. GPT-3, launched in 2020, expanded upon this by using '\n",
      " '175 billion parameters, offering even more nuanced and human-like text '\n",
      " 'generation and understanding capabilities, leading to applications across '\n",
      " 'creative writing, coding, and conversational agents. In 2023, OpenAI '\n",
      " \"introduced GPT-4, which further enhanced the model's depth and versatility, \"\n",
      " 'strengthening its ability to process and generate complex language-based '\n",
      " 'tasks. Overall, the evolution of GPT models has significantly advanced the '\n",
      " 'field of natural language processing and AI research.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response_text, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4a310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Generative Pre-trained Transformer (GPT) is a series of language models '\n",
      " 'developed by OpenAI, beginning with the release of GPT in 2018. This model '\n",
      " 'introduced the concept of leveraging unsupervised learning to pre-train a '\n",
      " 'transformer-based architecture on a large corpus of text, which could then '\n",
      " 'be fine-tuned for specific tasks. Successive iterations like GPT-2, released '\n",
      " 'in 2019, showcased significant improvements in generating coherent and '\n",
      " 'contextually relevant text, though its release was initially limited due to '\n",
      " 'concerns over misuse. GPT-3, launched in 2020, expanded upon this by using '\n",
      " '175 billion parameters, offering even more nuanced and human-like text '\n",
      " 'generation and understanding capabilities, leading to applications across '\n",
      " 'creative writing, coding, and conversational agents. In 2023, OpenAI '\n",
      " \"introduced GPT-4, which further enhanced the model's depth and versatility, \"\n",
      " 'strengthening its ability to process and generate complex language-based '\n",
      " 'tasks. Overall, the evolution of GPT models has significantly advanced the '\n",
      " 'field of natural language processing and AI research.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda6e7b",
   "metadata": {},
   "source": [
    "Entire model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55f61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'annotations': [],\n",
      "                          'audio': None,\n",
      "                          'content': 'Generative Pre-trained Transformer (GPT) '\n",
      "                                     'is a series of language models developed '\n",
      "                                     'by OpenAI, beginning with the release of '\n",
      "                                     'GPT in 2018. This model introduced the '\n",
      "                                     'concept of leveraging unsupervised '\n",
      "                                     'learning to pre-train a '\n",
      "                                     'transformer-based architecture on a '\n",
      "                                     'large corpus of text, which could then '\n",
      "                                     'be fine-tuned for specific tasks. '\n",
      "                                     'Successive iterations like GPT-2, '\n",
      "                                     'released in 2019, showcased significant '\n",
      "                                     'improvements in generating coherent and '\n",
      "                                     'contextually relevant text, though its '\n",
      "                                     'release was initially limited due to '\n",
      "                                     'concerns over misuse. GPT-3, launched in '\n",
      "                                     '2020, expanded upon this by using 175 '\n",
      "                                     'billion parameters, offering even more '\n",
      "                                     'nuanced and human-like text generation '\n",
      "                                     'and understanding capabilities, leading '\n",
      "                                     'to applications across creative writing, '\n",
      "                                     'coding, and conversational agents. In '\n",
      "                                     '2023, OpenAI introduced GPT-4, which '\n",
      "                                     \"further enhanced the model's depth and \"\n",
      "                                     'versatility, strengthening its ability '\n",
      "                                     'to process and generate complex '\n",
      "                                     'language-based tasks. Overall, the '\n",
      "                                     'evolution of GPT models has '\n",
      "                                     'significantly advanced the field of '\n",
      "                                     'natural language processing and AI '\n",
      "                                     'research.',\n",
      "                          'function_call': None,\n",
      "                          'refusal': None,\n",
      "                          'role': 'assistant',\n",
      "                          'tool_calls': None}}],\n",
      " 'created': 1756118209,\n",
      " 'id': 'chatcmpl-C8OrBcCR9a0TyVqlcBf7vLBix9Tdj',\n",
      " 'model': 'gpt-4o-2024-08-06',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': 'default',\n",
      " 'system_fingerprint': 'fp_80956533cb',\n",
      " 'usage': {'completion_tokens': 205,\n",
      "           'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                         'audio_tokens': 0,\n",
      "                                         'reasoning_tokens': 0,\n",
      "                                         'rejected_prediction_tokens': 0},\n",
      "           'prompt_tokens': 28,\n",
      "           'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
      "           'total_tokens': 233}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(completion.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff814f09",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "We are using the **chat completions** API where an autoregressive process that's running under the hood. Here the prompt to be completed is:\n",
    "\n",
    "```python\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic but terse assistant.\"},   # prompt\n",
    "    {\"role\": \"user\", \"content\": \"What is the color of the sky?\"}              # prompt\n",
    "]\n",
    "```\n",
    "\n",
    "And the completion is given by the API's output:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"content\": \"The sky's color shifts from azure to amber, a canvas for sun's daily journey.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Remark.** The generated response is statistically the most likely continuation of the prompt text sequence. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c766d",
   "metadata": {},
   "source": [
    "## Structured outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52597344",
   "metadata": {},
   "source": [
    "[Structured outputs](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses) is a feature that ensures that a model generates responses that adhere to a supplied **schema** (e.g. a Pydantic model). As such, the output can then be parsed using the same Pydantic model. Structured outputs makes prompting significantly simpler: no more need for strongly worded prompts to achieve consistent formatting, no explicitly having to retry incorrectly formatted responses, or having invalid hallucinated values (can specify **enums**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70811ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "        },\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc0da2",
   "metadata": {},
   "source": [
    "## Building block: Augmented LLM\n",
    "\n",
    "This is the foundational building block of agentic systems. The **augmented LLM** is a language model enhanced with **retrieval**, **tools**, and **memory**. Current models, due to their reasoning and understanding capabilities, are able to effectively generate their own search queries, select appropriate tools, and determinine what information in necessary to retain or retrieve to solve problems or perform tasks. Augmentation is necessary for LLMs to reason about data outside of their training data, or data that is outdated relative to the training cutoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb22b7f",
   "metadata": {},
   "source": [
    "![**The augmented LLM.** A conceptual diagram of a core LLM service augmented with retrieval, tools, and memory capabilities.](../img/augmented-llm.png){#fig-augmentedllm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98448acb",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Also known as **function calling**. Function calling give models access to external tools and data that they can use to respond to prompts. Since LLMs *only* consume and generate text, they cannot actually execute functions. Instead, the main program listens to the LLM hallucinate and executes the commands based on that (@fig-brainvat)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07436a08",
   "metadata": {},
   "source": [
    "![(**right**) LLM as brain in a vat that hallucinates outputs from information contained in inputs. It tells us *what* function to execute with *what* arguments. (**left**) The computer listens to the LLM and performs computation based on it.](../img/llm-brain-vat.png){#fig-brainvat width=80%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833803bf",
   "metadata": {},
   "source": [
    "To show tool calling, we find the weather in [Quisao](https://www.philatlas.com/luzon/r04a/rizal/pililla/quisao.html). We want the agent to call the following API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6bdad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': '2025-08-25T10:30',\n",
       " 'interval': 900,\n",
       " 'temperature_2m': 28.4,\n",
       " 'wind_speed_10m': 9.4,\n",
       " 'relative_humidity_2m': 83,\n",
       " 'precipitation': 0.5,\n",
       " 'precipitation_probability': 91}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    \"\"\"\n",
    "    Get current weather data for provided coordinates with units:\n",
    "    temperature (celsius), wind speed (kph), & precipitation (mm).\n",
    "    \"\"\"\n",
    "    response = requests.get((\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&\"\n",
    "        \"current=temperature_2m,wind_speed_10m,relative_humidity_2m,precipitation,precipitation_probability\"\n",
    "    ))\n",
    "    data = response.json()\n",
    "    return data[\"current\"]\n",
    "\n",
    "\n",
    "get_weather(latitude=14.4779, longitude=121.3214)  # true coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c253083",
   "metadata": {},
   "source": [
    "**Tool definition.** For the LLM to understand a specific tool, we have to define a schema that informs the model of what the tool does and its expected (required and optional) arguments. The following is the function definition for `get_weather`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c9d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\", # <1>\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",  # <2>\n",
    "            \"description\": \"Get current weather data for provided coordinates with units: temperature (celsius), wind speed (kph), & precipitation (mm).\",    # <3>\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\", # <4>\n",
    "                \"properties\": { # <5>\n",
    "                    \"latitude\": {\n",
    "                        \"type\": \"number\"\n",
    "                    },\n",
    "                    \"longitude\": {\n",
    "                        \"type\": \"number\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"latitude\", \"longitude\"],\n",
    "                \"additionalProperties\": False,  # <6>\n",
    "            },\n",
    "            \"strict\": True, # <7>\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f4811",
   "metadata": {},
   "source": [
    "1. Should always be function.\n",
    "2. Function's name (i.e. `get_weather`).\n",
    "3. Usually just the docstring. Should describe when and how to use the function.\n",
    "4. Parameters for LLMs are naturally JSON objects. Because `parameters` is defined by a JSON schema, you can leverage many of its rich features like property types, enums, descriptions, nested objects, and so on. For example, we can have: \n",
    "    ```\n",
    "    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"unit of measure for temperature\"}\n",
    "    ```\n",
    "5. List of arguments. Clearly the two arguments are required.\n",
    "6. Part of JSON schema that determines whether extra fields are valid or not.\n",
    "7. Not part of JSON schema, but OpenAI function calling option that *guides* the model to strictly follow the schema (i.e. not improvise). Setting `additionalProperties` to `False` and `strict` to `True` works together to ensure that the model generates the correct parameters schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014a64d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotations': [],\n",
      " 'audio': None,\n",
      " 'content': None,\n",
      " 'function_call': None,\n",
      " 'refusal': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"latitude\":14.4629,\"longitude\":121.3706}',\n",
      "                              'name': 'get_weather'},\n",
      "                 'id': 'call_H2GV3HT3Kod05YksTqoUHla2',\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful weather assistant.\"\n",
    "user_prompt = \"What's the weather like in Quisao, Pililla, Rizal right now?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "pprint(completion.choices[0].message.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281f9a4",
   "metadata": {},
   "source": [
    "Impressive that it's able to get fairly accurate coordinates without using web search. Also notice that whenever we have `tools`, the model responds with only `tool_calls` as nonempty (e.g. `content` is empty). \n",
    "We will iterate over tool calls and process them separately. Each function call and their results are then logged as part of the sequence of **chat messages**. This explains why we defined `messages` outside of the API call unlike the usual setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94696d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function(name, args):\n",
    "    fn = {\n",
    "        \"get_weather\": get_weather  \n",
    "    }\n",
    "    return fn[name](**args)\n",
    "\n",
    "\n",
    "assistant_message = completion.choices[0].message\n",
    "messages.append(assistant_message.model_dump())  # <1>\n",
    "\n",
    "for tool_call in assistant_message.tool_calls:\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    name = tool_call.function.name\n",
    "    tool_output = call_function(name, args)\n",
    "    messages.append({    # <2>\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": tool_call.id, \n",
    "        \"content\": json.dumps(tool_output)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14c4e7",
   "metadata": {},
   "source": [
    "1. The tool call is logged with role `assistant`.\n",
    "2. Next, we log the result of the function call with role `tool`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc27b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful weather assistant.', 'role': 'system'},\n",
      " {'content': \"What's the weather like in Quisao, Pililla, Rizal right now?\",\n",
      "  'role': 'user'},\n",
      " {'annotations': [],\n",
      "  'audio': None,\n",
      "  'content': None,\n",
      "  'function_call': None,\n",
      "  'refusal': None,\n",
      "  'role': 'assistant',\n",
      "  'tool_calls': [{'function': {'arguments': '{\"latitude\":14.4629,\"longitude\":121.3706}',\n",
      "                               'name': 'get_weather'},\n",
      "                  'id': 'call_H2GV3HT3Kod05YksTqoUHla2',\n",
      "                  'type': 'function'}]},\n",
      " {'content': '{\"time\": \"2025-08-25T10:30\", \"interval\": 900, \"temperature_2m\": '\n",
      "             '26.0, \"wind_speed_10m\": 4.6, \"relative_humidity_2m\": 89, '\n",
      "             '\"precipitation\": 0.5, \"precipitation_probability\": 92}',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_H2GV3HT3Kod05YksTqoUHla2'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af390cce",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Chat completion API calls have stateless single request-response cycles which gives the user straightforward control over the **message history**. This can be seen in the above example where we manually manage message history with tool calls declaration as well as actual function outputs.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f1dff",
   "metadata": {},
   "source": [
    "Next, we pass this thread to another API call (possibly to a different LLM) which will process the outputs of the tool calls along with earlier chat messages. To take advantage of structured outputs we again define a response format. Here we use Pydantic `Field` with a description that helps the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42e9bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    response: str = Field(description=\"A natural language response to the user's question.\")\n",
    "    temperature: float = Field(description=\"Current temperature in celsius for the given location.\")\n",
    "\n",
    "\n",
    "completion_weather = client.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=tools,    # <!>\n",
    "    response_format=WeatherResponse,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d568be6",
   "metadata": {},
   "source": [
    ":::{.callout-caution}\n",
    "The aggregator also needs access to tools for it to understand the context of each tool call!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0fdc2",
   "metadata": {},
   "source": [
    "**Final output.** Note that even units (included in the `get_weather` docstring) were correctly identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f1eda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n",
      "('The weather in Quisao, Pililla, Rizal is currently warm with a temperature '\n",
      " 'of 26Â°C. There is a gentle breeze blowing at 4.6 kph. The humidity level is '\n",
      " 'quite high at 89%, and there is light rain with 0.5 mm of precipitation and '\n",
      " 'a 92% probability of further rain.')\n"
     ]
    }
   ],
   "source": [
    "parsed_weather = completion_weather.choices[0].message.parsed\n",
    "print(parsed_weather.temperature)\n",
    "pprint(parsed_weather.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27b1c4",
   "metadata": {},
   "source": [
    "### Memory and retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76762d",
   "metadata": {},
   "source": [
    "The following is a toy example of an agent that reads and writes to an external data source. One characteristic of retrieval systems is that the entire process is **stateless**, e.g. it cannot learn from interactions. Retrieval systems generally involve queries to an external data source, then adding the response to the current model context.\n",
    "\n",
    "For the following example, the LLM also writes to the same memory store hence affecting future generation states. Hence, it is **stateful**. We can think of the external memory store as the **long-term memory** of the system. On the other hand, LLMs naturally have **short-term memory** in the form of its context. The architecture is shown in @fig-retrieval-system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd8b60",
   "metadata": {},
   "source": [
    "![The LLM expresses the intent to write to the memory store via the structured output. Then, it is up to the main program to perform the actual writing. This allows hooks like [guardrails](https://cookbook.openai.com/examples/how_to_use_guardrails) to be applied before executing the function. Note that the retrieval happens prior to LLM processing. It would be nice to have the LLM read the entire filestore but this becomes more expensive as the memory store grows. In practice, information retrieval techniques such as TF-IDF and embedding similarity can be used.\n",
    "](../img/retrieval-system.png){#fig-retrieval-system}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e31d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class MemoryItem(BaseModel):\n",
    "    tag: str\n",
    "    info: str\n",
    "    reason: str\n",
    "\n",
    "class RetainResponse(BaseModel):\n",
    "    items: List[MemoryItem]\n",
    "\n",
    "\n",
    "class MemoryStore:\n",
    "    def __init__(self, path=\"memory.json\"):\n",
    "        \"\"\"Load memory from JSON file in local path.\"\"\"\n",
    "        self.path = path\n",
    "        self.data = []\n",
    "        self.tags = set()\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        try:\n",
    "            self.data = json.load(open(self.path))\n",
    "        except FileNotFoundError:\n",
    "            self.reset()\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.path, \"w\") as f:\n",
    "            json.dump(self.data, f, indent=2)\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = []\n",
    "        self.tags = set()\n",
    "        self.save()\n",
    "    \n",
    "    def add(self, item: dict[str, str]):\n",
    "        tag = list(item.keys())[0]\n",
    "        self.tags.add(tag)\n",
    "        self.data.append(item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def retrieve(self, query: str, topk: int = 3) -> List[dict]:\n",
    "        \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "        \n",
    "        query_words = query.lower().split()\n",
    "        retrieved = []\n",
    "        \n",
    "        for memory in reversed(self.data):  # <1>\n",
    "            for tag, info in memory.items():\n",
    "                memory_text = f\"{tag} {info}\".lower()\n",
    "\n",
    "                if any(word in memory_text for word in query_words): # <2>\n",
    "                    retrieved.append(memory)\n",
    "            \n",
    "            if len(retrieved) == topk:\n",
    "                break\n",
    "        \n",
    "        return retrieved\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "mem = MemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68e9d4",
   "metadata": {},
   "source": [
    "1. More recent = more relevant.\n",
    "2. Substring check. e.g. `'commute' in 'commute_experience'` evaluates to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cc4dd",
   "metadata": {},
   "source": [
    "Next, we define the LLM generation and write steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(user_input, topk: int = 3):\n",
    "    \"\"\"Generate memory items from current input and existing memory.\"\"\"\n",
    "\n",
    "    retrieved_memories = mem.retrieve(user_input, topk)\n",
    "    current_tags = list(mem.tags)\n",
    "    system_prompt = f\"\"\"\n",
    "You are an assistant that logs daily interactions. \n",
    "Relevant past memories based on your current input:\n",
    "{retrieved_memories}\n",
    "\n",
    "Current tags:\n",
    "{current_tags}\n",
    "\n",
    "Your goal is to identify and retain atomic, standalone facts \n",
    "that are likely to be relevant for future interactions.\n",
    "\n",
    "**GUIDELINES:**\n",
    "\n",
    "1. **EXTRACT ATOMIC FACTS:**\n",
    "    - Break down information into the smallest meaningful, self-contained units.\n",
    "    - **Good**: \"User's favorite programmer is Jon Blow.\"\n",
    "    - **Bad**: \"User mentioned their favorite programmer is Jon Blow who is a famous game programmer\" (This has two facts.)\n",
    "    - The `info` must be a concise, direct paraphrase of the fact. Remove conversational fluff.\n",
    "    - **Good Info:** \"User's favorite city is Tokyo\"\n",
    "    - **Bad Info:** \"The user stated that if they had to pick a favorite city, they think it would be Tokyo.\"\n",
    "\n",
    "2.  **TAG EFFECTIVELY:**\n",
    "    - **Format:** Prefer generic, descriptive tags in `snake_case`.\n",
    "    - **Simple:** Prefer simple tags. Choose `commute` is better than `commute_experience`.\n",
    "    - **Reuse:** Strongly prefer existing tags. Create a new tag only if necessary.\n",
    "    - An example: For \"I really enjoy hiking in the Alps every summer,\" a good tag is `hobby` or `outdoor_activity`.\n",
    "    \n",
    "4.  **EVALUATE & DECIDE:**\n",
    "    - It is acceptable to save zero logs from an input if nothing is meaningfully new or relevant.\n",
    "    - Save multiple logs if the user provides multiple distinct pieces of information.\n",
    "    - Each memory item should make sense on its own. There should be no dependence between separate logs.\n",
    "\"\"\"\n",
    "    \n",
    "    completion = client.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ],\n",
    "        response_format=RetainResponse,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "\n",
    "def write(user_input: str) -> RetainResponse:\n",
    "    \"\"\"Write generated items to memory store.\"\"\"\n",
    "    response = llm_generate(user_input)\n",
    "    for item in response.items:\n",
    "        mem.add({item.tag: item.info})\n",
    "    \n",
    "    mem.save()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce0933b",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6599b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs = [\n",
    "    \"Woke up, showered, and left for work at the usual time. Had toast for breakfast before heading out.\",\n",
    "    \"Traffic was smooth, arrived at work earlier than usual.\",\n",
    "    \"Took the train, had to stand the whole ride since it was packed.\",\n",
    "    \"Stopped by the bakery on the way and picked up bread for the team.\",\n",
    "    \"Opened my email first thing at the office, mostly routine messages.\",\n",
    "    \"Woah! Some guy just came out of nowhere and darted into traffic. That was pretty shocking. Crazy.\",\n",
    "    \"Listened to a podcast while walking to the subway.\",\n",
    "    \"Grabbed a pen from my desk drawer because mine ran out of ink.\",\n",
    "]\n",
    "\n",
    "items = []\n",
    "for input_text in logs:\n",
    "    for item in write(input_text).items:\n",
    "        d = item.model_dump()\n",
    "        d[\"text\"] = input_text\n",
    "        items.append(d)\n",
    "\n",
    "df_resp = pd.DataFrame(items)\n",
    "mem.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b4ef6",
   "metadata": {},
   "source": [
    "The agent decides whether to reuse a tag or create a new one based on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d4931a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: (8 total logs, 3 tags, 7 saved)\n",
      "['morning_routine', 'commute', 'work_tool']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>info</th>\n",
       "      <th>reason</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>morning_routine</td>\n",
       "      <td>User ate toast for breakfast.</td>\n",
       "      <td>Information about breakfast can be relevant to dietary preferences or habits.</td>\n",
       "      <td>Woke up, showered, and left for work at the usual time. Had toast for breakfast before heading out.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>commute</td>\n",
       "      <td>User arrived at work earlier than usual due to smooth traffic.</td>\n",
       "      <td>The fact that traffic conditions affected the user's commute is an atomic detail relevant for potential future interactions about commuting patterns.</td>\n",
       "      <td>Traffic was smooth, arrived at work earlier than usual.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commute</td>\n",
       "      <td>User took a crowded train and had to stand for the whole ride.</td>\n",
       "      <td>This gives specific details about their commuting experience, which might affect future choices or preferences.</td>\n",
       "      <td>Took the train, had to stand the whole ride since it was packed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>commute</td>\n",
       "      <td>User stopped by the bakery on the way to pick up bread.</td>\n",
       "      <td>This is a separate action performed during the commute and complements the previously logged train commute information.</td>\n",
       "      <td>Stopped by the bakery on the way and picked up bread for the team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>commute</td>\n",
       "      <td>User witnessed a person darting into traffic.</td>\n",
       "      <td>This standalone fact about a commute experience is unique and might be relevant for understanding similar future conversation contexts.</td>\n",
       "      <td>Woah! Some guy just came out of nowhere and darted into traffic. That was pretty shocking. Crazy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>commute</td>\n",
       "      <td>User listens to a podcast while walking to the subway.</td>\n",
       "      <td>This information adds detail to the user's commute routine. It might be relevant for future recommendations or interactions about podcasts.</td>\n",
       "      <td>Listened to a podcast while walking to the subway.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>work_tool</td>\n",
       "      <td>User keeps extra pens in their desk drawer.</td>\n",
       "      <td>This detail about where extra pens are kept might be useful for future reminders about their workspace or work routine.</td>\n",
       "      <td>Grabbed a pen from my desk drawer because mine ran out of ink.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag  \\\n",
       "0  morning_routine   \n",
       "1          commute   \n",
       "2          commute   \n",
       "3          commute   \n",
       "4          commute   \n",
       "5          commute   \n",
       "6        work_tool   \n",
       "\n",
       "                                                             info  \\\n",
       "0                                   User ate toast for breakfast.   \n",
       "1  User arrived at work earlier than usual due to smooth traffic.   \n",
       "2  User took a crowded train and had to stand for the whole ride.   \n",
       "3         User stopped by the bakery on the way to pick up bread.   \n",
       "4                   User witnessed a person darting into traffic.   \n",
       "5          User listens to a podcast while walking to the subway.   \n",
       "6                     User keeps extra pens in their desk drawer.   \n",
       "\n",
       "                                                                                                                                                  reason  \\\n",
       "0                                                                          Information about breakfast can be relevant to dietary preferences or habits.   \n",
       "1  The fact that traffic conditions affected the user's commute is an atomic detail relevant for potential future interactions about commuting patterns.   \n",
       "2                                        This gives specific details about their commuting experience, which might affect future choices or preferences.   \n",
       "3                                This is a separate action performed during the commute and complements the previously logged train commute information.   \n",
       "4                This standalone fact about a commute experience is unique and might be relevant for understanding similar future conversation contexts.   \n",
       "5            This information adds detail to the user's commute routine. It might be relevant for future recommendations or interactions about podcasts.   \n",
       "6                                This detail about where extra pens are kept might be useful for future reminders about their workspace or work routine.   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Woke up, showered, and left for work at the usual time. Had toast for breakfast before heading out.  \n",
       "1                                              Traffic was smooth, arrived at work earlier than usual.  \n",
       "2                                     Took the train, had to stand the whole ride since it was packed.  \n",
       "3                                   Stopped by the bakery on the way and picked up bread for the team.  \n",
       "4    Woah! Some guy just came out of nowhere and darted into traffic. That was pretty shocking. Crazy.  \n",
       "5                                                   Listened to a podcast while walking to the subway.  \n",
       "6                                       Grabbed a pen from my desk drawer because mine ran out of ink.  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"tags:\", f\"({len(logs)} total logs, {len(df_resp.tag.unique())} tags, {len(df_resp)} saved)\")\n",
    "pprint(list(df_resp.tag.unique()))\n",
    "df_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec029b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
