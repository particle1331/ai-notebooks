{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c665168",
   "metadata": {},
   "source": [
    "# Augmenting LLMs with Reasoning and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b09e3",
   "metadata": {},
   "source": [
    "AI agents are artificial intelligence systems that interacts with its environments. We will be primarily interested in LLM-based agents that uses a core LLM system for **reasoning**, **planning**, and **tool calling**. In practice, this also involve developing tools that are designed to interface with LLM-based agents. The foundational framework of reasoning (via [chain-of-thought](@fig-cot-prompting)) and task-specific actions is explored in the **ReAct paper** [@ReAct2023] (@fig-react-paper):\n",
    "\n",
    "> ... we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. ... On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c887583",
   "metadata": {},
   "source": [
    "![**ReAct vs CoT only and Act only methods.** [ALFWorld](https://alfworld.github.io/) is a text-based environment (similar to text adventure RPGs in the old days) designed for agents to reason and learn high-level policies. [HotpotQA](https://hotpotqa.github.io/) is a question-answering dataset containing multi-hop (i.e. requiring resolving intermediate steps to get to the final answer) questions in natural language, with strong supervision for supporting facts.](../img/react-paper.png){#fig-react-paper}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b595e",
   "metadata": {},
   "source": [
    "![Chain-of-thought (CoT) prompting.](../img/zero-cot.png){#fig-cot-prompting}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2387513",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## LLM inference via APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ab857",
   "metadata": {},
   "source": [
    "### OpenAI API client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339c546",
   "metadata": {},
   "source": [
    "To explore building effective AI agents, we can start with pure Python and LLM APIs. In particular, we use the [Python SDK](https://github.com/openai/openai-python/tree/main) for the [OpenAI API](https://platform.openai.com/docs/api-reference/introduction). First, we need to load the API key in the environmental variables. The client expects the environmental variable `OPENAI_API_KEY` which we can load from the `.env` file. This is easy to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bfff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def load_dotenv(verbose=False):\n",
      "    with open(\".env\") as f:\n",
      "        for line in f.readlines():\n",
      "            k, v = line.split(\"=\")\n",
      "            os.environ[k] = v.strip().strip('\"')\n",
      "            if verbose:\n",
      "                print(f\"Loaded env variable: {k}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from notebooks.utils import load_dotenv\n",
    "print(inspect.getsource(load_dotenv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d77b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded env variable: OPENAI_API_KEY\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f34dc",
   "metadata": {},
   "source": [
    "Then the API key is automatically read by the **client**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424d6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You're a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me about the history of GPT in a single paragraph.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "response_text = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd2ad0",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "We can have **multiple completions** of the same prompt. This allows choosing between the responses.\n",
    "For example, we can set `temperature=0.9` to get more varied outputs, so that choosing becomes nontrivial. \n",
    "By default only 1 completion by default, hence `[0]`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055bb4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Generative Pre-trained Transformer (GPT) is a series of language models '\n",
      " 'developed by OpenAI. The first iteration, GPT, was introduced in 2018 and '\n",
      " 'showcased the potential of unsupervised pre-training on large text corpora '\n",
      " 'followed by fine-tuning for specific tasks. This was followed by GPT-2 in '\n",
      " '2019, which significantly increased the model size and capabilities but '\n",
      " 'raised concerns about potential misuse, leading to a cautious release. '\n",
      " 'GPT-3, released in 2020, further scaled the model, boasting 175 billion '\n",
      " 'parameters and exhibiting remarkable performance across a wide range of '\n",
      " 'natural language tasks. Building on this, OpenAI introduced GPT-4 in 2023, '\n",
      " 'further enhancing capabilities with improved context understanding and '\n",
      " 'multi-modal inputs, demonstrating a broader applicability in the field of '\n",
      " 'AI. Throughout its iterations, the GPT series has significantly impacted the '\n",
      " 'development and application of AI technologies in natural language '\n",
      " 'processing.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response_text, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4a310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Generative Pre-trained Transformer (GPT) is a series of language models '\n",
      " 'developed by OpenAI. The first iteration, GPT, was introduced in 2018 and '\n",
      " 'showcased the potential of unsupervised pre-training on large text corpora '\n",
      " 'followed by fine-tuning for specific tasks. This was followed by GPT-2 in '\n",
      " '2019, which significantly increased the model size and capabilities but '\n",
      " 'raised concerns about potential misuse, leading to a cautious release. '\n",
      " 'GPT-3, released in 2020, further scaled the model, boasting 175 billion '\n",
      " 'parameters and exhibiting remarkable performance across a wide range of '\n",
      " 'natural language tasks. Building on this, OpenAI introduced GPT-4 in 2023, '\n",
      " 'further enhancing capabilities with improved context understanding and '\n",
      " 'multi-modal inputs, demonstrating a broader applicability in the field of '\n",
      " 'AI. Throughout its iterations, the GPT series has significantly impacted the '\n",
      " 'development and application of AI technologies in natural language '\n",
      " 'processing.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda6e7b",
   "metadata": {},
   "source": [
    "Entire model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55f61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'annotations': [],\n",
      "                          'audio': None,\n",
      "                          'content': 'Generative Pre-trained Transformer (GPT) '\n",
      "                                     'is a series of language models developed '\n",
      "                                     'by OpenAI. The first iteration, GPT, was '\n",
      "                                     'introduced in 2018 and showcased the '\n",
      "                                     'potential of unsupervised pre-training '\n",
      "                                     'on large text corpora followed by '\n",
      "                                     'fine-tuning for specific tasks. This was '\n",
      "                                     'followed by GPT-2 in 2019, which '\n",
      "                                     'significantly increased the model size '\n",
      "                                     'and capabilities but raised concerns '\n",
      "                                     'about potential misuse, leading to a '\n",
      "                                     'cautious release. GPT-3, released in '\n",
      "                                     '2020, further scaled the model, boasting '\n",
      "                                     '175 billion parameters and exhibiting '\n",
      "                                     'remarkable performance across a wide '\n",
      "                                     'range of natural language tasks. '\n",
      "                                     'Building on this, OpenAI introduced '\n",
      "                                     'GPT-4 in 2023, further enhancing '\n",
      "                                     'capabilities with improved context '\n",
      "                                     'understanding and multi-modal inputs, '\n",
      "                                     'demonstrating a broader applicability in '\n",
      "                                     'the field of AI. Throughout its '\n",
      "                                     'iterations, the GPT series has '\n",
      "                                     'significantly impacted the development '\n",
      "                                     'and application of AI technologies in '\n",
      "                                     'natural language processing.',\n",
      "                          'function_call': None,\n",
      "                          'refusal': None,\n",
      "                          'role': 'assistant',\n",
      "                          'tool_calls': None}}],\n",
      " 'created': 1756475595,\n",
      " 'id': 'chatcmpl-C9tpTTVOvJ6PELwwgE76Q3rV2YOmI',\n",
      " 'model': 'gpt-4o-2024-08-06',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': 'default',\n",
      " 'system_fingerprint': 'fp_df0f7b956c',\n",
      " 'usage': {'completion_tokens': 180,\n",
      "           'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                         'audio_tokens': 0,\n",
      "                                         'reasoning_tokens': 0,\n",
      "                                         'rejected_prediction_tokens': 0},\n",
      "           'prompt_tokens': 28,\n",
      "           'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
      "           'total_tokens': 208}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(completion.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff814f09",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "We are using the **chat completions** API where an autoregressive process that's running under the hood. Here the prompt to be completed is:\n",
    "\n",
    "```python\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic but terse assistant.\"},   # prompt\n",
    "    {\"role\": \"user\", \"content\": \"What is the color of the sky?\"}              # prompt\n",
    "]\n",
    "```\n",
    "\n",
    "And the completion is given by the API's output:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"content\": \"The sky's color shifts from azure to amber, a canvas for sun's daily journey.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Remark.** The generated response is statistically the most likely continuation of the prompt text sequence. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c766d",
   "metadata": {},
   "source": [
    "### Structured outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52597344",
   "metadata": {},
   "source": [
    "[Structured outputs](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses) is a feature that ensures that a model generates responses that adhere to a supplied **schema** (e.g. a Pydantic model). As such, the output can then be parsed using the same Pydantic model. Structured outputs makes prompting significantly simpler: no more need for strongly worded prompts to achieve consistent formatting, no explicitly having to retry incorrectly formatted responses, or having invalid hallucinated values (can specify **enums**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70811ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "        },\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc0da2",
   "metadata": {},
   "source": [
    "## Extending LLMs with tools\n",
    "\n",
    "This is the foundational building block of agentic systems. Basically, a core language model service interfaces with **retrieval**, **tools**, and **memory** modules. Since current models are able to effectively plan and reason, this greatly improves the problem solving ability of such systems. Moreover, augmentation is necessary for LLMs to reason about data outside of their training data (i.e. data that is outdated relative to the training cutoff)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb22b7f",
   "metadata": {},
   "source": [
    "![A conceptual diagram of a core LLM service augmented with retrieval, tools, and memory capabilities.](../img/augmented-llm.png){#fig-augmentedllm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98448acb",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "Also known as **tool calling**. Function calling give models access to external tools and data that they can use to respond to prompts. Since LLMs *only* consume and generate text, they cannot actually execute functions. Instead, the main program listens to the LLM hallucinate and executes the commands based on that (@fig-brainvat)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07436a08",
   "metadata": {},
   "source": [
    "![(**right**) LLM as brain in a vat that hallucinates outputs from information contained in inputs. It tells us *what* function to execute with *what* arguments. (**left**) The computer listens to the LLM and performs computation based on it.](../img/llm-brain-vat.png){#fig-brainvat width=80%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833803bf",
   "metadata": {},
   "source": [
    "To show tool calling, we find the weather in [Quisao](https://www.philatlas.com/luzon/r04a/rizal/pililla/quisao.html). We want the agent to call the following API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6bdad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': '2025-08-29T13:45',\n",
       " 'interval': 900,\n",
       " 'temperature_2m': 28.4,\n",
       " 'wind_speed_10m': 3.8,\n",
       " 'relative_humidity_2m': 84,\n",
       " 'precipitation': 0.5,\n",
       " 'precipitation_probability': 25}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    \"\"\"\n",
    "    Get current weather data for provided coordinates with units:\n",
    "    temperature (celsius), wind speed (kph), & precipitation (mm).\n",
    "    \"\"\"\n",
    "    response = requests.get((\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&\"\n",
    "        \"current=temperature_2m,wind_speed_10m,relative_humidity_2m,precipitation,precipitation_probability\"\n",
    "    ))\n",
    "    data = response.json()\n",
    "    return data[\"current\"]\n",
    "\n",
    "\n",
    "get_weather(latitude=14.4779, longitude=121.3214)  # true coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c253083",
   "metadata": {},
   "source": [
    "**Tool definition.** For the LLM to understand a specific tool, we have to define a schema that informs the model of what the tool does and its expected (required and optional) arguments. The following is the function definition for `get_weather`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c9d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\", # <1>\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",  # <2>\n",
    "            \"description\": \"Get current weather data for provided coordinates with units: temperature (celsius), wind speed (kph), & precipitation (mm).\",    # <3>\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\", # <4>\n",
    "                \"properties\": { # <5>\n",
    "                    \"latitude\": {\n",
    "                        \"type\": \"number\"\n",
    "                    },\n",
    "                    \"longitude\": {\n",
    "                        \"type\": \"number\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"latitude\", \"longitude\"],\n",
    "                \"additionalProperties\": False,  # <6>\n",
    "            },\n",
    "            \"strict\": True, # <7>\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f4811",
   "metadata": {},
   "source": [
    "1. Should always be function.\n",
    "2. Function's name (i.e. `get_weather`).\n",
    "3. Usually just the docstring. Should describe when and how to use the function.\n",
    "4. Parameters for LLMs are naturally JSON objects. Because `parameters` is defined by a JSON schema, you can leverage many of its rich features like property types, enums, descriptions, nested objects, and so on. For example, we can have: \n",
    "    ```\n",
    "    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"unit of measure for temperature\"}\n",
    "    ```\n",
    "5. List of arguments. Clearly the two arguments are required.\n",
    "6. Part of JSON schema that determines whether extra fields are valid or not.\n",
    "7. Not part of JSON schema, but OpenAI function calling option that *guides* the model to strictly follow the schema (i.e. not improvise). Setting `additionalProperties` to `False` and `strict` to `True` works together to ensure that the model generates the correct parameters schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014a64d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotations': [],\n",
      " 'audio': None,\n",
      " 'content': None,\n",
      " 'function_call': None,\n",
      " 'refusal': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"latitude\":14.4604,\"longitude\":121.3284}',\n",
      "                              'name': 'get_weather'},\n",
      "                 'id': 'call_niULw1MKbRl6hS96Xe3CgWnN',\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful weather assistant.\"\n",
    "user_prompt = \"What's the weather like in Quisao, Pililla, Rizal right now?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "pprint(completion.choices[0].message.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281f9a4",
   "metadata": {},
   "source": [
    "Very impressive that it's able to get fairly accurate coordinates without using web search. Observe that whenever we have `tools`, the model responds with only `tool_calls` as nonnull (e.g. `content` is empty). \n",
    "We will iterate over tool calls and process them separately. Each function call and their results are then logged as part of the sequence of **chat messages**. This explains why we defined `messages` outside of the API call unlike the usual setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94696d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function(name, args):\n",
    "    fn = {\n",
    "        \"get_weather\": get_weather  \n",
    "    }\n",
    "    return fn[name](**args)\n",
    "\n",
    "\n",
    "assistant_message = completion.choices[0].message\n",
    "messages.append(assistant_message.model_dump())  # <1>\n",
    "\n",
    "for tool_call in assistant_message.tool_calls:\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    name = tool_call.function.name\n",
    "    tool_output = call_function(name, args)\n",
    "    messages.append({    # <2>\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": tool_call.id, \n",
    "        \"content\": json.dumps(tool_output)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14c4e7",
   "metadata": {},
   "source": [
    "1. The tool call is logged with role `assistant`.\n",
    "2. Next, we log the result of the function call with role `tool`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc27b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful weather assistant.', 'role': 'system'},\n",
      " {'content': \"What's the weather like in Quisao, Pililla, Rizal right now?\",\n",
      "  'role': 'user'},\n",
      " {'annotations': [],\n",
      "  'audio': None,\n",
      "  'content': None,\n",
      "  'function_call': None,\n",
      "  'refusal': None,\n",
      "  'role': 'assistant',\n",
      "  'tool_calls': [{'function': {'arguments': '{\"latitude\":14.4604,\"longitude\":121.3284}',\n",
      "                               'name': 'get_weather'},\n",
      "                  'id': 'call_niULw1MKbRl6hS96Xe3CgWnN',\n",
      "                  'type': 'function'}]},\n",
      " {'content': '{\"time\": \"2025-08-29T13:45\", \"interval\": 900, \"temperature_2m\": '\n",
      "             '28.6, \"wind_speed_10m\": 3.8, \"relative_humidity_2m\": 84, '\n",
      "             '\"precipitation\": 0.5, \"precipitation_probability\": 10}',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_niULw1MKbRl6hS96Xe3CgWnN'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af390cce",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Chat completion API calls have stateless single request-response cycles which gives the user straightforward control over the **message history**. This can be seen in the above example where we manually manage message history with tool calls declaration as well as actual function outputs.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f1dff",
   "metadata": {},
   "source": [
    "Next, we pass this thread to another API call (possibly to a different, more specialized model) which will process the outputs of the tool calls along with earlier chat messages. To take advantage of structured outputs we again define a response format. Here we use Pydantic `Field` with a description that helps the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42e9bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    response: str = Field(description=\"A natural language response to the user's question.\")\n",
    "    temperature: float = Field(description=\"Current temperature in celsius for the given location.\")\n",
    "\n",
    "\n",
    "completion_weather = client.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=tools,    # <!>\n",
    "    response_format=WeatherResponse,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d568be6",
   "metadata": {},
   "source": [
    ":::{.callout-caution}\n",
    "The aggregator also needs access to tools for it to understand the context of each tool call!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0fdc2",
   "metadata": {},
   "source": [
    "**Final output.** Note that even units (included in the `get_weather` docstring) were correctly identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f1eda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.6\n",
      "('The current weather in Quisao, Pililla, Rizal is warm with a temperature of '\n",
      " \"28.6°C. There's a light wind blowing at 3.8 kph, and the humidity is quite \"\n",
      " 'high at 84%. There is a slight chance of precipitation, with 0.5 mm observed '\n",
      " 'so far.')\n"
     ]
    }
   ],
   "source": [
    "parsed_weather = completion_weather.choices[0].message.parsed\n",
    "print(parsed_weather.temperature)\n",
    "pprint(parsed_weather.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27b1c4",
   "metadata": {},
   "source": [
    "### Memory and retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76762d",
   "metadata": {},
   "source": [
    "The following is a toy example of an agent that reads and writes to an external data source. One characteristic of retrieval systems is that the entire process is **stateless**, e.g. it cannot learn from interactions. Retrieval systems generally involve queries to an external data source, then adding the response to the current model context.\n",
    "\n",
    "For the following example, the LLM also writes to the same memory store hence affecting future generation states. It follows that this system is **stateful**. We can think of the external memory store as the **long-term memory** of the system. On the other hand, LLMs naturally have **short-term memory** in the form of its context. The architecture is shown in @fig-retrieval-system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd8b60",
   "metadata": {},
   "source": [
    "![The LLM expresses the intent to write to the memory store via the structured output. Then, it is up to the main program to perform the actual writing. This allows hooks like [guardrails](https://cookbook.openai.com/examples/how_to_use_guardrails) to be applied before executing the function. Note that the retrieval happens prior to LLM processing. It would be nice to have the LLM read the entire filestore but this becomes more expensive as the memory store grows. In practice, information retrieval techniques such as TF-IDF and embedding similarity can be used.\n",
    "](../img/retrieval-system.png){#fig-retrieval-system}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ada622",
   "metadata": {},
   "source": [
    "Below, we do keyword search in the retrieval step. So we define a function for removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d4ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Punk is a pre-trained unsupervised machine learning model for tokenization. It's one of the most crucial and widely used components in the NLTK library.\n",
      "Filtered: ['punk', 'pre-trained', 'unsupervised', 'machine', 'learning', 'model', 'tokenization', \"'s\", 'one', 'crucial', 'widely', 'used', 'components', 'nltk', 'library']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize into words then remove stopwords.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return filtered_tokens\n",
    "\n",
    "text = \"Punk is a pre-trained unsupervised machine learning model for tokenization. It's one of the most crucial and widely used components in the NLTK library.\"\n",
    "print(\"Original:\", text)\n",
    "print(\"Filtered:\", tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e31d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class MemoryItem(BaseModel):\n",
    "    tag: str\n",
    "    info: str\n",
    "    reason: str\n",
    "\n",
    "class MemoryResponse(BaseModel):\n",
    "    items: List[MemoryItem]\n",
    "\n",
    "\n",
    "class MemoryStore:\n",
    "    def __init__(self, path=\"memory.json\"):\n",
    "        \"\"\"Load memory from JSON file in local path.\"\"\"\n",
    "        self.path = path\n",
    "        self.data = []\n",
    "        self.tags = set()\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        try:\n",
    "            self.data = json.load(open(self.path))\n",
    "        except FileNotFoundError:\n",
    "            self.reset()\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.path, \"w\") as f:\n",
    "            json.dump(self.data, f, indent=2)\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = []\n",
    "        self.tags = set()\n",
    "        self.save()\n",
    "    \n",
    "    def add(self, item: MemoryItem):\n",
    "        tag, info = item.tag, item.info\n",
    "        self.tags.add(tag)\n",
    "        self.data.append({\"tag\": tag, \"info\": info})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def retrieve(self, query: str, topk: int = 3) -> List[dict]:\n",
    "        \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "        \n",
    "        query_words = tokenize(query)\n",
    "        retrieved = []\n",
    "        \n",
    "        for memory in reversed(self.data):  # <1>\n",
    "            tag, info = memory[\"tag\"], memory[\"info\"]\n",
    "            info = ' '.join(tokenize(info))\n",
    "            memory_text = f\"{tag} {info}\".lower()\n",
    "\n",
    "            for word in query_words:\n",
    "                if word in memory_text: # <2>\n",
    "                    retrieved.append(memory)\n",
    "                    break\n",
    "            \n",
    "            if len(retrieved) == topk:\n",
    "                break\n",
    "        \n",
    "        return retrieved\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "mem = MemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68e9d4",
   "metadata": {},
   "source": [
    "1. More recent = more relevant.\n",
    "2. Substring check. e.g. `'commute' in 'commute_experience'` evaluates to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cc4dd",
   "metadata": {},
   "source": [
    "Next, we define the **LLM generation step** and **write step**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78de9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = lambda memories, tags: f\"\"\"\n",
    "You are an assistant that processes daily user logs. For each log, extract a concise, \n",
    "factual summary of what happened. Each summary should be atomic, standalone, and \n",
    "likely useful for future interactions. Assign a relevant `tag` to each summary (`info`) \n",
    "before saving it to memory. \n",
    "\n",
    "The following are relevant entries (based on the current input) in the Memory Store:\n",
    "{memories}\n",
    "\n",
    "The following are the current tags:\n",
    "{tags}\n",
    "\n",
    "**GUIDELINES:**\n",
    "\n",
    "1. **EXTRACT ATOMIC FACTS:**\n",
    "    - Break down information into the smallest meaningful, self-contained units.\n",
    "    - **Good**: \"User's favorite programmer is Jon Blow.\"\n",
    "    - **Bad**: \"User mentioned their favorite programmer is Jon Blow who is a famous game programmer\" (This has two facts.)\n",
    "    - The `info` must be a concise, direct paraphrase of the fact. Remove conversational fluff.\n",
    "    - **Good Info:** \"User's favorite city is Tokyo\"\n",
    "    - **Bad Info:** \"The user stated that if they had to pick a favorite city, they think it would be Tokyo.\"\n",
    "\n",
    "2.  **TAG EFFECTIVELY:**\n",
    "    - **Format:** Prefer generic, descriptive tags in `snake_case`.\n",
    "    - **Simple:** Prefer simple tags. Choose `commute` is better than `commute_experience`.\n",
    "    - **Reuse:** Strongly prefer existing tags. Create a new tag only if necessary.\n",
    "    - An example: For \"I really enjoy hiking in the Alps every summer,\" a good tag is `hobby` or `outdoor_activity`.\n",
    "    \n",
    "4.  **EVALUATE & DECIDE:**\n",
    "    - It is acceptable to save zero logs from an input if nothing is meaningfully new or relevant.\n",
    "    - Save multiple logs if the user provides multiple distinct pieces of information.\n",
    "    - Each memory item should make sense on its own. There should be no dependence between separate logs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_memory_items(user_input: str, topk: int = 3) -> MemoryResponse:\n",
    "    \"\"\"Generate memory items from current input & existing memory.\"\"\"\n",
    "\n",
    "    retrieved_memories = mem.retrieve(user_input, topk)\n",
    "    current_tags = list(mem.tags)\n",
    "    system_prompt = prompt_template(retrieved_memories, current_tags)\n",
    "\n",
    "    completion = client.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ],\n",
    "        response_format=MemoryResponse,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "\n",
    "def write(user_log: str) -> MemoryResponse:\n",
    "    \"\"\"Write to memory store from user logs.\"\"\"\n",
    "    response = generate_memory_items(user_log)\n",
    "    for item in response.items:\n",
    "        mem.add(item)\n",
    "    \n",
    "    mem.save()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce0933b",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6599b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs = [\n",
    "    \"Woke up, showered, and left for work at the usual time. Had toast for breakfast before heading out.\",\n",
    "    \"Traffic was smooth, arrived at work earlier than usual.\",\n",
    "    \"Took the train, had to stand the whole ride since it was packed.\",\n",
    "    \"Stopped by the bakery on the way and picked up bread for the team.\",\n",
    "    \"Opened my email first thing at the office, mostly routine messages.\",\n",
    "    \"Woah! Some guy just came out of nowhere and darted into traffic. That was pretty shocking. Crazy.\",\n",
    "    \"Listened to a podcast while walking to the subway.\",\n",
    "    \"Grabbed a pen from my desk drawer because mine ran out of ink.\",\n",
    "]\n",
    "\n",
    "items = []\n",
    "for input_text in logs:\n",
    "    for item in write(input_text).items:\n",
    "        d = item.model_dump()\n",
    "        d[\"text\"] = input_text\n",
    "        items.append(d)\n",
    "\n",
    "df_resp = pd.DataFrame(items)\n",
    "mem.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b4ef6",
   "metadata": {},
   "source": [
    "The agent decides whether to reuse a tag or create a new one based on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d4931a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: (8 total logs, 4 tags, 7 saved)\n",
      "['breakfast', 'commute', 'bakery', 'work_routine']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>info</th>\n",
       "      <th>reason</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breakfast</td>\n",
       "      <td>User had toast for breakfast.</td>\n",
       "      <td>The user specifies what they had for breakfast, which is a specific and recurring event, making it useful information.</td>\n",
       "      <td>Woke up, showered, and left for work at the usual time. Had toast for breakfast before heading out.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>commute</td>\n",
       "      <td>User arrived at work earlier than usual due to smooth traffic.</td>\n",
       "      <td>The fact describes the user's commute experience and outcome.</td>\n",
       "      <td>Traffic was smooth, arrived at work earlier than usual.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commute</td>\n",
       "      <td>User had to stand during a packed train ride.</td>\n",
       "      <td>The user shared details about their train commute.</td>\n",
       "      <td>Took the train, had to stand the whole ride since it was packed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bakery</td>\n",
       "      <td>User picked up bread from the bakery for the team.</td>\n",
       "      <td>This information specifies an activity and location related to food purchasing.</td>\n",
       "      <td>Stopped by the bakery on the way and picked up bread for the team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>work_routine</td>\n",
       "      <td>User opened their email first thing at the office to check routine messages.</td>\n",
       "      <td>This statement provides insight into the user's work routine and morning priorities at the office.</td>\n",
       "      <td>Opened my email first thing at the office, mostly routine messages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>commute</td>\n",
       "      <td>User listened to a podcast while walking to the subway.</td>\n",
       "      <td>This is a standalone fact about the user's commute routine, relevant under the existing commute tag.</td>\n",
       "      <td>Listened to a podcast while walking to the subway.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>work_routine</td>\n",
       "      <td>User replaced their pen after it ran out of ink.</td>\n",
       "      <td>This action is notable as it reflects a routine response to office supplies running out.</td>\n",
       "      <td>Grabbed a pen from my desk drawer because mine ran out of ink.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tag  \\\n",
       "0     breakfast   \n",
       "1       commute   \n",
       "2       commute   \n",
       "3        bakery   \n",
       "4  work_routine   \n",
       "5       commute   \n",
       "6  work_routine   \n",
       "\n",
       "                                                                           info  \\\n",
       "0                                                 User had toast for breakfast.   \n",
       "1                User arrived at work earlier than usual due to smooth traffic.   \n",
       "2                                 User had to stand during a packed train ride.   \n",
       "3                            User picked up bread from the bakery for the team.   \n",
       "4  User opened their email first thing at the office to check routine messages.   \n",
       "5                       User listened to a podcast while walking to the subway.   \n",
       "6                              User replaced their pen after it ran out of ink.   \n",
       "\n",
       "                                                                                                                   reason  \\\n",
       "0  The user specifies what they had for breakfast, which is a specific and recurring event, making it useful information.   \n",
       "1                                                           The fact describes the user's commute experience and outcome.   \n",
       "2                                                                      The user shared details about their train commute.   \n",
       "3                                         This information specifies an activity and location related to food purchasing.   \n",
       "4                      This statement provides insight into the user's work routine and morning priorities at the office.   \n",
       "5                    This is a standalone fact about the user's commute routine, relevant under the existing commute tag.   \n",
       "6                                This action is notable as it reflects a routine response to office supplies running out.   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Woke up, showered, and left for work at the usual time. Had toast for breakfast before heading out.  \n",
       "1                                              Traffic was smooth, arrived at work earlier than usual.  \n",
       "2                                     Took the train, had to stand the whole ride since it was packed.  \n",
       "3                                   Stopped by the bakery on the way and picked up bread for the team.  \n",
       "4                                  Opened my email first thing at the office, mostly routine messages.  \n",
       "5                                                   Listened to a podcast while walking to the subway.  \n",
       "6                                       Grabbed a pen from my desk drawer because mine ran out of ink.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"tags:\", f\"({len(logs)} total logs, {len(df_resp.tag.unique())} tags, {len(df_resp)} saved)\")\n",
    "pprint(list(df_resp.tag.unique()))\n",
    "df_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23dd8f",
   "metadata": {},
   "source": [
    "## Agent Supervisor Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8710ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "import functools\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4527f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_87143/2934351315.py:28: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  analyze_sentiment(\"Wish people understood how draining it is to explain the same thing over and over.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Detecting Tool Definition\n",
    "sentiment_analysis_template = \"\"\"\n",
    "You are a cutting edge emotion sentiment classification assistant.\\\n",
    "You analyze a social media comment, and apply one sentiment label to it. \\\n",
    "The sentiment labels are simple positive, neutral, and negative.\n",
    "Your output should simply be just the respective sentiment. \\\n",
    "\n",
    "The comment is here: {comment}\n",
    "\"\"\"\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o\")\n",
    "sentiment_analysis_prompt = ChatPromptTemplate.from_template(sentiment_analysis_template)\n",
    "\n",
    "sentiment_chain = (\n",
    "    {\"comment\": RunnablePassthrough()}\n",
    "    | sentiment_analysis_prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "@tool\n",
    "def analyze_sentiment(query: str) -> str:\n",
    "    \"\"\"Analyze the sentiment of a string of text\"\"\"\n",
    "    sentiment = sentiment_chain.invoke(query)\n",
    "    return sentiment\n",
    "\n",
    "analyze_sentiment(\"Wish people understood how draining it is to explain the same thing over and over.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c3de5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
