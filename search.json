[
  {
    "objectID": "topics/deep/index.html",
    "href": "topics/deep/index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep learning systems solved problems that were considered hard prior to 2010, e.g. obtaining superhuman / SOTA scores on tasks and challenges such as ImageNet, CASP, and Go (board game)1. Later, in the early 2020s, unprecedented progress in text & image generation were made with models like GPT-3 and Stable Diffusion:",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/index.html#learning-objectives",
    "href": "topics/deep/index.html#learning-objectives",
    "title": "Deep Learning",
    "section": "Learning objectives",
    "text": "Learning objectives\nThis course will provide you will an introduction to the functioning of modern deep learning systems. You will learn about the underlying concepts of modern deep learning systems like automatic differentiation, neural network architectures, optimization, and efficient operations on systems like GPUs.\n\nUnderstand the basic functioning of modern deep learning libraries\nIncluding concepts like automatic differentiation, gradient-based optimization\nImplement standard DL architectures (MLPs, ConvNets, RNNs, Seq2Seq, Transformers)",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/index.html#references",
    "href": "topics/deep/index.html#references",
    "title": "Deep Learning",
    "section": "References",
    "text": "References\n\nCMU 10-414/714: Deep Learning Systems (2022)",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/index.html#footnotes",
    "href": "topics/deep/index.html#footnotes",
    "title": "Deep Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGame tree complexity of 10^{360} at 250 moves over 150 move games.↩︎",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/02.html",
    "href": "topics/deep/02.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Recall that a hypothesis function (i.e. a choice of architecture) h\\colon \\mathbb{R}^d \\to \\mathbb{R}^K maps input data to desired outputs. Initially, we used a linear hypothesis class h_\\Theta(\\mathbf{x}) = \\Theta^\\top \\mathbf{x} where \\Theta \\in \\mathbb{R}^{d \\times K}. This forms K linear functions of the input and predicts the class with the largest value. This turns out to be equivalent to partitioning the input space into K linear convex regions corresponding to each class.\nRemark. In \\mathbb{R}^2 with 3 classes, we have 2 inequality constraints \\theta_1^\\top \\mathbf{x} \\geq \\theta_2^\\top \\mathbf{x} and \\theta_1^\\top \\mathbf{x} \\geq \\theta_3^\\top \\mathbf{x}. This resuts in a convex polyhedron which is the intersection of two linear half-spaces. Similarly, for the other two. It can be shown that the interior of these subsets are disjoint and the union of the three cover all of \\mathbb{R}^2.\nQ. What about data that are not linearly separable? We want some way to separate these points via a nonlinear set of class boundaries.\nimport math\nimport numpy as np\n\nN = 100\nr_eps = 0.3\n\ndef circle_data(radius: float, r_eps=r_eps, num_points=N):\n    r0 = radius\n    t = 2 * math.pi * np.random.random(N)\n    r = r0 + r_eps * np.random.randn(N)\n    x, y = r * np.cos(t), r * np.sin(t)\n    return x, y\n\nx0, y0 = circle_data(3)\nx1, y1 = circle_data(5)\nx2, y2 = circle_data(1)\nCode\n%config InlineBackend.figure_formats = [\"svg\"] \nimport matplotlib.pyplot as plt\n\nplt.scatter(x0, y0, edgecolor=\"k\")\nplt.scatter(x1, y1, marker=\"*\", edgecolor=\"k\", s=100)\nplt.scatter(x2, y2, marker=\"s\", edgecolor=\"k\")\nplt.axis(\"equal\");",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#nonlinear-features",
    "href": "topics/deep/02.html#nonlinear-features",
    "title": "Neural Networks",
    "section": "Nonlinear features",
    "text": "Nonlinear features\nOne idea: Apply a linear classifier to some (potentially higher-dimensional) features of the data:\nh_\\Theta(\\mathbf{x}) = \\Theta^\\top \\phi(\\mathbf{x})\nwhere \\Theta \\in \\mathbb{R}^{h \\times K} and \\phi\\colon \\mathbb{R}^d \\to \\mathbb{R}^h is a feature transformation.\nExample. For the above dataset, we can define \\phi(x, y) = (x, y, x^2 + y^2) (i.e. r^2) which makes the dataset separable in \\mathbb{R}^3:\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(x0, y0, x0 ** 2 + y0 ** 2, edgecolor=\"k\")\nax.scatter(x1, y1, x1 ** 2 + y1 ** 2, marker=\"*\", edgecolor=\"k\", s=100)\nax.scatter(x2, y2, x2 ** 2 + y2 ** 2, marker=\"s\", edgecolor=\"k\");\n\n\n\n\n\n\n\n\nQ. How do we create the features?\n\nManual feature engineering (see above).\nIn a way that \\phi itself is learned from data (i.e. \\phi parametric).\n\nNote that \\phi linear doesn’t work since we just get a linear classifier. If \\phi(\\mathbf{x}) = \\Phi^\\top \\mathbf{x} where \\Phi \\in \\mathbb{R}^{d \\times h}, then\n\n\\begin{aligned}\nh_\\Theta(\\mathbf{x})\n&= \\Theta^\\top \\phi(\\mathbf{x}) \\\\\n&= \\Theta^\\top \\Phi^\\top \\mathbf{x} = (\\Phi \\Theta)^\\top \\mathbf{x}\n\\end{aligned}\n\nThus, \\phi must be nonlinear. It turns out that applying a univariate nonlinear function \\sigma\\colon \\mathbb{R} \\to \\mathbb{R} called an activation function suffices to create rich hypothesis classes (see Section 5). Hence, we set\n\\phi(\\mathbf{x}) = \\sigma(\\Phi^\\top \\mathbf{x}).\nRemark. More precisely, we require \\sigma to have a good range of values and almost everywhere differentiability to be usable. The term “activation” has roots in neuroscience where it represents the action potential firing in the cell. Finally, observe that we can feature transform features, i.e. compose feature transformations. This is the main idea behind deep networks as we will see below.",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#neural-networks",
    "href": "topics/deep/02.html#neural-networks",
    "title": "Neural Networks",
    "section": "Neural networks",
    "text": "Neural networks\nA neural network1 refers to a particular type of hypothesis class, consisting of multiple, parameterized differentiable functions (a.k.a. “layers”) composed together in any manner to form the output. Since neural networks involve composing a lot of functions (sometimes hundreds), it is usually referred to as deep neural networks, although there is really no requirement on depth beyond being not linear.\n\nTwo-layer neural network\nThe simplest form of neural network is basically just the nonlinear features presented earlier:\n\n\\begin{aligned}\nh_\\Theta(\\mathbf{X}) = \\sigma ( \\mathbf{X} \\mathbf{W_1}) \\mathbf{W}_2\n\\end{aligned}\n\nwhere \\Theta = \\{ \\mathbf{W}_1 \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_2 \\in \\mathbb{R}^{h \\times K} \\} are called the trainable parameters of the network and \\sigma\\colon \\mathbb{R} \\to \\mathbb{R} is the activation function that is applied elementwise to each vector. A commonly used one is ReLU defined as \\sigma(z) = \\max(0, z):\n\nz = np.linspace(-10, 10, 1000)\nplt.figure(figsize=(5, 2))\nplt.plot(z, np.clip(z, a_min=0, a_max=None), label=\"relu(z)\", linewidth=2, color=\"k\")\nplt.grid(alpha=0.6, linestyle=\"dotted\"); plt.ylim(-1, 11); plt.legend();\n\n\n\n\n\n\n\n\n\n\nFully-connected deep networks\nObserve that for the 2-layer network, we have |\\Theta| = 2. A more generic form is the L-layer neural network, sometimes called a multi-layer perceptron (MLP), feedforward network (FFN), or fully-connected network (FC) written in batch form as:\n\nh_\\Theta(\\mathbf{X}) = \\sigma(\\sigma(\\ldots \\sigma(\\mathbf{X}\\mathbf{W}_1)\\mathbf{W}_2 \\ldots) \\mathbf{W}_{L-1}) \\mathbf{W}_L\n\nor\n\n\\begin{aligned}\n\\mathbf{Z}_0 &= \\mathbf{X} \\\\\n\\mathbf{Z}_{i} &= \\sigma_i (\\mathbf{Z}_{i-1} \\mathbf{W}_i), \\quad \\forall i = 1, \\ldots, L \\\\\nh_\\Theta(\\mathbf{X}) &= \\mathbf{Z}_{L}\n\\end{aligned}\n\nwhere \\mathbf{Z}_i \\in \\mathbb{R}^{N \\times d_i} and \\mathbf{W}_i \\in \\mathbb{R}^{d_{i-1} \\times d_{i}}, d_0 = d and d_{L} = K, and \\sigma_i: \\mathbb{R} \\to \\mathbb{R} are applied elementwise. The weights of the network is given by \\Theta = \\{ \\mathbf{W}_1, \\ldots, \\mathbf{W}_L \\}, so that |\\Theta| = L. Also, we typically set \\sigma_L = \\text{Id} for the output layer.\nRemark. Again a bias term can be added. But in theoretical analysis we can just think there is an extra column containing ones to simplify the computation. The index i = 1, \\ldots, L corresponds to the number of layers applied to the input. In particular, the output layer is indexed L since we apply L transformations to the input indexed 0.\nExample. Shapes for a 3-layer neural network which classifies inputs in \\mathbb{R}^{16} into 10 classes:\n\n\n\n\ni\nd_{i-1}\nd_{i}\nPyTorch object\n\n\n\n\n1\n16\n32\nLinear(16, 32)\n\n\n2\n32\n64\nLinear(32, 64)\n\n\n3\n64\n10\nLinear(64, 10)",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#backpropagation",
    "href": "topics/deep/02.html#backpropagation",
    "title": "Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\nRecall that to train the linear function via softmax regression and SGD, we had to calculate the gradients across cross-entropy and the matrix product. For a deep neural network, we need to calculate the gradient for each \\mathbf{W}_i \\in \\Theta, i.e. across each layer i = 1, 2, \\ldots, L. Since the gradients are calculated from the loss to the inputs, this part of the computation is called backward pass. The algorithm for caching intermediate results and accumulating the gradients is collectively called backpropagation (BP) or simply “backprop”.\nThe best way to understand and calculate backprop is to view neural nets as computational graphs with certain defined operations (e.g. entire layers, or lower-level tensor operations):\n\n\nCode\nfrom graphviz import Digraph\n\ndot = Digraph(format=\"png\")\ndot.attr(rankdir=\"LR\")\ndot.node_attr.update(shape=\"box\", style=\"rounded,filled\", fontsize=\"10\", fontname=\"Helvetica\")\ndot.node(\"X\", \"X\", fillcolor=\"lightgray\")\ndot.node(\"W1\", \"W₁\", fillcolor=\"lightyellow\")\ndot.node(\"Z1\", \"Z₁ = σ(XW₁)\", fillcolor=\"lightgreen\")\ndot.node(\"W2\", \"W₂\", fillcolor=\"lightyellow\")\ndot.node(\"Z2\", \"Z₂ = Z₁W₂\", fillcolor=\"lightgreen\")\ndot.node(\"loss\", \"CE loss\", fillcolor=\"lightblue\")\ndot.node(\"Y\", \"Y\", fillcolor=\"lightgrey\")\n\ndot.edges([(\"X\", \"Z1\"), (\"W1\", \"Z1\"), (\"Z1\", \"Z2\"), (\"W2\", \"Z2\"), (\"Z2\", \"loss\"), (\"Y\", \"loss\")])\ndot\n\n\n\n\n\n\n\n\nFigure 1: A computational graph of a two-layer neural network with softmax regression.\n\n\n\n\n\n\nGradients of a 2-layer NN\nConsider a two-layer neural network for softmax regression. Our goal is to find \\nabla_{\\mathbf{W}_i} \\mathcal{L}(h_\\Theta(\\mathbf{X}), \\mathbf{y}) for i = 1, 2. Let’s write this as \\mathbf{Z}_1 = \\sigma(\\mathbf{X} \\mathbf{W}_1) and \\mathbf{Z}_2 = \\mathbf{Z_1}\\mathbf{W}_2. First, we branch towards the linear dependencies of \\mathbf{Z}_2:\n\n\\boxed{\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2}\n&= \\frac{1}{B} (\\mathbf{P} - \\mathbf{E}_\\mathbf{y}) \\\\[0.75em]\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_2}\n&=\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2}\n\\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{W}_2}\n=\n\\frac{1}{B}\\mathbf{Z}_1^\\top (\\mathbf{P} - \\mathbf{E}_\\mathbf{y}) \\\\[0.75em]\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_1} &=\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2}\n\\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{Z}_1} =\n\\frac{1}{B}\n(\\mathbf{P} - \\mathbf{E}_\\mathbf{y})\n\\mathbf{W}_2^\\top.\n\\end{aligned}\n}\n\nFor the second equation, LHS has (d_1, K), while RHS has (d_1, B) \\times (B, K). OK. The last formula is obtained by considering a single instance: {\\partial z_2^{j}}/{\\partial z_1^{i}} = w^{ij}_2 = (w^\\top_2)^{ji}. This has the expected shape (B, K) \\times (K, d_1) = (B, d_1).\nNext, we calculate the gradient across \\sigma. Here we are taking the derivative of d_1 functions with respect to the weight tensor of shape (d, d_1). Let \\mathbf{U}_1 = \\mathbf{X}\\mathbf{W}_1, so that \\mathbf{Z}_1 = \\sigma(\\mathbf{U}_1). Then,\n\n\\begin{align*}\n\\frac{\\partial {z}_1^j}{\\partial w_1^{ik}}\n&= \\sum_l\n\\frac{\\partial z_1^j}{\\partial u_1^l}\n\\frac{\\partial u_1^l}{\\partial w_1^{ik}}  \\\\\n&= \\sum_l \\delta^{jl} \\sigma^\\prime(u_1^l) \\, x^i \\delta^{lk} = \\delta^{jk} \\sigma^\\prime(u_1^j) \\, x^i.\n\\end{align*}\n\\tag{1}\nHere \\delta refers to the Kronecker delta. The two Kronecker delta “contracted” into one. The effect of \\delta^{jk} is that \\sigma^\\prime(u^j) essentially multiplies element-wise to the incoming gradient. Moreover, we aggregate the contribution of the batch instances by multiplying \\mathbf{X}^\\top. Thus, in batch form:\n\n\\boxed{\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_1}\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_1} \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1} \\\\[0.7em]\n&= \\mathbf{X}^\\top \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_1} \\odot \\sigma^\\prime(\\mathbf{X} \\mathbf{W}_1) \\right) \\\\[0.7em]\n&= \\frac{1}{B} \\mathbf{X}^\\top \\Big( [(\\mathbf{P} - \\mathbf{E}_\\mathbf{y})\n\\mathbf{W}_2^\\top ] \\odot \\sigma^\\prime(\\mathbf{X} \\mathbf{W}_1) \\Big).\n\\end{aligned}\n}\n\n\n\nGraph visualization\nBackward dependence can be inspected using the torchviz library. The graph shows the tensors being stored (🟧) during forward pass to compute the gradients. The weights (🟦) are instances of leaf tensors, i.e. the outermost nodes with no parent nodes. Hence, backward iteration stops at the leaf nodes. It follows that calculating gradients require:\n\ntwice the memory of forward pass due to caching\nsame time complexity \\mathcal{O}(S) where S is the network size.\n\n\nfrom torchviz import make_dot\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nmodel = nn.Sequential(\n    nn.Linear(16, 32, bias=False),\n    nn.ReLU(),\n    nn.Linear(32, 10, bias=False)\n)\n\nB = 8\nx = torch.randn(B, 16)\nz = model(x)\ny = torch.randint(0, 10, size=(B,))\nloss = F.cross_entropy(z, y)\n\nmake_dot(loss.mean(), params=dict(model.named_parameters()), show_saved=True)\n\n\n\n\n\n\n\n\n\n\nGeneral formula for L-layers\nFor an L-layer network, we can traverse along the \\mathbf{Z}_i’s to find \\partial{\\mathcal{L}} / \\partial{\\mathbf{Z_i}} by taking the product with the local gradients \\partial{\\mathbf{Z}_{i}}/\\partial{\\mathbf{Z}_{i-1}}. This can be thought of as traversing the main trunk of the network (Figure 1). Meanwhile, the gradient for the weights (leaf tensors) can be calculated using the incoming gradients and \\partial{\\mathbf{Z}_{i}}/\\partial{\\mathbf{W}_{i}} (see Equation 1). Calculating \\partial{\\mathbf{Z}_{i}}/\\partial{\\mathbf{Z}_{i-1}} can be done similarly as follows. Observe that indices check out:\n\n\\begin{aligned}\n\\frac{\\partial {z}_i^j}{\\partial {z}_{i-1}^k}\n&= \\sum_l\n\\frac{\\partial {z}_i^j}{\\partial u_i^l}\n\\frac{\\partial u_i^l}{\\partial {z}_{i-1}^k}  \\\\\n&= \\sum_l \\delta^{jl} \\sigma^\\prime(u_i^l) \\, w_{i}^{kl} = \\sigma^\\prime(u_i^j) \\, ({w^\\top})^{jk}.\n\\end{aligned}\n\nLet \\mathbf{G}_{i} \\coloneqq \\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathbf{Z}_{i}}} with shape (B, d_{i}). Then for i = L, \\ldots, 1:\n\n\\boxed{\n\\begin{aligned}\n\\mathbf{G}_{L} &= \\frac{1}{B}(\\mathbf{P} - \\mathbf{E}_\\mathbf{y}) \\quad \\quad \\;\\text{(cross-entropy)} \\\\\n\\mathbf{G}_{i-1} &=\n    \\mathbf{G}_{i}\n    \\frac{\\partial \\mathbf{Z}_{i}}{\\partial \\mathbf{Z}_{i-1}}\n    = [\n        \\mathbf{G}_{i} \\odot \\sigma_i^\\prime(\\mathbf{Z}_{i-1} \\mathbf{W}_i)\n    ] \\mathbf{W}_i^\\top \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{i}}\n&= \\mathbf{G}_{i}\n\\frac{\\partial \\mathbf{Z}_{i}}{\\partial \\mathbf{W}_{i}}\n= \\mathbf{Z}_{i-1}^\\top[\\mathbf{G}_{i} \\odot \\sigma_i^\\prime(\\mathbf{Z}_{i-1} \\mathbf{W}_i)]\n\\end{aligned}\n}\n\nThis looks very compact. You can verify that the shapes are correct and that it is consistent with the equations for a two-layer network with \\sigma_2 = \\text{Id}. The formulas essentially describe the flow of gradients from the loss to the input. However, each update relies only on the gradients from the next layer.\nRemark. The gradients are modulated by \\sigma_i^\\prime and then multiplied by the weights. This is analogous to forward pass, but involves only basic operations (\\odot, MATMUL). Also, the equations have the form loss_grad x local_grad where the loss gradient \\mathbf{G}_{i} from the next layer is, in a sense, “global” in contrast to the “local” gradients between adjacent nodes. This pattern is quite general as we will see in the next chapter.",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#appendix-gradient-check-using-pytorch",
    "href": "topics/deep/02.html#appendix-gradient-check-using-pytorch",
    "title": "Neural Networks",
    "section": "Appendix: Gradient check using PyTorch",
    "text": "Appendix: Gradient check using PyTorch\nCalculating manually the gradients of a 3-layer neural network:\n\n# network parameters\nw1 = torch.randn(16, 64, requires_grad=True)\nw2 = torch.randn(64, 32, requires_grad=True)\nw3 = torch.randn(32, 10, requires_grad=True)\n\n# forward pass\nB  = 8\nx  = torch.randn(B, 16)\nz1 = torch.tanh(x @ w1)\nz2 = torch.tanh(z1 @ w2)\nz3 = z2 @ w3\n\nfor u in [w1, w2, w3, z1, z2, z3]:\n    u.retain_grad()\n\nh = z3\ny = torch.randint(0, 10, size=(B,))\nloss = F.cross_entropy(h, y)\nloss.backward()\n\nBackward pass:\n\n# walking down the trunk of the network\ng3 = (F.softmax(h, dim=1) - F.one_hot(y, num_classes=10)) / B\ng2 = g3 @ w3.T\ng1 = (g2 * (1 - z2 ** 2)) @ w2.T\n\n# branching to the leaf tensors (weights)\nz0 = x\ndw3 = z2.T @ g3\ndw2 = z1.T @ (g2 * (1 - z2 ** 2))\ndw1 = z0.T @ (g1 * (1 - z1 ** 2))\n\nNote that shapes are equal, otherwise we can’t subtract:\n\nerrors = []\n\nerrors.append(torch.abs(g3 - z3.grad).max().item())\nerrors.append(torch.abs(g2 - z2.grad).max().item())\nerrors.append(torch.abs(g1 - z1.grad).max().item())\n\nerrors.append(torch.abs(dw1 - w1.grad).max().item())\nerrors.append(torch.abs(dw2 - w2.grad).max().item())\nerrors.append(torch.abs(dw3 - w3.grad).max().item())\n\nprint(f\"Max absolute error: {max(errors):.2e}\")\n\nMax absolute error: 1.19e-07",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#sec-univapprox",
    "href": "topics/deep/02.html#sec-univapprox",
    "title": "Neural Networks",
    "section": "Appendix: Universal approximation",
    "text": "Appendix: Universal approximation\n[Cybenko 1989]. It turns out that any continuous map f\\colon \\Omega \\subset \\mathbb{R}^d \\to \\mathbb{R}^m defined on a compact set \\Omega can be approximated by a 1-layer (wide) fully-connected network. Continuity on a compact domain is a reasonable assumption about a ground truth function that we assume exists.\nDemo. Approximating a one-dimensional curve with a ReLU network:\n\nimport torch\n\n# Ground truth\nx = torch.linspace(-2 * torch.pi, 2 * torch.pi, 1000)\ny = torch.sin(x) + 0.3 * x\n\n# Get sorted sample. Shifted for demo\nB = sorted(torch.randint(30, 970, size=(24,)))\nxs = x[B,]\nys = y[B,]\n\n# ReLU approximation\nz = torch.zeros(1000,) + ys[0]\nfor i in range(len(xs) - 1):\n    if torch.isclose(xs[i + 1], xs[i]):\n        m = torch.tensor(0.0)\n    else:\n        m = (ys[i+1] - ys[i]) / (xs[i+1] - xs[i])\n    z += m * (torch.relu(x - xs[i]) - torch.relu(x - xs[i+1]))\n\nNOTE: This only works for target f with compact domain [a, b] consistent with the theorem.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(7, 3))\nax[0].scatter(xs, ys, facecolor=\"none\", s=12, edgecolor=\"k\", zorder=3, label=\"data\")\nax[0].plot(x, y, color=\"C1\", label=\"f\")\nax[0].set_xlabel(\"x\")\nax[0].set_ylabel(\"y\")\nax[0].legend(loc=\"upper left\")\n\nax[1].scatter(xs, ys, facecolor=\"none\", s=12, edgecolor=\"k\", zorder=4, label=\"data\")\nax[1].plot(x, z, color=\"C0\", label=f\"relu approx. (B={len(B)})\", zorder=3)\nax[1].plot(x, y, color=\"C1\")\nax[1].set_xlabel(\"x\")\nax[1].set_ylabel(\"y\")\nax[1].legend(loc=\"lower right\", fontsize=7.5)\nfig.tight_layout();",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#appendix-linearly-separable-features",
    "href": "topics/deep/02.html#appendix-linearly-separable-features",
    "title": "Neural Networks",
    "section": "Appendix: Linearly separable features",
    "text": "Appendix: Linearly separable features\nAn ideal classifier is that where the sequence of layers transform the input \\mathbf{x}_i \\in \\mathbb{R}^{d} into data points F_{\\Phi}(\\mathbf{x}_i) \\in \\mathbb{R}^{d_{L-1}} that is linearly separable. That is, we essentially extend linear classification to input that is not linearly separable. This explains why the final layer has no activation, i.e. we can think of the network f as\nf_{(\\Theta, \\Phi)} = h_{\\Theta} \\circ F_{\\Phi}\nwhere h_\\Theta \\colon \\mathbb{R}^{d_{L-1}} \\to \\mathbb{R}^K is a linear hypothesis, while the earlier L-1 layers are feature extractors that compose F_{\\Phi}.\nDemo. Generating a dataset in \\mathbb{R}^2. Our goal is to separate this with a plane in \\mathbb{R}^3.\n\nimport torch\ntorch.manual_seed(2)\n\ndef generate_data(M: int):\n    noise = lambda e: torch.randn(M, 2) * e\n    t = 2 * torch.pi * torch.rand(M, 1)\n    s = 2 * torch.pi * torch.rand(M, 1)\n\n    x0 = torch.cat([0.3 * torch.cos(s), 0.3 * torch.sin(s)], dim=1) + noise(0.2)\n    x1 = torch.cat([3.0 * torch.cos(t), 3.0 * torch.sin(t)], dim=1) + noise(0.3)\n    y0 = (torch.ones(M,) * 0).long()\n    y1 = (torch.ones(M,) * 1).long()\n\n    return x0, y0, x1, y1\n\n\nx0, y0, x1, y1 = generate_data(1500)\n\nLet’s do a simple 2-layer neural net where the feature extractor maps to \\mathbb{R}^3:\n\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Linear(2, 3), nn.Tanh(),\n    nn.Linear(3, 2)\n)\n\nModel training:\n\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\noptim = torch.optim.SGD(model.parameters(), lr=0.01)\n\nx = torch.cat([x0, x1])\ny = torch.cat([y0, y1])\nhistory = {\"accs\": [], \"loss\": []}\nfor step in tqdm(range(15000)):\n    s = model(x)\n    loss = F.cross_entropy(s, y)\n    loss.backward()\n    optim.step()\n    optim.zero_grad()\n    history[\"loss\"].append(loss.item())\n    history[\"accs\"].append(100 * (y == torch.argmax(s, dim=1)).float().mean())\n\n100%|██████████| 15000/15000 [00:07&lt;00:00, 1898.47it/s]\n\n\n\n\nCode\nfig, ax1 = plt.subplots(figsize=(8, 3))\nax2 = ax1.twinx()\n\nax1.plot(history[\"loss\"], color=\"blue\", linewidth=2)\nax2.plot(history[\"accs\"], color=\"red\",  linewidth=2)\nax1.set_xlabel(\"step\")\nax1.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(3, 3))\nax1.grid(axis=\"both\", linestyle=\"dotted\", alpha=0.8)\n\nax1.set_ylabel(\"Batch loss\")\nax2.set_ylabel(\"Batch accs (%)\")\nax1.yaxis.label.set_color(\"blue\")\nax2.yaxis.label.set_color(\"red\");\n\n\n\n\n\n\n\n\n\nObserve that accuracy trend does not exactly match the steadily decreasing loss. This is expected since accuracy considers hard labels, while the loss is calculated with respect to soft probability distributions.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# transformations\nwith torch.no_grad():\n    linear_0 = model[0](x0)\n    linear_1 = model[0](x1)\n    linear_act_0 = model[1](model[0](x0))\n    linear_act_1 = model[1](model[0](x1))\n\n    # separating hyperplane (see above discussion, i.e. w &lt;- w1 - w0  == logistic reg)\n    h = 1\n    w, b = model[2].parameters()\n    w, b = (w[h] - w[h-1]), (b[h] - b[h-1])\n\n# plot\nfig = plt.figure(figsize=(8, 3))\nax0 = fig.add_subplot(131)\nax1 = fig.add_subplot(132, projection=\"3d\")\nax2 = fig.add_subplot(133, projection=\"3d\")\n\nax0.grid(alpha=0.8, linestyle=\"dotted\")\nax0.set_axisbelow(True)\nax0.scatter(x0[:, 0], x0[:, 1], s=2.0, label=0, color=\"C0\")\nax0.scatter(x1[:, 0], x1[:, 1], s=2.0, label=1, color=\"C1\")\nax0.set_xlabel(\"$x_1$\")\nax0.set_ylabel(\"$x_2$\")\nax0.set_xlim(-1.5, 1.5)\nax0.set_ylim(-1.5, 1.5)\nax0.set_title(\"(a) input\")\nax0.legend()\nax0.set_facecolor(\"whitesmoke\")\nax0.axis(\"equal\")\n\nax1.scatter(linear_0[:, 0], linear_0[:, 1], linear_0[:, 2], s=3, label=0, color=\"C0\", alpha=0.8)\nax1.scatter(linear_1[:, 0], linear_1[:, 1], linear_1[:, 2], s=3, label=1, color=\"C1\", alpha=0.8)\nax1.set_xlabel(\"$x_1$\")\nax1.set_ylabel(\"$x_2$\")\nax1.set_zlabel(\"$x_3$\")\nax1.set_title(\"(b) linear\")\n\nax2.scatter(linear_act_0[:, 0], linear_act_0[:, 1], linear_act_0[:, 2], s=3, label=0, color=\"C0\")\nax2.scatter(linear_act_1[:, 0], linear_act_1[:, 1], linear_act_1[:, 2], s=3, label=1, color=\"C1\")\nax2.set_xlabel(\"$x_1$\")\nax2.set_ylabel(\"$x_2$\")\nax2.set_zlabel(\"$x_3$\")\nax2.set_title(\"(c) linear + tanh\")\n\n# Generate grid of points\nx_min = min(linear_act_1[:, 0].min(), linear_act_0[:, 0].min())\nx_max = max(linear_act_1[:, 0].max(), linear_act_0[:, 0].max())\ny_min = min(linear_act_1[:, 1].min(), linear_act_0[:, 1].min())\ny_max = max(linear_act_1[:, 1].max(), linear_act_0[:, 1].max())\na, b, c, d = w[0], w[1], w[2], b\nx = np.linspace(x_min, x_max, 50)\ny = np.linspace(y_min, y_max, 50)\nX, Y = np.meshgrid(x, y)\nZ = (-a * X - b * Y - d) / c\n\n# Plot the hyperplane for the positive class\nax2.plot_surface(X, Y, Z, alpha=0.5, color=f\"C{h}\")\nfig.tight_layout();\n\n\n\n\n\n\n\n\n\nRemark. The last linear layer (the “logits”) defines the separating hyperplane.\nPredicting on \\mathbb{R}^2:\n\n# Create a grid of points\nN = 300\nx = np.linspace(-6, 6, N)\ny = np.linspace(-4, 4, N)\nX, Y = np.meshgrid(x, y)\n\n# Calculate p[1] for each point in grid\ninp = torch.tensor(list(zip(X, Y)), dtype=torch.float32).permute(0, 2, 1).reshape(-1, 2)\nout = F.softmax(model(inp), dim=1)\nZ = out[:, 1].reshape(300, 300).detach().numpy()\n\nThe plot below is obtained by applying the model on each point in the grid. The coloring essentially represents the model on test data \\mathbf{x} \\in \\mathbb{R}^2. Notice that we essentially have a nonlinear decision boundary:\n\n\nCode\n# create a color plot\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Define custom colormap\ncolors = [\"C0\", \"C1\"]\nn_bins = 100\ncm = LinearSegmentedColormap.from_list(name=\"\", colors=colors, N=n_bins)\n\nfig = plt.figure(figsize=(6, 3))\nplt.pcolormesh(X, Y, Z, shading=\"auto\", cmap=cm, rasterized=True)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n\nplt.scatter(x0[:, 0], x0[:, 1], s=10.0, label=0, color=\"C0\", edgecolor=\"black\")\nplt.scatter(x1[:, 0], x1[:, 1], s=10.0, label=1, color=\"C1\", edgecolor=\"black\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.legend()\nplt.gca().set_aspect(\"equal\", adjustable=\"box\");",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#footnotes",
    "href": "topics/deep/02.html#footnotes",
    "title": "Neural Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe term stems from biological inspiration, but at this point, literally any hypothesis function of the type above is referred to as a neural network.↩︎",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "AI notebooks",
    "section": "",
    "text": "Some jupyter notebooks containing notes and implementation of AI models, algorithms, & applications. Or generally AI / ML related stuff like chess engines. Ultimately, our goal is to learn how to build performant, computationally efficient, sustainable, and aligned intelligent systems: 🤖 = 🧠 + ⌛ + ♻️ + 🚃.\n\n\n\nThe venv used to run the notebooks can be re-created easily using uv:\nuv venv --python 3.13\nuv sync\nNOTE: You may have to add the .venv as ipykernel in JupyterLab:\nuv add ipykernel\nuv run python -m ipykernel install --user --name=ai-notebooks\n\n\n\nThe notebooks for each topic can be found in separate folders in the /topics directory:\n\n\n\nTopic\nFolder\nPrimary Reference(s)\n\n\n\n\nDeep Learning Systems\n/deep\nCMU 10-414/714: Deep Learning Systems (Fall 2024)"
  },
  {
    "objectID": "README.html#venv",
    "href": "README.html#venv",
    "title": "AI notebooks",
    "section": "",
    "text": "The venv used to run the notebooks can be re-created easily using uv:\nuv venv --python 3.13\nuv sync\nNOTE: You may have to add the .venv as ipykernel in JupyterLab:\nuv add ipykernel\nuv run python -m ipykernel install --user --name=ai-notebooks"
  },
  {
    "objectID": "README.html#the-notebooks",
    "href": "README.html#the-notebooks",
    "title": "AI notebooks",
    "section": "",
    "text": "The notebooks for each topic can be found in separate folders in the /topics directory:\n\n\n\nTopic\nFolder\nPrimary Reference(s)\n\n\n\n\nDeep Learning Systems\n/deep\nCMU 10-414/714: Deep Learning Systems (Fall 2024)"
  },
  {
    "objectID": "topics/deep/01.html",
    "href": "topics/deep/01.html",
    "title": "Softmax Regression",
    "section": "",
    "text": "Suppose you want to write a program that will classify handwritten drawing of digits into their appropriate category: 0, 1, 2, …, 9. You could, think hard about the nature of digits, try to determine the logic of what indicates what kind of digit, and write a program to codify this logic. Or you could take advantage of the statistics of the data, e.g. pixel intensity in a 28 x 28 grid, as discriminative features of each instance. For this task we will use the MNIST dataset:",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#ml-as-data-driven-programming",
    "href": "topics/deep/01.html#ml-as-data-driven-programming",
    "title": "Softmax Regression",
    "section": "ML as data-driven programming",
    "text": "ML as data-driven programming\nMachine learning approach. Collect a training set of images with known labels and feed these into a machine learning algorithm, which, if done well, will automatically produce a “program” that solves this task. The said program will consist of a large number of magic numbers, but it nonetheless performs a sequence of computations to determine the output class:\n\n\n\n^The latter input is different from training data. In practice, we want the models to perform well on unlabeled data.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#three-elements-of-a-ml-algorithm",
    "href": "topics/deep/01.html#three-elements-of-a-ml-algorithm",
    "title": "Softmax Regression",
    "section": "Three elements of a ML algorithm",
    "text": "Three elements of a ML algorithm\nEvery machine learning algorithm has the ff. elements:\n\nElements of a ML training algorithm\n\n\n\n\n\n\n\nElement\nObject\nDescription\n\n\n\n\nHypothesis class\n\\mathcal{H}\nThis defines the “program structure”. In deep learning, the hypothesis class is parameterized via a set of parameters \\Theta. The parameters describe how we map inputs (images of digits) to outputs (labels, or probabilities for each class). Formally, \\mathcal{H} = \\{h_\\Theta \\mid \\Theta \\in \\mathbb{R}^d \\} where h_\\Theta(\\mathbf{x}) = \\hat{y} or \\hat{\\mathbf{p}} for an input \\mathbf{x}.\n\n\nLoss function\n\\mathcal{L}, \\ell\nA function that specifies how “well” a given hypothesis h_\\Theta (i.e. a choice of parameters) performs on the task of interest. The loss is expressed in terms of a pointwise loss function \\ell such that: \\mathcal{L}(h_\\Theta, \\mathcal{D}) = \\frac{1}{N}\\sum_{i=1}^N \\ell(h_\\Theta(\\mathbf{x}_i), y_i) where \\ell \\geq 0 and \\ell \\to 0 whenever the predictions are accurate, and \\ell \\to \\infty as the predictions become increasingly worse.\n\n\nOptimization algorithm\ne.g.  torch.optim.*,  LRScheduler\nProcedure for determining a set of parameters that (approximately) minimizes the training loss. More precisely, the optimizer handles state and the calculation required to find optimal parameters \\Theta^* \\approx {\\text{argmin}}_{\\Theta} \\; \\mathcal{L}(h_\\Theta, \\mathcal{D}). For deep learning, this is typically an iterative approach via SGD and its variants.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#multi-class-classification",
    "href": "topics/deep/01.html#multi-class-classification",
    "title": "Softmax Regression",
    "section": "Multi-class classification",
    "text": "Multi-class classification\nFor multi-class classification, the training dataset \\mathcal{D} consists of input-output pairs \\mathcal{D} = (\\mathbf{x}_i, y_i)_{i=1}^N where \\mathbf{x}_i \\in \\mathbb{R}^d with d is the input dimensionality and y_i \\in [1, K] \\subset \\mathbb{Z} where K is the number of classes. Here N = | \\mathcal{D} | is the size of the dataset. In this setting, a hypothesis function h maps input vectors \\mathbf{x} to K-dimensional vectors: h \\colon \\; \\mathbb{R}^d \\to \\mathbb{R}^K. The output h_j(\\mathbf{x}) indicates some measure of “belief” in how much likely the label is to be class j. That is, the most likely class for an input \\mathbf{x} is predicted as the coordinate \\hat{j} = \\text{argmax}_j \\; h_j(\\mathbf{x}).\nExample. For MNIST, d = 28 \\times 28 = 784, K = 10 and M = 60,000.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#linear-hypothesis-class",
    "href": "topics/deep/01.html#linear-hypothesis-class",
    "title": "Softmax Regression",
    "section": "Linear hypothesis class",
    "text": "Linear hypothesis class\nA linear hypothesis function has matrix multiplication as core operation: h_\\Theta(\\mathbf{x}) = \\Theta^\\top \\mathbf{x} for parameters \\Theta \\in \\mathbb{R}^{d \\times K}. In practice, we usually write this using matrix-batch notation since we process inputs in parallel as a matrix:\n\nh_\\Theta(\\mathbf{X}) =  \\underbrace{\\mathbf{X}}_{\\mathbb{R}^{M \\times d}} \\; \\underbrace{\\Theta}_{\\mathbb{R}^{d \\times K}} \\in \\mathbb{R}^{M \\times K}\n\nwhere the inputs are laid out as row vectors inside the matrix:\n\n\\begin{equation}\n\\mathbf{X}=\\left[\\begin{array}{c}\n-\\, \\mathbf{x}^{(1)\\top}- \\\\\n\\vdots \\\\\n-\\, \\mathbf{x}^{(M)\\top}-\n\\end{array}\\right] \\in \\mathbb{R}^{M \\times d}\n\\end{equation}.\n\nRemark. Geometrically each \\Theta_j = \\Theta_{[:, j]} \\in \\mathbb{R}^d defines a separating hyperplane for class j \\in [K]. So a linear hypothesis class is able to learn to separate linearly separable data points in \\mathbb{R}^d using K separating hyperplanes by assigning a score s_j = \\Theta_j^\\top \\mathbf{x} \\in \\mathbb{R} proportional to its weighted distance from the hyperplane, and the “weight” of that hyperplane \\lVert\\Theta_j\\rVert.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#loss-functions-softmax-and-cross-entropy",
    "href": "topics/deep/01.html#loss-functions-softmax-and-cross-entropy",
    "title": "Softmax Regression",
    "section": "Loss functions, softmax, and cross-entropy",
    "text": "Loss functions, softmax, and cross-entropy\nThe simplest loss is just the classification error: \\ell_{\\text{err}} =\n\\begin{cases}\n0 & \\operatorname{argmax}_i h_i(\\mathbf{x}) = y \\\\\n1 & \\text{else}\n\\end{cases}\n\nWe typically use this loss function to assess the quality of classifiers. Unfortunately, the error is a bad loss function to use for optimization, because it is not differentiable, i.e. we can smoothly adjust the parameters without seeing a change in \\ell_{\\text{err}} or that it changes abruptly.\nInstead, we look at the probabilities assigned by the model to each class. To do this, we have to convert the class scores to probabilities exponentiating and normalizing its entries (i.e. making \\sum_j p = 1 s.t. p_j \\geq 0). Class scores h_j(\\mathbf{x}) = \\Theta_j^\\top \\mathbf{x} are exponentiated before normalizing:\n\np_j = \\frac{\\exp(h_j(\\mathbf{x}))}{\\sum_l \\exp(h_l(\\mathbf{x}))} \\eqqcolon \\text{Softmax} (h(\\mathbf{x}))_j.\n\nThen, the cross-entropy loss is given by the negative log of the probability of the true class y:\n\n\\begin{aligned}\n\\ell_{\\text{CE}}(h(\\mathbf{x}), y)\n&= -\\log \\text{Softmax} (h(\\mathbf{x}))_y \\\\\n&= -h_y(\\mathbf{x})+\\log \\sum_{j=1}^K \\exp \\left(h_j(\\mathbf{x})\\right).\n\\end{aligned}\n\nRemark. Exponentiating means that scores does not scale linearly. In fact,\n\np_j = \\frac{\\exp(\\Delta h_j(\\mathbf{x}))}{\\sum_l \\exp(\\Delta h_l(\\mathbf{x}))}\n\nwhere \\Delta h_l(\\mathbf{x}) = h_l(\\mathbf{x}) - \\max_{m} h_m(\\mathbf{x}). Thus log-softmax becomes \\log (1 + \\sum a_j) where 0 &lt; a_j \\leq 1, preventing both underflow and overflow. Moreover, it shows that the individual scores scale exponentially with the difference from the largest score. Hence, this transformation is sometimes called soft-argmax since it tends to pick out the largest entry.\n\n\nCode\n%config InlineBackend.figure_formats = ['svg'] \nimport torch\nimport matplotlib.pyplot as plt\n\nz = torch.tensor([1, 2, 3]).float()\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].bar(range(3), z.numpy(), label=\"z\")\nax[1].bar(range(3), z.exp().numpy(), color=\"C1\", label=\"exp(z)\")\nax[2].bar(range(3), z.exp().numpy() / z.exp().numpy().sum(), color=\"C2\", label=\"Softmax(z)\")\nax[0].legend(); ax[1].legend(); ax[2].legend();\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nFinally, taking the negative log of the true class suffices since we have the constraint \\sum p_j = 1. Moreover, p_y = 1 implies \\ell_{\\text{ce}} = -\\log 1 = 0 while p_y = 0 implies -\\log 0 = +\\infty. Observe that the penalized most severely when the model is confidently wrong.\n\n\nCode\nimport numpy as np\neps = 1e-5\np = np.linspace(0 + eps, 1, 10000)\nplt.figure(figsize=(5, 4))\nplt.plot(p, -np.log(p), linewidth=2, label=\"-log(p)\")\nplt.grid(alpha=0.6, linestyle=\"dashed\")\nplt.xlabel(\"p\"); plt.ylabel(\"NLL\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nMeanwhile, looking at the gradient, the NLL has the nice property that it does not saturate as it approaches perfect prediction:\n\\frac{\\partial\\ell_{\\text{NLL}}}{\\partial p_y} = -\\frac{1}{p_y}.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#optimization-problem",
    "href": "topics/deep/01.html#optimization-problem",
    "title": "Softmax Regression",
    "section": "Optimization problem",
    "text": "Optimization problem\nThe third ingredient of a machine learning algorithm is a method for solving the associated optimization problem, i.e. the problem of minimizing the average loss on the training set:\n\n\\hat{\\Theta} = \\underset{\\Theta}{\\operatorname{min}} \\frac{1}{N} \\sum_{i=1}^N \\ell_{\\text{CE}} (h_\\Theta(\\mathbf{x}_i), y_i)\n\nHow do we find an optimal set of parameters \\hat{\\Theta}?\n\nGradient Descent\nFor a matrix-input, scalar-output function f\\colon \\mathbb{R}^{d \\times k} \\to \\mathbb{R} the gradient \\nabla_\\Theta f(\\Theta) is defined as the matrix of partial derivatives:\n\n\\nabla_\\Theta f(\\Theta) =\\left[\\begin{array}{ccc}\n\\frac{\\partial f(\\Theta)}{\\partial \\Theta_{11}} & \\cdots & \\frac{\\partial f(\\Theta)}{\\partial \\Theta_{1 k}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\Theta)}{\\partial \\Theta_{d 1}} & \\cdots & \\frac{\\partial f(\\Theta)}{\\partial \\Theta_{d k}}\n\\end{array}\\right] \\in \\mathbb{R}^{d \\times k}\n\nNOTE: \\nabla_\\Theta f always has the same shape as \\Theta when f is a scalar.\nFrom the multivariate Taylor expansion\n\nf(\\Theta + \\Delta \\Theta) \\approx f(\\Theta) + \\nabla_\\Theta f(\\Theta) \\cdot \\Delta\\Theta + \\mathcal{O}(\\Delta\\Theta^2).\n\nSo that the gradient locally points in the direction that most increases f, i.e. to first order. Hence, to minimize f, we iteratively update the weight by -\\nabla_\\Theta f at each point in the surface defined by f:\n\n\\Theta_{t + 1} = \\Theta_t - \\alpha \\cdot \\nabla_\\Theta f (\\Theta_t)\n\nwhere \\alpha &gt; 0 is called the learning rate. GD naturally is sensitive to the scale of the learning rate:\n\n\n\nStochastic Gradient Descent (SGD)\nIn practice, we don’t typically want to compute the gradient using all examples to make a single update to the parameter. This is costly in cases where N \\gg 1 which is typical in deep learning. Instead, we take many gradient steps where each update is based on a randomly sampled subset mini-batch \\mathcal{B} \\subset \\mathcal{D} where B = |\\mathcal{B}| \\ll N. At each training step:\n\nSample \\mathcal{B} \\subset \\mathcal{D} so that we get \\mathbf{X}_\\mathcal{B} \\in \\mathbb{R}^{B \\times d} and \\mathbf{y} \\in [K]^B.\nUpdate parameters: \\begin{aligned}\\Theta_{t + 1}\n= \\Theta_t - \\frac{\\alpha}{B} \\, \\sum_{b \\in I_\\mathcal{B}} \\nabla_\\Theta \\ell (h_{\\Theta_t}(\\mathbf{x}_b), y_b).\\end{aligned}\n\nIt follows that the sample dataset varies at each training step. Unlike the previous case where \\mathcal{D} is fixed. This mechanism of SGD reduces overfitting by implicit regularization of the gradient, i.e. adding noise in the training process.\n\n\nGradient of cross-entropy\nHow do we actually compute \\mathcal{L}_{\\text{CE}}? This can be done using the chain rule and tracking functional dependencies. Recall:\n\n\\ell_{\\text{CE}}(h_\\Theta(\\mathbf{x}), y) = -h_\\Theta(\\mathbf{x})_y + \\log \\sum_{j=1}^K \\exp \\left(h_\\Theta(\\mathbf{x})_j\\right).\n\nLet’s start by deriving the gradient of the softmax loss itself. For a vector \\mathbf{h} \\in \\mathbb{R}^K:\n\n\\frac{\\partial \\ell_{\\text{CE}}}{\\partial h_j} = - \\delta_{yj} + \\frac{\\exp h_j}{\\sum_{l=1}^K \\exp h_l} = - \\delta_{yj} + p_j.\n\nIn vector form, \\nabla_{\\mathbf{h}} \\ell_{\\text{CE}} = \\mathbf{p} - \\mathbf{e}_y where \\mathbf{e}_y is a one-hot vector with 1 on index y.\nNext, to calculate the derivative with respect to \\Theta, we use the chain rule:\n\n\\frac{\\partial \\ell_{\\text{CE}}}{\\partial \\Theta_{ul}} =  \\frac{\\partial \\ell_{\\text{CE}}}{\\partial h_j} \\frac{{\\partial h_j}}{\\partial \\Theta_{ul}} = \\underbrace{(p_l - \\delta_{yl})}_{K-\\text{dim}} \\; \\underbrace{\\vphantom{(}x_u}_{d-\\text{dim}}.\n\nFor the dimensions to make sense, \\frac{\\partial \\ell_{\\text{CE}}}{\\partial \\Theta} = \\mathbf{x}(\\mathbf{p} - \\mathbf{e}_y)^\\top in matrix form. Recall that our vectors are column vectors. Here the product reverses since we traverse the dependence backwards from the loss.\nBatch form. The same process works for a batch of inputs, except that we have an additional batch index which we sum over since \\ell depends on all input instances. The contribution of each input is matched and aggregated using matrix multiplication:\n\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial \\Theta} = \\frac{1}{B}\\,\\underbrace{\\vphantom{(}\\mathbf{X}^\\top}_{d \\times M} \\;\\; \\underbrace{(\\mathbf{P} - \\mathbf{E}_{\\mathbf{y}})}_{M \\times K}.\n\nHere the transposes switched since \\mathbf{X} is constructed such that it has rows of \\mathbf{x}^\\top, hence we internally get a double transpose. Putting it all together, we can write the SGD update rule for softmax regression as follows:\n\n\\Theta_{t + 1} = \\Theta_t - \\frac{\\alpha}{B} \\; \\mathbf{X}^\\top (\\mathbf{P} - \\mathbf{E}_{\\mathbf{y}}).\n\nHere we have \\frac{1}{B} since \\mathcal{L} = \\frac{1}{B}\\sum_b { \\ell}_b. Also it makes sense to scale down since the sum grows with batch size B. To recap, we have the following equations:\n\n\\boxed{\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial \\mathbf{H}} &= \\frac{1}{B}  (\\mathbf{P} - \\mathbf{E}_{\\mathbf{y}}) \\\\[0.75em]\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial \\Theta} &= \\frac{1}{B} \\mathbf{X}^\\top  (\\mathbf{P} - \\mathbf{E}_{\\mathbf{y}}) \\\\[0.60em]\n\\Theta_{t + 1} &= \\Theta_t - \\frac{\\alpha}{B} \\, \\mathbf{X}^\\top (\\mathbf{P} - \\mathbf{E}_{\\mathbf{y}})\n\\end{aligned}\n}",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#code-implementation",
    "href": "topics/deep/01.html#code-implementation",
    "title": "Softmax Regression",
    "section": "Code implementation",
    "text": "Code implementation\nWe will train a classification model based on the SGD update rule above:\n\nimport torch\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n])\n\ntrain_dataset = datasets.MNIST('./data', train=True,  transform=transform)\nvalid_dataset = datasets.MNIST('./data', train=False, transform=transform)\n\nDefining the linear model:\n\nimport torch.nn as nn\nmodel = nn.Linear(784, 10, bias=False)\n\nRemark. The linear model can be extended to have a bias vector \\beta, so that h_\\Theta = \\mathbf{X}\\Theta + \\beta where \\beta \\in \\mathbb{R}^K. But it turns out there is a “bias trick” in deep learning where an additional dimension containing only +1 is added so that the input becomes of shape (N, d + 1). For simplicity, we stick with no bias.\n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nloss_train = []\nN_EPOCHS = 5\nB = 16\nALPHA = 0.01\n\n@torch.no_grad()\ndef train_step(x, y):\n    x = x.reshape(-1, 784)\n    h = model(x)\n\n    for theta in model.parameters():\n        p = F.softmax(h, dim=1)\n        e = F.one_hot(y, num_classes=10)\n        g = x.T @ (p - e) / B\n\n        theta -= ALPHA * g.T\n\n    loss = -p[torch.arange(B), y].log().mean()\n    return loss.item()\n\n\ndl = DataLoader(train_dataset, batch_size=B, shuffle=True)  # SGD!\nfor _ in tqdm(range(N_EPOCHS)):\n    for x, y in dl:\n        loss = train_step(x, y)\n        loss_train.append(loss)\n\n100%|██████████| 5/5 [00:30&lt;00:00,  6.03s/it]\n\n\nRemark. Pytorch nn.Linear computes x @ θ.T + b. Hence, we take g.T before updating the parameter θ.\n\n\nCode\nplt.plot(np.array(loss_train).reshape(-1, 10).mean(1), label=\"train\")\nplt.xlabel(\"step\")\nplt.ylabel(\"loss\")\nplt.grid(alpha=0.3, linestyle=\"dashed\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nLabel prediction sample:\n\ndv = DataLoader(valid_dataset, batch_size=B, shuffle=False)\nx, y = next(iter(dv))\nout = model(x.reshape(B, -1)).argmax(1)\nacc = (out == y).float()\nprint(\"Batch acc:\", f\"{acc.sum().int()}/{B}\", f\"({acc.mean().item() * 100}%)\")\n\nBatch acc: 15/16 (93.75%)\n\n\n\n\nCode\nfig, ax = plt.subplots(4, 4)\nfor i in range(B):\n    a, b = divmod(i, 4)\n    ax[a, b].imshow(x[i].reshape(28, 28), cmap=\"Greys\")\n    ax[a, b].set_title(f\"pred: {out[i]}\")\n    ax[a, b].axis(\"off\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nEvals. The practical goal of training is not actually to minimize \\mathcal{L}(h_\\Theta, \\mathcal{D}). But to minimize the loss for samples outside of the training dataset. That is, the model should be accurate on test data. If the model does not overfit and the test distribution does not drift too far from the training distribution, then we should be good.\n\ntot = 0\nacc = 0\nfor x, y in dv:\n    out = model(x.reshape(B, -1)).argmax(1)\n    correct = (out == y).float()\n    tot += len(y)\n    acc += correct.sum()\n\nprint(f\"Test acc: {acc / tot * 100:.2f}%\")\n\nTest acc: 92.01%",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#appendix-model-complexity",
    "href": "topics/deep/01.html#appendix-model-complexity",
    "title": "Softmax Regression",
    "section": "Appendix: Model complexity",
    "text": "Appendix: Model complexity\nSince SGD relies on a stochastic process, the performance of the resulting trained model varies. How are we sure that we aren’t just lucky for this particular run / random seed? What is the variance of the trained model performance over multiple runs? Is there a way to control this? This issue is at the core of learning theory and precisely what the Bias-Variance Tradeoff addresses.\nIn practice, the crucial parameter to control is model complexity. Here model capacity or complexity is a measure of how complicated a pattern or relationship a model architecture can express. Let f be the true function that underlies the task. If model capacity is sufficiently large, the model class \\mathcal{F} = \\{f_{\\Theta} \\mid \\Theta \\in \\mathbb{R}^d \\} contains an approximation \\hat{f} \\in \\mathcal{F} such that \\| f - \\hat{f} \\| &lt; \\epsilon for a small enough \\epsilon &gt; 0.\nThe capacity of a model class can be controlled, for example, by the number of learnable parameters in practical architectures. It can also be constrained directly by applying regularization or certain prior knowledge such as invariances. This biases the model towards certain solutions, so these constraints are sometimes referred to as inductive biases — such knowledge is bias in the sense that it makes some solutions more likely, and others less likely. The tradeoff is that the model are steered to biased solutions more efficiently.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/03.html",
    "href": "topics/deep/03.html",
    "title": "Automatic Differentiation",
    "section": "",
    "text": "Remark. Let \\mathbf{Z}_{i+1} = f(\\mathbf{Z}_i; \\mathbf{W}_i) be a custom layer. From the equations, it sufficies to specify (e.g. by manual calculation) local gradients \\frac{\\partial{f}}{\\partial{\\mathbf{Z}_{i}}} and \\frac{\\partial{f}}{\\partial{\\mathbf{W}_{i}}}. This locality allows neural network layers to be moduler, i.e. composed arbitrarily, and new layers (or generally, tensor operations) to be easily integrated into the library. Note that if the custom layer can be expressed in terms of existing ops and layers, then this is not strictly necessary, although it may be desirable for efficiency reasons.\nThe form out = f(in, params) is general\n\n\n\n Back to top",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  }
]