[
  {
    "objectID": "topics/dlsys/index.html",
    "href": "topics/dlsys/index.html",
    "title": "Deep Learning Systems",
    "section": "",
    "text": "Primary reference\n\nCMU 10-414/714: Deep Learning Systems (Fall 2024)\nInstructors: J. Zico Kolter and Tianqi Chen\nThis course will provide you will an introduction to the functioning of modern deep learning systems. You will learn about the underlying concepts of modern deep learning systems like automatic differentiation, neural network architectures, optimization, and efficient operations on systems like GPUs.\nFinally, to solidify understanding, the homeworks build from scratch needle — a deep learning library loosely similar to PyTorch, and implement many common architectures in the library."
  },
  {
    "objectID": "topics/dlsys/index.html#prerequisites",
    "href": "topics/dlsys/index.html#prerequisites",
    "title": "Deep Learning Systems",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nSystems programming\nLinear algebra\nOther mathematical background: e.g., calculus, probability, basic proofs\nPython and C++ development\nPrior experience with basic ML (overfitting, model validation)"
  },
  {
    "objectID": "topics/dlsys/index.html#learning-objectives",
    "href": "topics/dlsys/index.html#learning-objectives",
    "title": "Deep Learning Systems",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nUnderstand the basic functioning of modern deep learning libraries\nIncluding concepts like automatic differentiation, gradient-based optimization\nBe able to implement several standard deep learning architectures\nMLPs, ConvNets, RNNs, Seq2Seq, Transformers, truly from scratch\nUnderstand how hardware acceleration (e.g., on GPUs) works under the hood\nBe able to develop your own highly efficient code for modern DL"
  },
  {
    "objectID": "topics/dlsys/index.html#contents",
    "href": "topics/dlsys/index.html#contents",
    "title": "Deep Learning Systems",
    "section": "Contents",
    "text": "Contents\n\n01 Introduction\n02 Softmax Regression\n03 “Manual” Neural Networks\n04 Automatic Differentiation"
  },
  {
    "objectID": "topics/dlsys/03.html",
    "href": "topics/dlsys/03.html",
    "title": "“Manual” Neural Networks",
    "section": "",
    "text": "Recall that a hypothesis function (i.e. a choice of architecture) \\(h\\colon \\mathbb{R}^d \\to \\mathbb{R}^K\\) maps input data to desired outputs. Initially, we used a linear hypothesis class \\(h_\\Theta(\\mathbf{x}) = \\Theta^\\top \\mathbf{x}\\) where \\(\\Theta \\in \\mathbb{R}^{d \\times K}.\\) This forms \\(K\\) linear functions of the input and predicts the class with the largest value. This turns out to be equivalent to partitioning the input space into \\(K\\) linear convex regions corresponding to each class.\nRemark. To see this in \\(\\mathbb{R}^2\\) with \\(3\\) classes, we have \\(2\\) inequality constraints \\(\\theta_1^\\top \\mathbf{x} \\geq \\theta_2^\\top \\mathbf{x}\\) and \\(\\theta_1^\\top \\mathbf{x} \\geq \\theta_3^\\top \\mathbf{x}.\\) This resuts in a convex polyhedron which is the intersection of two linear half-spaces. Similarly, for the other two. It can be shown that the interior of these subsets are disjoint and the union of the three cover all of \\(\\mathbb{R}^2.\\)\nQ. What about data that are not linearly separable? We want some way to separate these points via a nonlinear set of class boundaries.\nimport math\nimport numpy as np\n\nN = 100\nr_eps = 0.3\n\ndef circle_data(radius: float, r_eps=r_eps, num_points=N):\n    r0 = radius\n    t = 2 * math.pi * np.random.random(N)\n    r = r0 + r_eps * np.random.randn(N)\n    x, y = r * np.cos(t), r * np.sin(t)\n    return x, y\n\nx0, y0 = circle_data(3)\nx1, y1 = circle_data(5)\nx2, y2 = circle_data(1)\nCode\n%config InlineBackend.figure_formats = [\"svg\"] \nimport matplotlib.pyplot as plt\n\nplt.scatter(x0, y0, edgecolor=\"k\")\nplt.scatter(x1, y1, marker=\"*\", edgecolor=\"k\", s=100)\nplt.scatter(x2, y2, marker=\"s\", edgecolor=\"k\")\nplt.axis(\"equal\");"
  },
  {
    "objectID": "topics/dlsys/03.html#nonlinear-features",
    "href": "topics/dlsys/03.html#nonlinear-features",
    "title": "“Manual” Neural Networks",
    "section": "Nonlinear features",
    "text": "Nonlinear features\nOne idea: Apply a linear classifier to some (potentially higher-dimensional) features of the data:\n\\[h_\\Theta(\\mathbf{x}) = \\Theta^\\top \\phi(\\mathbf{x})\\]\nwhere \\(\\Theta \\in \\mathbb{R}^{n \\times K}\\) and \\(\\phi\\colon \\mathbb{R}^d \\to \\mathbb{R}^n\\) is a feature transformation.\nExample. For the above dataset, we can define \\(\\phi(x, y) = (x, y, x^2 + y^2)\\) (i.e. \\(r^2\\)) which makes the dataset separable in \\(\\mathbb{R}^3\\):\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(x0, y0, x0 ** 2 + y0 ** 2, edgecolor=\"k\")\nax.scatter(x1, y1, x1 ** 2 + y1 ** 2, marker=\"*\", edgecolor=\"k\", s=100)\nax.scatter(x2, y2, x2 ** 2 + y2 ** 2, marker=\"s\", edgecolor=\"k\");\n\n\n\n\n\n\n\n\nQ. How do we create the features?\n\nManual feature engineering (see above).\nIn a way that \\(\\phi\\) itself is learned from data (i.e. \\(\\phi\\) parametric).\n\nNote that \\(\\phi\\) linear doesn’t work since we just get a linear classifier. If \\(\\phi(\\mathbf{x}) = \\Phi^\\top \\mathbf{x}\\) where \\(\\Phi \\in \\mathbb{R}^{d \\times n}\\), then\n\\[\n\\begin{aligned}\nh_\\Theta(\\mathbf{x})\n&= \\Theta^\\top \\phi(\\mathbf{x}) \\\\\n&= \\Theta^\\top \\Phi^\\top \\mathbf{x} = (\\Phi \\Theta)^\\top \\mathbf{x}\n\\end{aligned}\n\\]\nThus, \\(\\phi\\) must be nonlinear. It turns out that applying a nonlinear univariate function \\(\\sigma\\colon \\mathbb{R} \\to \\mathbb{R}\\) suffices. Hence, we set\n\\[\\phi(\\mathbf{x}) = \\sigma(\\Phi^\\top \\mathbf{x}).\\]\nThe function \\(\\sigma\\colon \\mathbb{R} \\to \\mathbb{R}\\) is called an activation function. For example, with \\(\\sigma = \\cos\\) we get what is called random Fourier features which work great for many problems. Moreover, observe that we can feature transform features, i.e. compose feature transformations."
  },
  {
    "objectID": "topics/dlsys/03.html#neural-networks",
    "href": "topics/dlsys/03.html#neural-networks",
    "title": "“Manual” Neural Networks",
    "section": "Neural networks",
    "text": "Neural networks\nA neural network refers to a particular type of hypothesis class, consisting of multiple, parameterized differentiable functions (a.k.a. “layers”) composed together in any manner to form the output. Since neural networks involve composing a lot of functions (sometimes hundreds), it is usually referred to as deep neural networks, although there is really no requirement on depth beyond being not linear.\nRemark. The term stems from biological inspiratioM, but at this point, literally any hypothesis function of the type above is referred to as a neural network.\n\nTwo-layer neural network\nThe simplest form of neural network is basically just the nonlinear features presented earlier:\n\\[\n\\begin{aligned}\nh_\\Theta(\\mathbf{X}) = \\sigma ( \\mathbf{X} \\mathbf{W_1}) \\mathbf{W}_2\n\\end{aligned}\n\\]\nwhere \\(\\Theta = \\{ \\mathbf{W}_1 \\in \\mathbb{R}^{d \\times n}, \\mathbf{W}_2 \\in \\mathbb{R}^{n \\times K} \\}\\) are the trainable parameters and \\(\\sigma\\colon \\mathbb{R} \\to \\mathbb{R}\\) is the activation function that is applied elementwise to each vector. A commonly used one is ReLU defined as \\(\\sigma(z) = \\max(0, z).\\)\n\nz = np.linspace(-10, 10, 1000)\nplt.figure(figsize=(5, 2))\nplt.plot(z, np.clip(z, a_min=0, a_max=None), label=\"relu(z)\", linewidth=2, color=\"k\")\nplt.grid(alpha=0.6, linestyle=\"dotted\"); plt.ylim(-1, 11); plt.legend();\n\n\n\n\n\n\n\n\n\n\nFully-connected deep networks\nObserve that for the 2-layer network, we have \\(|\\Theta| = 2.\\) A more generic form is the \\(L\\)-layer neural network, sometimes called a multi-layer perceptron (MLP), feedforward network (FFN), or fully-connected network (FC) written in batch form as:\n\\[\nh_\\Theta(\\mathbf{X}) = \\sigma(\\sigma(\\ldots \\sigma(\\mathbf{X}\\mathbf{W}_1)\\mathbf{W}_2 \\ldots) \\mathbf{W}_{L-1}) \\mathbf{W}_L\n\\]\nor\n\\[\n\\begin{aligned}\n\\mathbf{Z}_1 &= \\mathbf{X} \\\\\n\\mathbf{Z}_{i + 1} &= \\sigma_i (\\mathbf{Z}_i \\mathbf{W}_i), \\quad \\forall i = 1, \\ldots, L \\\\\nh_\\Theta(\\mathbf{X}) &= \\mathbf{Z}_{L + 1}\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{Z}_i \\in \\mathbb{R}^{M \\times d_I}\\) and \\(\\mathbf{W}_i \\in \\mathbb{R}^{d_I \\times d_{i+1}}\\), such that \\(d_1 = d\\) and \\(d_{L+1} = K\\) and with nonlinearities \\(\\sigma_i\\colon \\mathbb{R} \\to \\mathbb{R}\\) applied elementwise, and weights \\(\\Theta = \\{ \\mathbf{W}_1, \\ldots, \\mathbf{W}_L \\}.\\) It follows that \\(|\\Theta| = L.\\) Also, we typically set \\(\\sigma_L = \\text{Id}\\) for the output layer.\nRemark. Again a bias term can be added. But in theoretical analysis we can just think there is an extra column containing +1 to simplify the computation. Example shapes for a 3-layer neural network:\n\n\n\n\n\\(i\\)\n\\(d_i\\)\n\\(d_{i+1}\\)\n\n\n\n\n1\n16\n32\n\n\n2\n32\n64\n\n\n3\n64\n10\n\n\n4\n10"
  },
  {
    "objectID": "topics/dlsys/03.html#backpropagation",
    "href": "topics/dlsys/03.html#backpropagation",
    "title": "“Manual” Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\nRecall that to train the linear function via softmax regression and SGD, we had to calculate the gradients across cross-entropy and the matrix product. For a deep neural network, we need to calculate the gradient for each \\(\\mathbf{W}_i \\in \\Theta,\\) i.e. across each layer \\(i = 1, 2, \\ldots, L.\\) Since the gradients are calculated from the loss to the inputs, this part of the computation is called backward pass. The algorithm for caching intermediate results and accumulating the gradients is collectively called backpropagation (BP) or simply “backprop”.\nThe best way to understand and calculate backprop is to view neural nets as computational graphs with certain defined operations (e.g. entire layers, or lower-level tensor transforms):\n\n\nCode\nfrom graphviz import Digraph\n\ndot = Digraph(format=\"png\")\ndot.attr(rankdir=\"LR\")\ndot.node_attr.update(shape=\"box\", style=\"rounded,filled\", fontsize=\"10\", fontname=\"Helvetica\")\ndot.node(\"X\", \"X\", fillcolor=\"lightgray\")\ndot.node(\"W1\", \"W₁\", fillcolor=\"lightyellow\")\ndot.node(\"Z2\", \"Z₂ = σ(XW₁)\", fillcolor=\"lightgreen\")\ndot.node(\"W2\", \"W₂\", fillcolor=\"lightyellow\")\ndot.node(\"Z3\", \"Z₃ = Z₂W₂\", fillcolor=\"lightgreen\")\ndot.node(\"loss\", \"CE loss\", fillcolor=\"lightblue\")\ndot.node(\"Y\", \"Y\", fillcolor=\"lightgrey\")\n\ndot.edges([(\"X\", \"Z2\"), (\"W1\", \"Z2\"), (\"Z2\", \"Z3\"), (\"W2\", \"Z3\"), (\"Z3\", \"loss\"), (\"Y\", \"loss\")])\ndot\n\n\n\n\n\n\n\n\n\n\nGradients of a 2-layer NN\nLet us work through a softmax regression with a two-layer neural network. Here we want to find \\(\\nabla_{\\mathbf{W}_i} \\mathcal{L}_\\text{CE}(h_\\Theta(\\mathbf{X}), \\mathbf{y})\\) for \\(i = 1, 2.\\) Let’s write this as \\(\\mathbf{Z}_2 = \\sigma(\\mathbf{X} \\mathbf{W}_1)\\) and \\(\\mathbf{Z}_3 = \\mathbf{Z_2}\\mathbf{W}_2.\\) From the previous lecture:\n\\[\n\\boxed{\n\\frac{\\partial \\mathcal{L}_\\text{CE}}{\\partial \\mathbf{W}_2} =\n\\frac{\\partial \\mathcal{L}_\\text{CE}}{\\partial \\mathbf{Z}_3}\n\\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_2}\n= \\frac{1}{B}\\mathbf{Z}_2^\\top (\\mathbf{P} - \\mathbf{E}_\\mathbf{y}).\n}\n\\]\nQ. Shapes. LHS has \\((d_2, K)\\), while \\((d_2, B) \\times (B, K)\\) on the RHS. OK. In the above equation, we branched towards the weight. But two tensors point towards \\(\\mathbf{Z}_3\\), i.e. we now move down towards \\(\\mathbf{Z}_2\\) and connect this to \\(\\mathbf{W}_1\\) via chain rule:\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}_\\text{CE}}{\\partial \\mathbf{W}_1}\n&=\n\\frac{\\partial \\mathcal{L}_\\text{CE}}{\\partial \\mathbf{Z}_3}\n\\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{Z}_2}\n\\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{W}_1}.\n\\end{aligned}\n\\]\nFirst, we calculate the first two terms. The middle term is obtained by considering a single instance: \\({\\partial z_3^{j}}/{\\partial z_2^{i}} = w^{ij}_2.\\) Thus,\n\\[\n\\boxed{\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}_\\text{CE}}{\\partial \\mathbf{Z}_2} =\n\\frac{\\partial \\mathcal{L}_\\text{CE}}{\\partial \\mathbf{Z}_3}\n\\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{Z}_2} =\n\\frac{1}{B}\n(\\mathbf{P} - \\mathbf{E}_\\mathbf{y})\n\\mathbf{W}_2^\\top.\n\\end{aligned}\n}\n\\]\nThis has shape \\((B, K) \\times (K, d_2) = (B, d_2)\\) which looks correct.\nFinally, we calculate the last term. Here we are taking the derivative of \\(d_2\\) functions with respect to the weight of shape \\((d, d_2).\\) Let \\(\\mathbf{y} = \\mathbf{W}_1^\\top \\mathbf{x}\\), then\n\\[\n\\begin{aligned}\n\\frac{\\partial {z}_2^j}{\\partial w_1^{ik}}\n&= \\sum_l\n\\frac{\\partial z_2^j}{\\partial y^l}\n\\frac{\\partial y^l}{\\partial w_1^{ik}}  \\\\\n&= \\sum_l \\delta^{jl} \\sigma^\\prime(y^j) \\, x^i \\delta^{lk} = \\delta^{jk} \\sigma^\\prime(y^j) \\, x^i.\n\\end{aligned}\n\\]\nHere \\(\\delta =\\) Kronecker delta. The two Kronecker delta “contracted” into one. We can actually just start with \\(k=j\\) but I wanted to practice some tensor calculus. The effect of \\(\\delta^{jk}\\) is that \\(\\sigma^\\prime(y^j)\\) essentially multiplies element-wise to the incoming gradient. Moreover, we aggregate the contribution of the batch instances by multiplying \\(\\mathbf{X}^\\top.\\) Thus, in batch form:\n\\[\n\\boxed{\n\\frac{\\partial \\mathcal{L}_\\text{CE}}{\\partial \\mathbf{W}_1}  =\n\\frac{1}{B} \\mathbf{X}^\\top \\Big( [(\\mathbf{P} - \\mathbf{E}_\\mathbf{y})\n\\mathbf{W}_2^\\top ] \\odot \\sigma^\\prime(\\mathbf{X} \\mathbf{W}_1) \\Big)\n}\n\\]\n\n\nGraph visualization\nBackward dependence can be inspected using the torchviz library. The graph shows the tensors being stored (🟧) during forward pass to compute the gradients. The weights (🟦) are instances of leaf tensors, i.e. the outermost nodes with no parent nodes. Hence, backward iteration stops at the leaf nodes.\nIt follows that calculating gradients require: - twice the memory of forward pass due to caching - roughly the same time complexity \\(\\mathcal{O}(S)\\) where \\(S\\) is the network size.\n\nfrom torchviz import make_dot\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nmodel = nn.Sequential(\n    nn.Linear(16, 32, bias=False),\n    nn.ReLU(),\n    nn.Linear(32, 10, bias=False)\n)\n\nB = 8\nx = torch.randn(B, 16)\nz = model(x)\ny = torch.randint(0, 10, size=(B,))\nloss = F.cross_entropy(z, y)\n\nmake_dot(loss.mean(), params=dict(model.named_parameters()), show_saved=True)\n\n\n\n\n\n\n\n\n\n\nGeneral formula for \\(L\\)-layers\nIs there a method to this madness? Consider our fully-connected network \\(\\mathbf{Z}_{i+1} = \\sigma_i (\\mathbf{Z}_i \\mathbf{W}_i)\\) for \\(i = 1, \\ldots, L.\\) Observe that we can traverse along the \\(\\mathbf{Z}_i\\)’s which can be thought of as the trunk of the tree, while the gradient for the leaf tensors (i.e. the weights) can be calculated using the incoming gradients \\(\\partial{\\mathcal{L}}/\\partial{\\mathbf{Z}_{i+1}}\\) along the main trunk.\n\n\nCode\ndot  # showing again\n\n\n\n\n\n\n\n\n\nDefine \\(\\mathbf{G}_{i} \\coloneqq \\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathbf{Z}_{i}}}\\) with shape \\((B, d_{i}).\\) Then for \\(i = 1, \\ldots, L + 1\\):\n\\[\n\\boxed{\n\\begin{aligned}\n\\mathbf{G}_{L + 1} &= \\frac{1}{B}(\\mathbf{P} - \\mathbf{E}_\\mathbf{y}) \\quad \\quad \\;\\text{(cross-entropy)} \\\\\n\\mathbf{G}_i &= \\mathbf{G}_{i + 1} \\frac{\\partial \\mathbf{Z}_{i + 1}}{\\partial \\mathbf{Z}_{i}} = [\\mathbf{G}_{i + 1} \\odot \\sigma_i^\\prime(\\mathbf{Z}_i \\mathbf{W}_i)] \\mathbf{W}_i^\\top \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{i}}\n&= \\mathbf{G}_{i + 1}\n\\frac{\\partial \\mathbf{Z}_{i + 1}}{\\partial \\mathbf{W}_{i}}\n= \\mathbf{Z}_i^\\top[\\mathbf{G}_{i + 1} \\odot \\sigma_i^\\prime(\\mathbf{Z}_i \\mathbf{W}_i)]\n\\end{aligned}\n}\n\\]\nThis looks very compact. You can verify that the shapes are correct and that the above derivation for two-layer networks is consistent with the current formulas with \\(\\sigma_2 = \\text{Id}\\). The formulas essentially describe the flow of gradients from the loss to the input. However, each update relies only on the gradient from the next layer.\nThe gradients are modulated by gradient of the activation \\(\\sigma_i\\) (analogous to how activations squash linear outs during forward pass). Also, observe the form loss_grad x local_grad where the loss gradient \\(\\mathbf{G}_{i+1}\\) from the next layer is, in a sense, “global” in contrast. This pattern is quite general. Finally, the shape checks out: \\((B, d_{i + 1}) \\times (d_{i+1}, d_i) = (B, d_{i}).\\) And \\(\\mathbf{W}_i^\\top\\) attaches to \\(d_{i+1}\\) and projects back to \\(d_i.\\) Meanwhile, \\(\\mathbf{Z}_i^\\top\\) accumulates \\(M\\)-many \\(d_{i+1}\\) dimensional vectors for each of \\(d_i\\) coordinates.\nRemark. Let \\(\\mathbf{Z}_{i+1} = f(\\mathbf{Z}_i; \\mathbf{W}_i)\\) be a custom layer. From the equations, it sufficies to specify (e.g. by manual calculation) local gradients \\(\\frac{\\partial{f}}{\\partial{\\mathbf{Z}_{i}}}\\) and \\(\\frac{\\partial{f}}{\\partial{\\mathbf{W}_{i}}}.\\) This modularity allows neural network layers to be composed arbitrarily, and new layers / tensor operations to be integrated into the library. Note that if the custom layer can be expressed in terms of existing ops and layers, then this is not necessary, although it may be desirable for efficiency reasons.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "topics/dlsys/03.html#appendix-gradient-check-using-pytorch",
    "href": "topics/dlsys/03.html#appendix-gradient-check-using-pytorch",
    "title": "“Manual” Neural Networks",
    "section": "Appendix: Gradient check using PyTorch",
    "text": "Appendix: Gradient check using PyTorch\nCalculating manually the gradients of a 3-layer neural network:\n\n# network parameters\nw1 = torch.randn(16, 64, requires_grad=True)\nw2 = torch.randn(64, 32, requires_grad=True)\nw3 = torch.randn(32, 10, requires_grad=True)\n\n# forward pass\nB  = 8\nx  = torch.randn(B, 16)\nz2 = torch.tanh(x @ w1)\nz3 = torch.tanh(z2 @ w2)\nz4 = z3 @ w3\n\nfor u in [w1, w2, w3, z2, z3, z4]:\n    u.retain_grad()\n\nh = z4\ny = torch.randint(0, 10, size=(B,))\nloss = F.cross_entropy(h, y)\nloss.backward()\n\nBackward pass:\n\n# walking down the trunk of the network\ng4 = (F.softmax(h, dim=1) - F.one_hot(y, num_classes=10)) / B\ng3 = g4 @ w3.T\ng2 = (g3 * (1 - z3 ** 2)) @ w2.T\n\n# branching to the leaf tensors (weights)\nz1 = x\ndw3 = z3.T @ g4\ndw2 = z2.T @ (g3 * (1 - z3 ** 2))\ndw1 = z1.T @ (g2 * (1 - z2 ** 2))\n\nNote that shapes are equal, otherwise we can’t subtract:\n\nerrors = []\n\nerrors.append(torch.abs(g4 - z4.grad).max().item())\nerrors.append(torch.abs(g3 - z3.grad).max().item())\nerrors.append(torch.abs(g2 - z2.grad).max().item())\n\nerrors.append(torch.abs(dw1 - w1.grad).max().item())\nerrors.append(torch.abs(dw2 - w2.grad).max().item())\nerrors.append(torch.abs(dw3 - w3.grad).max().item())\n\nprint(f\"Max absolute error: {max(errors):.2e}\")\n\nMax absolute error: 1.19e-07"
  },
  {
    "objectID": "topics/dlsys/03.html#appendix-universal-approximation",
    "href": "topics/dlsys/03.html#appendix-universal-approximation",
    "title": "“Manual” Neural Networks",
    "section": "Appendix: Universal approximation",
    "text": "Appendix: Universal approximation\n[Cybenko 1989]. It turns out that any continuous map \\(f\\colon K \\subset \\mathbb{R}^d \\to \\mathbb{R}^m\\) defined on a compact set \\(K\\) can be approximated by a 1-layer fully-connected network. Continuity on a compact domain is a reasonable assumptions about a ground truth function that we assume exists.\nDemo. The following demo shows a one-dimensional curve approximated with a ReLU network:\n\nimport torch\n\n# Ground truth\nx = torch.linspace(-2 * torch.pi, 2 * torch.pi, 1000)\ny = torch.sin(x) + 0.3 * x\n\n# Get sorted sample. Shifted for demo\nB = sorted(torch.randint(30, 970, size=(24,)))\nxs = x[B,]\nys = y[B,]\n\n# ReLU approximation\nz = torch.zeros(1000,) + ys[0]\nfor i in range(len(xs) - 1):\n    if torch.isclose(xs[i + 1], xs[i]):\n        m = torch.tensor(0.0)\n    else:\n        m = (ys[i+1] - ys[i]) / (xs[i+1] - xs[i])\n    z += m * (torch.relu(x - xs[i]) - torch.relu(x - xs[i+1]))\n\nNOTE: This only works for target \\(f\\) with compact domain \\([a, b]\\) consistent with the theorem.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(7, 3))\nax[0].scatter(xs, ys, facecolor=\"none\", s=12, edgecolor=\"k\", zorder=3, label=\"data\")\nax[0].plot(x, y, color=\"C1\", label=\"f\")\nax[0].set_xlabel(\"x\")\nax[0].set_ylabel(\"y\")\nax[0].legend(loc=\"upper left\")\n\nax[1].scatter(xs, ys, facecolor=\"none\", s=12, edgecolor=\"k\", zorder=4, label=\"data\")\nax[1].plot(x, z, color=\"C0\", label=f\"relu approx. (B={len(B)})\", zorder=3)\nax[1].plot(x, y, color=\"C1\")\nax[1].set_xlabel(\"x\")\nax[1].set_ylabel(\"y\")\nax[1].legend(loc=\"lower right\", fontsize=7.5)\nfig.tight_layout();"
  },
  {
    "objectID": "topics/dlsys/03.html#appendix-linearly-separable-features",
    "href": "topics/dlsys/03.html#appendix-linearly-separable-features",
    "title": "“Manual” Neural Networks",
    "section": "Appendix: Linearly separable features",
    "text": "Appendix: Linearly separable features\nAn ideal classifier is that where the sequence of layers transform the input \\(\\mathbf{x}_i\\) into data points \\(F(\\mathbf{x}_i) \\in \\mathbb{R}^{d_{L+1}}\\) that is linearly separable. That is, we essentially extend linear classification to input that is not linearly separable. This explains why the final layer has no activatioM, i.e. we can think of the network \\(f\\) as\n\\[f_{(\\Theta, \\Phi)} = h_{\\Theta} \\circ F_{\\Phi}\\]\nwhere \\(h_\\Theta \\colon \\mathbb{R}^{L + 1} \\to \\mathbb{R}^K\\) is a linear hypothesis, while the earlier \\(L-1\\) layers are feature extractors that compose \\(F_{\\Phi}.\\)\n\nimport torch\ntorch.manual_seed(2)\n\ndef generate_data(M: int):\n    noise = lambda e: torch.randn(M, 2) * e\n    t = 2 * torch.pi * torch.rand(M, 1)\n    s = 2 * torch.pi * torch.rand(M, 1)\n\n    x0 = torch.cat([0.3 * torch.cos(s), 0.3 * torch.sin(s)], dim=1) + noise(0.2)\n    x1 = torch.cat([3.0 * torch.cos(t), 3.0 * torch.sin(t)], dim=1) + noise(0.3)\n    y0 = (torch.ones(M,) * 0).long()\n    y1 = (torch.ones(M,) * 1).long()\n\n    return x0, y0, x1, y1\n\n\nx0, y0, x1, y1 = generate_data(1500)\n\n\n\nCode\nfig = plt.figure(figsize=(5, 3))\nplt.scatter(x0[:, 0], x0[:, 1], s=10.0, label=0, color=\"C0\", alpha=0.9, edgecolor=\"k\")\nplt.scatter(x1[:, 0], x1[:, 1], s=10.0, label=1, color=\"C1\", alpha=0.9, edgecolor=\"k\")\n\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.grid(alpha=0.6, linestyle=\"dotted\")\nplt.axis(\"equal\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nLet’s do a simple 2-layer neural net where the feature extractor maps to \\(\\mathbb{R}^3\\):\n\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Linear(2, 3), nn.Tanh(),\n    nn.Linear(3, 2)\n)\n\nModel training:\n\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\noptim = torch.optim.SGD(model.parameters(), lr=0.01)\n\nx = torch.cat([x0, x1])\ny = torch.cat([y0, y1])\nhistory = {\"accs\": [], \"loss\": []}\nfor step in tqdm(range(15000)):\n    s = model(x)\n    loss = F.cross_entropy(s, y)\n    loss.backward()\n    optim.step()\n    optim.zero_grad()\n    history[\"loss\"].append(loss.item())\n    history[\"accs\"].append(100 * (y == torch.argmax(s, dim=1)).float().mean())\n\n100%|██████████| 15000/15000 [00:09&lt;00:00, 1643.60it/s]\n\n\n\n\nCode\nfig, ax1 = plt.subplots(figsize=(8, 3))\nax2 = ax1.twinx()\n\nax1.plot(history[\"loss\"], color=\"blue\", linewidth=2)\nax2.plot(history[\"accs\"], color=\"red\",  linewidth=2)\nax1.set_xlabel(\"step\")\nax1.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(3, 3))\nax1.grid(axis=\"both\", linestyle=\"dotted\", alpha=0.8)\n\nax1.set_ylabel(\"Batch loss\")\nax2.set_ylabel(\"Batch accs (%)\")\nax1.yaxis.label.set_color(\"blue\")\nax2.yaxis.label.set_color(\"red\");\n\n\n\n\n\n\n\n\n\nObserve that accuracy trend does not exactly match the steadily decreasing loss. This is expected since accuracy considers hard labels whereas the loss is calculated with respect to soft probability distributions.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# transformations\nwith torch.no_grad():\n    linear_0 = model[0](x0)\n    linear_1 = model[0](x1)\n    linear_act_0 = model[1](model[0](x0))\n    linear_act_1 = model[1](model[0](x1))\n\n    # separating hyperplane (see above discussion, i.e. w &lt;- w1 - w0  == logistic reg)\n    h = 1\n    w, b = model[2].parameters()\n    w, b = (w[h] - w[h-1]), (b[h] - b[h-1])\n\n# plot\nfig = plt.figure(figsize=(8, 3))\nax0 = fig.add_subplot(131)\nax1 = fig.add_subplot(132, projection=\"3d\")\nax2 = fig.add_subplot(133, projection=\"3d\")\n\nax0.grid(alpha=0.8, linestyle=\"dotted\")\nax0.set_axisbelow(True)\nax0.scatter(x0[:, 0], x0[:, 1], s=2.0, label=0, color=\"C0\")\nax0.scatter(x1[:, 0], x1[:, 1], s=2.0, label=1, color=\"C1\")\nax0.set_xlabel(\"$x_1$\")\nax0.set_ylabel(\"$x_2$\")\nax0.set_xlim(-1.5, 1.5)\nax0.set_ylim(-1.5, 1.5)\nax0.set_title(\"(a) input\")\nax0.legend()\nax0.set_facecolor(\"whitesmoke\")\nax0.axis(\"equal\")\n\nax1.scatter(linear_0[:, 0], linear_0[:, 1], linear_0[:, 2], s=3, label=0, color=\"C0\", alpha=0.8)\nax1.scatter(linear_1[:, 0], linear_1[:, 1], linear_1[:, 2], s=3, label=1, color=\"C1\", alpha=0.8)\nax1.set_xlabel(\"$x_1$\")\nax1.set_ylabel(\"$x_2$\")\nax1.set_zlabel(\"$x_3$\")\nax1.set_title(\"(b) linear\")\n\nax2.scatter(linear_act_0[:, 0], linear_act_0[:, 1], linear_act_0[:, 2], s=3, label=0, color=\"C0\")\nax2.scatter(linear_act_1[:, 0], linear_act_1[:, 1], linear_act_1[:, 2], s=3, label=1, color=\"C1\")\nax2.set_xlabel(\"$x_1$\")\nax2.set_ylabel(\"$x_2$\")\nax2.set_zlabel(\"$x_3$\")\nax2.set_title(\"(c) linear + tanh\")\n\n# Generate grid of points\nx_min = min(linear_act_1[:, 0].min(), linear_act_0[:, 0].min())\nx_max = max(linear_act_1[:, 0].max(), linear_act_0[:, 0].max())\ny_min = min(linear_act_1[:, 1].min(), linear_act_0[:, 1].min())\ny_max = max(linear_act_1[:, 1].max(), linear_act_0[:, 1].max())\na, b, c, d = w[0], w[1], w[2], b\nx = np.linspace(x_min, x_max, 50)\ny = np.linspace(y_min, y_max, 50)\nX, Y = np.meshgrid(x, y)\nZ = (-a * X - b * Y - d) / c\n\n# Plot the hyperplane for the positive class\nax2.plot_surface(X, Y, Z, alpha=0.5, color=f\"C{h}\")\nfig.tight_layout();\n\n\n\n\n\n\n\n\n\nRemark. The last linear layer (the “logits”) defines the separating hyperplane.\nPredicting on \\(\\mathbb{R}^2\\):\n\n# Create a grid of points\nN = 300\nx = np.linspace(-6, 6, N)\ny = np.linspace(-4, 4, N)\nX, Y = np.meshgrid(x, y)\n\n# Calculate p[1] for each point in grid\ninp = torch.tensor(list(zip(X, Y)), dtype=torch.float32).permute(0, 2, 1).reshape(-1, 2)\nout = F.softmax(model(inp),  dim=1)\nZ = out[:, 1].reshape(300, 300).detach().numpy()\n\nThe plot below is obtained by applying the model on each point in the grid. The coloring essentially represents the model on test data \\(\\mathbf{x} \\in \\mathbb{R}^2\\). Notice that we essentially have a nonlinear decision boundary:\n\n\nCode\n# create a color plot\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Define custom colormap\ncolors = [\"C0\", \"C1\"]\nn_bins = 100\ncm = LinearSegmentedColormap.from_list(name=\"\", colors=colors, N=n_bins)\n\nfig = plt.figure(figsize=(6, 3))\nplt.pcolormesh(X, Y, Z, shading=\"auto\", cmap=cm, rasterized=True)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n\nplt.scatter(x0[:, 0], x0[:, 1], s=10.0, label=0, color=\"C0\", edgecolor=\"black\")\nplt.scatter(x1[:, 0], x1[:, 1], s=10.0, label=1, color=\"C1\", edgecolor=\"black\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.legend()\nplt.gca().set_aspect(\"equal\", adjustable=\"box\");"
  },
  {
    "objectID": "topics/dlsys/01.html",
    "href": "topics/dlsys/01.html",
    "title": "Introduction to Deep Learning Systems",
    "section": "",
    "text": "Deep Learning Systems (DLS) solved problems considered hard prior to 2010, e.g. obtaining superhuman / SOTA scores on tasks and challenges such as ImageNet, CASP, and Go (board game) \\(^{[1]}\\). Later, in the early 2020s, unprecedented progress in text & image generation were made with models like GPT-3 and Stable Diffusion:\n[1] Game tree complexity of \\(10^{360}\\) at 250 moves over 150 move games.\n\n\n\n\nDespite the dominance of deep learning libraries and TensorFlow and PyTorch, the playing field in this space is remarkably fluid (see e.g., recent emergence of JAX). You may want to work on developing existing frameworks (virtually all of which are open source), or developing your own new frameworks for specific tasks.\nDLS is not just for the “big players”:\n\nClaim. The single largest driver of widespread adoption of DL has been the creation of easy-to-use autodiff libraries:\n\nDeep learning systems are continuously evolving:\n\n\n\n\nUnderstanding how the internals of existing deep learning systems work let you use them much more efficiently. For example, you can make your custom non-standard layer run (much) faster in TensorFlow / PyTorch by understanding how these operations are executed. Understanding deep learning systems is a “superpower” that will let you accomplish your research aims much more efficiently.\n\n\n\nDespite their seeming complexity, the core underlying algorithms behind deep learning systems (automatic differentiation + gradient-based optimization) are extremely simple. Unlike (say) operating systems, you could probably write a “reasonable” deep learning library in &lt;2000 lines of (dense) code.\nThe first time you build your automatic differentiation library, and realize you can take the gradient of a gradient without actually knowing how you would even go about deriving that mathematically. For example, you can automatically differentiate complex operations like batch norm, which requires careful dependency tracking when done by hand. Or the gradient of a for-loop which can be tedious to follow manually."
  },
  {
    "objectID": "topics/dlsys/01.html#why-study-deep-learning",
    "href": "topics/dlsys/01.html#why-study-deep-learning",
    "title": "Introduction to Deep Learning Systems",
    "section": "",
    "text": "Deep Learning Systems (DLS) solved problems considered hard prior to 2010, e.g. obtaining superhuman / SOTA scores on tasks and challenges such as ImageNet, CASP, and Go (board game) \\(^{[1]}\\). Later, in the early 2020s, unprecedented progress in text & image generation were made with models like GPT-3 and Stable Diffusion:\n[1] Game tree complexity of \\(10^{360}\\) at 250 moves over 150 move games.\n\n\n\n\nDespite the dominance of deep learning libraries and TensorFlow and PyTorch, the playing field in this space is remarkably fluid (see e.g., recent emergence of JAX). You may want to work on developing existing frameworks (virtually all of which are open source), or developing your own new frameworks for specific tasks.\nDLS is not just for the “big players”:\n\nClaim. The single largest driver of widespread adoption of DL has been the creation of easy-to-use autodiff libraries:\n\nDeep learning systems are continuously evolving:\n\n\n\n\nUnderstanding how the internals of existing deep learning systems work let you use them much more efficiently. For example, you can make your custom non-standard layer run (much) faster in TensorFlow / PyTorch by understanding how these operations are executed. Understanding deep learning systems is a “superpower” that will let you accomplish your research aims much more efficiently.\n\n\n\nDespite their seeming complexity, the core underlying algorithms behind deep learning systems (automatic differentiation + gradient-based optimization) are extremely simple. Unlike (say) operating systems, you could probably write a “reasonable” deep learning library in &lt;2000 lines of (dense) code.\nThe first time you build your automatic differentiation library, and realize you can take the gradient of a gradient without actually knowing how you would even go about deriving that mathematically. For example, you can automatically differentiate complex operations like batch norm, which requires careful dependency tracking when done by hand. Or the gradient of a for-loop which can be tedious to follow manually."
  },
  {
    "objectID": "topics/dlsys/01.html#elements-of-deep-learning-systems",
    "href": "topics/dlsys/01.html#elements-of-deep-learning-systems",
    "title": "Introduction to Deep Learning Systems",
    "section": "Elements of deep learning systems",
    "text": "Elements of deep learning systems\n\nCompose multiple tensor operations to build modern ML models.\nTransform a sequence of operations (automatic differentiation).\nAccelerate computation via specialized hardware.\nExtend to more hardware backends, and more operators."
  },
  {
    "objectID": "topics/cpp/04.html",
    "href": "topics/cpp/04.html",
    "title": "Functions",
    "section": "",
    "text": "Functions in C++ require a return type and a body. It may return a variable, value or nothing (void). A void return type means that assigning the function output to a variable results in an erorr. This behavior is different from Python which allows assignment of None which is the default return value whenever none is specified.\nfrom utils import *\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint add_one(int x) {\n    return x + 1;\n}\n\nvoid hello() {\n    cout &lt;&lt; \"hello\\n\";\n}\n\nint main() {\n    cout &lt;&lt; add_one(3) &lt;&lt; endl;\n    hello();\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n4\nhello"
  },
  {
    "objectID": "topics/cpp/04.html#newtons-method",
    "href": "topics/cpp/04.html#newtons-method",
    "title": "Functions",
    "section": "Newton’s method",
    "text": "Newton’s method\nExample. Let’s define an actual meaningful function. Recall Newton’s Method for getting the zero of \\(f\\):\n\\[x_{k+1} = x_k - \\frac{f(x)}{f^\\prime(x)}\\]\nGiven a number \\(a &gt; 0\\) we want to solve for its square root using \\(f(x) = x^2 - a.\\)\n\\[\nx_{k + 1} = x_k - \\frac{x_k^2 - a}{2 x_k} = \\frac{1}{2} \\left( x_k + \\frac{a}{x_k} \\right).\n\\]\n\n%%runcpp 04_newton_sqrt.cpp --run=false\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\nusing namespace std;\n\ndouble squareroot(float a, int n_steps = 10) {\n    int i;\n    double x = a / 2;\n    cout &lt;&lt; fixed &lt;&lt; setprecision(10);\n    \n    for (i = 0; i &lt; n_steps; i++) {\n        x = 0.5 * (x + a / x);\n        cout &lt;&lt; x &lt;&lt; \"\\t\" &lt;&lt; \"| err: \" &lt;&lt; x * x - a &lt;&lt; endl;\n    }\n    \n    return x;\n}\n\nint main() {\n    double a; cin &gt;&gt; a;\n    double x;\n    x = squareroot(a);\n    cout &lt;&lt; \"\\noutput: \" &lt;&lt; x;\n    return 0;\n}\n\ng++ -std=c++23 ./code/04_newton_sqrt.cpp -o ./code/04_newton_sqrt\n\n\n\nNOTE: Here we use &lt;iomanip&gt; for std::fixed and std::setprecision. To increase cout precision from 6 to 8;\n\n!echo 1011.25 | ./code/04_newton_sqrt\n\n253.8125000000  | err: 63409.5351562500\n128.8983701674  | err: 15603.5398318240\n68.3718491123   | err: 3663.4597510372\n41.5811465161   | err: 717.7417455967\n32.9505313728   | err: 74.4875177519\n31.8202382539   | err: 1.2775625347\n31.8001635687   | err: 0.0004029930\n31.8001572323   | err: 0.0000000000\n31.8001572323   | err: -0.0000000000\n31.8001572323   | err: -0.0000000000\n\noutput: 31.8001572323\n\n\n\nround(1011.25 ** 0.5, 10)\n\n31.8001572323"
  },
  {
    "objectID": "topics/cpp/04.html#parameter-passing",
    "href": "topics/cpp/04.html#parameter-passing",
    "title": "Functions",
    "section": "Parameter passing",
    "text": "Parameter passing\nCalling a function by value involves copying the contents of the arguments into the memory locations of the corresponding formal parameters. If the function changes the values of the parameters, the original contents in the memory referenced by the arguments of the calling function do not change. This is analogous to local variables in Python functions.\nIn C++, we can pass a reference to a location in memory which allows changing the value in that location. To let the compiler know that you intend to use pass by reference, you attach an & to the end of the type name in the formal parameter list in the function declaration and header.\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nvoid add_one(int& x) {\n    x = x + 1;\n}\n\nint main() {\n    int u = 41; \n    add_one(u);\n    cout &lt;&lt; u &lt;&lt; endl;\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n42\n\n\nRemark. Removing the & in add_one results in u being the same."
  },
  {
    "objectID": "topics/cpp/04.html#arrays-as-parameters-in-functions",
    "href": "topics/cpp/04.html#arrays-as-parameters-in-functions",
    "title": "Functions",
    "section": "Arrays as Parameters in Functions",
    "text": "Arrays as Parameters in Functions\nAn array is a collection data type that is the ancestor of the Python list. We will discuss arrays in more detail in the next chapter. Functions can be used with array parameters to maintain a structured design. However, a formal parameter for an array is neither a call-by-value nor a call-by-reference, but a new type of parameter pass called an array parameter:\nfloat f(int arr[]) {\n    ...\n} \nMoreover, an array is defined using the same notation.\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\ndouble avg(int arr[], int n) {\n    double s = 0;\n    for (int i = 0; i &lt; n; i++) {\n        s += arr[i];\n    }\n    return s / n;\n}\n\nint main() {\n    int arr[] = {1, 2};\n    cout &lt;&lt; avg(arr, 2) &lt;&lt; endl;\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n1.5\n\n\nNOTE: The functions with array parameters do not make private copies of the arrays. Instead, the reference is passed to reduce the impact on memory. Arrays can therefore always be permanently changed when passed as arguments to functions. ⚠️\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nvoid zero(int arr[]) {\n    arr[0] = 0;\n}\n\nint main() {\n    int arr[] = {1, 2};\n    println(\"{}, {}\", arr[0], arr[1]);\n    zero(arr);\n    println(\"{}, {}\", arr[0], arr[1]);\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n1, 2\n0, 2\n\n\nTo prevent ourselves from accidentally modifying any of these arrays, we can add the modifier const in the function head. The compiler will then raise an error if any statement within the function’s definition modifies the elements of the const array:\n\n!rm ./code/tmp\n\n\n%%runcpp --exitcode=true\n#include &lt;iostream&gt;\nusing namespace std;\n\nvoid zero(const int arr[]) {\n    arr[0] = 0;\n}\n\nint main() {\n    int arr[] = {1, 2};\n    println(\"{}, {}\", arr[0], arr[1]);\n    zero(arr);\n    println(\"{}, {}\", arr[0], arr[1]);\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n127\n\n\n./code/tmp.cpp:5:12: error: read-only variable is not assignable\n    5 |     arr[0] = 0;\n      |     ~~~~~~ ^\n1 error generated.\n/bin/sh: line 1: ./code/tmp: No such file or directory"
  },
  {
    "objectID": "topics/cpp/04.html#function-overloading",
    "href": "topics/cpp/04.html#function-overloading",
    "title": "Functions",
    "section": "Function Overloading",
    "text": "Function Overloading\nFunction overloading is a unique feature of C++ (not present in Python) where functions of the same name but with different implementations can be defined. The functions are distinguished based on their parameters. Note that assigning different return types result in a warning:\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nvoid f(int n) {\n    println(\"1 params: {}\", n);\n}\n\nvoid f(int n, int m) {\n    println(\"2 params: {}, {}\", n, m);\n}\n\nint main() {\n    f(4);\n    f(5, 6);\n    f(0);\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n1 params: 4\n2 params: 5, 6\n1 params: 0"
  },
  {
    "objectID": "topics/cpp/02.html",
    "href": "topics/cpp/02.html",
    "title": "Atomic C++ Data Types",
    "section": "",
    "text": "Recall that C++ requires users to specify the data type of each variable. The primary built-in data types in C++ are: int, float, double, bool, and char. There is also a special type called pointer which holds a memory location. C++ also has a collection of compound data types."
  },
  {
    "objectID": "topics/cpp/02.html#boolean-data",
    "href": "topics/cpp/02.html#boolean-data",
    "title": "Atomic C++ Data Types",
    "section": "Boolean data",
    "text": "Boolean data\nC++ uses the keyword bool which is not capitalized. The possible state values for a C++ Boolean are true and false (note: lower case). C++ uses the standard Boolean operators: && (AND), || (OR), and ! (NOT).\nNote that the internally stored values representing true and false are 1 and 0, respectively. Moreover, you can perform math operations of the 1s and 0s where they are casted as a numeric type. Indeed:\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    auto print_type = [](auto expr) {\n        cout &lt;&lt; \"(\" &lt;&lt; typeid(expr).name() &lt;&lt; \") \" &lt;&lt; expr &lt;&lt; endl;\n    };\n\n    print_type(true);\n    print_type(false);\n    cout &lt;&lt; (true || false) &lt;&lt; endl;\n    cout &lt;&lt; (true && false) &lt;&lt; endl;\n    print_type(true + true);\n    print_type(true * 1.);\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n(b) 1\n(b) 0\n1\n0\n(i) 2\n(d) 1\n\n\nBoolean data are also used as results for things like comparison (&lt;, ==, etc.). As usual, relational and logical operators can be combined to form complex logical conditions. The following function demonstrates relational operators:\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    cout &lt;&lt; (5 == 10) &lt;&lt; endl;\n    cout &lt;&lt; (10 &gt; 5) &lt;&lt; endl;\n    cout &lt;&lt; ((5 &gt;= 1) && (5 &lt;= 10)) &lt;&lt; endl;\n    cout &lt;&lt; ((5 &gt;= 1) || (1 / 0)) &lt;&lt; endl;\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n0\n1\n1\n1\n\n\nRemark. Observe short circuit with OR works as 1/0 was not evaluated.\nOne gotcha with booleans is assignment. If you assign a non-null (i.e. nonzero), then it becomes evaluated as 1. That is, false = 0 and !false = true. To see this:\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    bool bool_var = -1;\n    cout &lt;&lt; bool_var &lt;&lt; endl;\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n1\n\n\nAssigning -1 to a bool evaluates to 1! But this is intended behavior due to static typing."
  },
  {
    "objectID": "topics/cpp/02.html#character-data",
    "href": "topics/cpp/02.html#character-data",
    "title": "Atomic C++ Data Types",
    "section": "Character data",
    "text": "Character data\nIn Python strings can be created with single or double quotes. In C++ single quotes (’) are used for the character (char) data type, and double quotes (“) are used for the string data type.\nStrings in C++ provides dynamic memory management of an array of chars. It supports built-in methods (e.g. .size(), .substr(), .find()), and features safety and flexibility over raw C-style strings (e.g. char cstr[] = \"hello\";). Note that \"a\" == 'a' results in an error in C++.\nchar a = 'h';\nchar cstr[] = \"hello\";      // C-style string (char array)\nstd::string str = \"hello\";  // C++ string object\nBut you can convert between them:\nstr.c_str();         // std::string → const char*\nstd::string(cstr);   // char* → std::string"
  },
  {
    "objectID": "topics/cpp/02.html#pointers",
    "href": "topics/cpp/02.html#pointers",
    "title": "Atomic C++ Data Types",
    "section": "Pointers",
    "text": "Pointers\nIn C++, an assignment like below consists of (1) allocating space in memory and (2) storing the value of a variable:\nint var = 100;\nTo get the address of a variable in memory we use the address-of operator &:\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int var = 100;\n    cout &lt;&lt; &var &lt;&lt; \" \" &lt;&lt; var &lt;&lt; endl;\n    var = 200;\n    cout &lt;&lt; &var &lt;&lt; \" \" &lt;&lt; var &lt;&lt; endl;\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n0x16bba6938 100\n0x16bba6938 200\n\n\nRemark. For basic types declared in a function scope, the address is fixed during their lifetime.\nIn Python, it is impossible to store a variable directly. Instead, a reference to the data object is used. In C++, variables store values directly, because they are faster to reference. References are slower, but they are sometimes useful. If in C++, we want to create an analogous reference to a memory location, we must use a special data type called a pointer.\n\nPointer syntax\nPointer init. The syntax for initializing a pointer is similar to the usual assignment except we use * between the data type and the identifier:\nint* ptr;\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int* ptr;\n    cout &lt;&lt; ptr &lt;&lt; endl;\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n0x100953d7c\n\n\n\n\nAssigning an address using &\nOne way to do this is to have a pointer refer to another variable by using the address-of operator &, which we know returns the address of a variable.\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int  var = 100;\n    int* ptr = &var;\n    cout &lt;&lt;  ptr &lt;&lt; endl;    // address of var\n    cout &lt;&lt; *ptr &lt;&lt; endl;    // dereference =&gt; value\n\n    // trying out reassignment\n    var = 200;\n    cout &lt;&lt; &var &lt;&lt; endl;\n    cout &lt;&lt; *ptr &lt;&lt; endl;\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n0x16b92a938\n100\n0x16b92a938\n200\n\n\nRemark. If we assign an address for a location that is outside of your segment (area in memory reserved for your program), the operating system will jump in with a message about a “segmentation fault” (aka segfault). (Although such an error message looks bad, a seg fault is in fact a helpful error because unlike the elusive logical errors, the reason is fairly localized.)\n\n\nNull pointer\nLike None in Python, the null pointer (nullptr) in C++ points to nothing. Or more precisely, it does not point to an address that is mapped to the program’s memory space. The null pointer is often used in conditions in conjunction with AND / OR in logical operations.\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int* nptr = nullptr;\n    cout &lt;&lt;  nptr &lt;&lt; endl;\n    cout &lt;&lt; !nptr &lt;&lt; endl;   // evaluates to true (1) since nptr is null\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n0x0\n1\n\n\nThe following example demonstrates how the null pointer can be used. The variable ptrx initially has the address of x when it is declared. On the first iteration of the loop, it is assigned the value of nullptr, which evaluates to a false value; thereby ending the loop:\n\n%%runcpp --exitcode=true\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int x = 12345;\n    int* ptrx = &x;\n\n    while (ptrx) {\n        cout &lt;&lt; \"Pointer ptrx points to \" &lt;&lt; ptrx &lt;&lt; endl;\n        ptrx = nullptr;\n    }\n\n    cout &lt;&lt; \"Pointer ptrx points to nothing: \" &lt;&lt; ptrx &lt;&lt; endl;\n    cout &lt;&lt; *ptrx &lt;&lt; \"segfault?\" &lt;&lt; endl;\n\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\nPointer ptrx points to 0x16fc02938\nPointer ptrx points to nothing: 0x0\n-11\n\n\nNOTE: Since the address 0x0 is unmapped, dereferencing the null pointer results in undefined behavior. Hence, the program crashed before it can print the last string (-11 = segfault)."
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "AI notebooks",
    "section": "",
    "text": "Some jupyter notebooks containing notes and implementation of AI models, algorithms, & applications. Or generally AI / ML related stuff like chess engines. Ultimately, our goal is to learn how to build performant, computationally efficient, sustainable, and aligned intelligent systems: 🤖 = 🧠 + ⌛ + ♻️ + 🚃.\n\n\n\nThe venv used to run the notebooks can be re-created easily using uv:\nuv venv --python 3.13\nuv sync\nNOTE: You may have to add the .venv as ipykernel in JupyterLab:\nuv add ipykernel\nuv run python -m ipykernel install --user --name=ai-notebooks\n\n\n\nThe notebooks for each topic can be found in separate folders in the /topics directory:\n\n\n\nTopic\nFolder\nPrimary Reference(s)\nFocus\n\n\n\n\nDeep Learning Systems\n/dlsys\nCMU 9-414/714: Deep Learning Systems. Fall 2024 + 2022 video lectures\n#algorithms, #implementation"
  },
  {
    "objectID": "README.html#venv",
    "href": "README.html#venv",
    "title": "AI notebooks",
    "section": "",
    "text": "The venv used to run the notebooks can be re-created easily using uv:\nuv venv --python 3.13\nuv sync\nNOTE: You may have to add the .venv as ipykernel in JupyterLab:\nuv add ipykernel\nuv run python -m ipykernel install --user --name=ai-notebooks"
  },
  {
    "objectID": "README.html#the-notebooks",
    "href": "README.html#the-notebooks",
    "title": "AI notebooks",
    "section": "",
    "text": "The notebooks for each topic can be found in separate folders in the /topics directory:\n\n\n\nTopic\nFolder\nPrimary Reference(s)\nFocus\n\n\n\n\nDeep Learning Systems\n/dlsys\nCMU 9-414/714: Deep Learning Systems. Fall 2024 + 2022 video lectures\n#algorithms, #implementation"
  },
  {
    "objectID": "topics/cpp/01.html",
    "href": "topics/cpp/01.html",
    "title": "Introduction to C++ for Python Programmers",
    "section": "",
    "text": "We look at primary constructs that are common to nearly all programming languages:\nNext, we look at additional useful features of C++ including:"
  },
  {
    "objectID": "topics/cpp/01.html#why-learn-c",
    "href": "topics/cpp/01.html#why-learn-c",
    "title": "Introduction to C++ for Python Programmers",
    "section": "Why learn C++?",
    "text": "Why learn C++?\nThese other more formal languages have some advantages of their own. First, is speed: For very large programs C and C++ are likely to give you the best performance. Second, is their maintainability. Python requires you to remember certain things. For example if you set variable t to reference a turtle, and forget later that t is a turtle but try to invoke a string method on it, you will get an error. C++ protects you from this kind of error by forcing you to be upfront and formal about the type of object each variable is going to refer to.\nC++ is an industrial strength programming language and is very often used today for large systems by large groups of people. C++ is particularly good at interacting directly with computer hardware, making execution very fast. C++ supports the four primary features of OOP: abstraction, inheritance, polymorphism, and encapsulation. C++ allows the programmer to create and use a data type called a pointer explicitly, which can increase control over both memory and performance under certain circumstances. Because C++ is fast, it is currently the language of choice for virtual reality. Also, because C++ is fast, it is the language of choice of many 2D and 3D game engines. For all of the above reasons, even though C++ is an older language, it is still one of the top listed in job advertisements."
  },
  {
    "objectID": "topics/cpp/01.html#hello-world",
    "href": "topics/cpp/01.html#hello-world",
    "title": "Introduction to C++ for Python Programmers",
    "section": "Hello, world!",
    "text": "Hello, world!\nThe time honored tradition. First, we recall in Python:\n\nprint(\"Hello, world!\")\n\nHello, world!\n\n\nThis can be made into a more complicated version:\n\ndef main():\n    print(\"Hello, world!\")\n\nmain()\n\nHello, world!\n\n\nThe version in C++:\n\n%%writefile ./code/hello.cpp\n#include &lt;iostream&gt;\n\nint main() {\n    std::cout &lt;&lt; \"Hello, world!\\n\";\n    return 0;\n}\n\nWriting ./code/hello.cpp\n\n\n\n!g++ ./code/hello.cpp -o ./code/hello\n!./code/hello\n\nHello, world!\n\n\nNOTE: Compiling and then running. In my case, I have gcc (v15) installed using Homebrew on a MacBook. That’s a lot of writing, so we define a magic method for running the code in a jupyter cell:\n\nimport shlex\nimport argparse\nimport subprocess\nfrom IPython.core.magic import register_cell_magic\n\ndef parse_args(line):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"filename\", nargs=\"?\", default=\"tmp.cpp\")\n    parser.add_argument(\"--run\", type=lambda x: x.lower() != \"false\", default=True)\n    parser.add_argument(\"--exitcode\", type=lambda x: x.lower() == \"true\", default=False)\n    args = parser.parse_args(shlex.split(line))\n    return args\n\n@register_cell_magic\ndef runcpp(line, cell):\n    args = parse_args(line)\n    with open(f\"./code/{args.filename}\", \"w\") as file:\n        file.write(cell)\n\n    # compile and run\n    fn = args.filename.split(\".\")[0]\n    run = f\"\\n./code/{fn}\" * int(args.run)\n    cmd = f\"g++ -std=c++23 ./code/{fn}.cpp -o ./code/{fn}\" + run\n\n    print(cmd + \"\\n\")\n    ret = subprocess.run(cmd, shell=True)\n    if args.exitcode:\n        print(ret.returncode)\n\nNOTE: We use C++23 which is the current version as of July 2025.\nThis can be called with and without a filename:\n\nprint(parse_args(\"\"))\nprint(parse_args(\"hello.txt\"))\nprint(parse_args(\"hello.txt --run=false\"))\n\nNamespace(filename='tmp.cpp', run=True, exitcode=False)\nNamespace(filename='hello.txt', run=True, exitcode=False)\nNamespace(filename='hello.txt', run=False, exitcode=False)\n\n\nTesting the cell magic:\n\n%%runcpp hello.cpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    cout &lt;&lt; \"Hello World!\\n\";\n    return 0;\n}\n\ng++ -std=c++23 ./code/hello.cpp -o ./code/hello\n./code/hello\n\nHello World!\n\n\nRemark. The using keyword allows us to skip writing std:: every time we use a component in the C++ standard library."
  },
  {
    "objectID": "topics/cpp/01.html#compilation",
    "href": "topics/cpp/01.html#compilation",
    "title": "Introduction to C++ for Python Programmers",
    "section": "Compilation",
    "text": "Compilation\nTo run the C++ programs in the previous section, we compiled it using g++. This allows (1) early error detection (e.g. syntax or type errors), and (2) faster program execution. The job of the compiler is to turn your C++ code into language that your machine can understand. We call the code that the computer understands machine code. The computer interprets the machine code much like the Python interpreter interprets your Python. However, since machine code is much closer to the native language of the computer, it can run faster.\nSidenote. Python code is not directly converted into machine code that a computer’s CPU can execute. Instead, Python code is first compiled into an intermediate form called bytecode, which is then interpreted by the Python Virtual Machine (PVM). The PVM translates the bytecode into machine code instructions that the specific CPU can understand and run.\n\nHeaders and libraries\nPreprocessor directives in C++ appear as statements preceded by the hash sign #. These tell the preprocessor which file, header, or library to make available to the compiler. For example, #include &lt;iostream&gt; will make sure that the iostream library is available at compile time. Here, the term header is used for a type of C++ file that contains definitions of functions and variables, but not the function implementations.\nThis is similar to Python imports. But note that there are two ways to use #include in C++:\n#include &lt;libraryname&gt;\n#include \"filename\"\nHere the angle-brackets &lt;&gt; are used to include libraries or headers provided by the implementation, such as the headers in the standard library (iostream, string, etc.). The double quotes \" are used for headers and files not provided by the implementation (i.e. compiler and your version of the standard library).\n#include &lt;vector&gt;      // provided by the C++ standard library\n#include \"utils.h\"     // a custom file in your project\n\n\nLinking\nThe &lt;iostream&gt; header provides declarations for input / output classes like std::cin, std::cout, and std::endl. These are defined in the standard C++ library, which is automatically linked by the compiler. You don’t need to manually link any .a or .so file — the compiler links the C++ standard library by default. Some libraries require explicit linking:\ng++ main.cpp -lm     # link the math library (libm)\ng++ main.cpp         # links the standard C++ library automatically\nIn Python when you do import math, you’re not recompiling the math module. Python just links your code to an already compiled .so or .pyd file. The same idea applies in C++ — but it happens at compile + link time instead of import time. Pre-compiled libraries (compiled by the compiler vendor or maintainer) allow compilation of new code to be faster (i.e. compile once, reuse multiple times).\nGoing back to &lt;iostream&gt;, this header declares std::cout. The implementation of std::cout lives in a compiled library like libc++.dylib (macOS). The compiler knows where to find and link that binary automatically.\n\n%%time\n!g++ ./code/hello.cpp -o ./code/hello\n!./code/hello\n\nHello World!\nCPU times: user 5.7 ms, sys: 11 ms, total: 16.7 ms\nWall time: 1.05 s\n\n\n\n%%time\n!./code/hello\n\nHello World!\nCPU times: user 1.78 ms, sys: 5.95 ms, total: 7.73 ms\nWall time: 294 ms\n\n\n\n!g++ --version\n\nApple clang version 16.0.0 (clang-1600.0.26.6)\nTarget: arm64-apple-darwin23.6.0\nThread model: posix\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin"
  },
  {
    "objectID": "topics/cpp/01.html#the-main-function",
    "href": "topics/cpp/01.html#the-main-function",
    "title": "Introduction to C++ for Python Programmers",
    "section": "The main function",
    "text": "The main function\nUnlike Python, every C++ program must have a main function which begins with int main(). This main function is called implicitly instead of explicitly like we must do in Python when we have a main function. This is why you do not see an explicit function call invoking main.\nThis is very similar to the Python version. Except that we have to specify the type of the return value. And that the integer return value must be specified to some integer value (customary to return 0 whenever there is no error, and specific return values for error handling). I tried to do void main() in C++ which is closer in spirit, but it results in an error:\nerror: '::main' must return 'int'\nA more accurate Python version should then be:\n\ndef main():\n    print(\"Hello, world!\")\n    return 42\n\nmain()\n\nHello, world!\n\n\n42\n\n\nThat translates to:\n\n%%runcpp --exitcode=true\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    cout &lt;&lt; \"Hello World!\\n\";\n    return 42;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n42Hello World!"
  },
  {
    "objectID": "topics/cpp/01.html#standard-io",
    "href": "topics/cpp/01.html#standard-io",
    "title": "Introduction to C++ for Python Programmers",
    "section": "Standard I/O",
    "text": "Standard I/O\nStandard output. Let’s take a step back and look at cout. This actually stands for “character output” which sends character data to the screen. The operator &lt;&lt; directs the string to this output device. Note that multiple uses of &lt;&lt; onto cout corresponds to concatenating the strings:\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    cout &lt;&lt; \"Ever heard of rubber duck debugging?\" &lt;&lt; endl;\n    cout &lt;&lt; \"                __     \" &lt;&lt; endl;\n    cout &lt;&lt; \"              &lt;(o )___-\" &lt;&lt; endl;\n    cout &lt;&lt; \"               ( .__&gt; /\" &lt;&lt; endl;\n    cout &lt;&lt; \"                `----' \" &lt;&lt; endl;\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\nEver heard of rubber duck debugging?\n                __     \n              &lt;(o )___-\n               ( .__&gt; /\n                `----' \n\n\nQ. Why use endl when we can just + \"\\n\"? It turns out that these have different side-effects. Inserting a newline character does not flush the output buffer. On the other hand, using std::endl inserts a newline and flushes the output stream (i.e., forces it to write to the terminal immediately). Flushing can be useful in interactive programs (e.g., before cin &gt;&gt; ...), but adds overhead.\nStandard input. Like cout, cin is also a character stream with &gt;&gt; used to direct input onto it. For standard input, we can pipe the input via terminal:\n\n%%runcpp 01_cin.cpp --run=false\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    string resp;\n    cout &lt;&lt; \"\\nEver heard of rubber duck debugging?\" &lt;&lt; endl;\n    cin  &gt;&gt; resp;\n    if ((resp[0] == 'Y') || (resp[0] == 'y')) {\n        cout &lt;&lt; \"cool B)\" &lt;&lt; endl;\n    }\n    else {\n        cout &lt;&lt; \"Here you go.\" &lt;&lt; endl;\n        cout &lt;&lt; \"                __     \" &lt;&lt; endl;\n        cout &lt;&lt; \"              &lt;(o )___-\" &lt;&lt; endl;\n        cout &lt;&lt; \"               ( .__&gt; /\" &lt;&lt; endl;\n        cout &lt;&lt; \"                `----' \" &lt;&lt; endl;\n    }\n\n    return 0;\n}\n\ng++ -std=c++23 ./code/01_cin.cpp -o ./code/01_cin\n\n\n\nRemark. Using \"Y\" fails. This is because response[0] is a char that should be compared with another char Y. Meanwhile, \"Y\" is a const char* (C-style string).\n\n!echo \"y\" | ./code/01_cin\n!echo \"n\" | ./code/01_cin\n\n\nEver heard of rubber duck debugging?\ncool B)\n\nEver heard of rubber duck debugging?\nHere you go.\n                __     \n              &lt;(o )___-\n               ( .__&gt; /\n                `----'"
  },
  {
    "objectID": "topics/cpp/01.html#type-declarations",
    "href": "topics/cpp/01.html#type-declarations",
    "title": "Introduction to C++ for Python Programmers",
    "section": "Type declarations",
    "text": "Type declarations\nIn the above examples, we declare new variables with a type, e.g. string resp;. Just like functions, all variables in C++ must be declared before use, and they cannot change type. This is known as static typing.\nThe line string resp essentially tells the compiler to set aside sufficient space for a floating point number, and to name this memory location resp. Then whatever the user types in will be stored in the resp variable."
  },
  {
    "objectID": "topics/cpp/01.html#conclusion",
    "href": "topics/cpp/01.html#conclusion",
    "title": "Introduction to C++ for Python Programmers",
    "section": "Conclusion",
    "text": "Conclusion\nWe can now look back at the initial program with better understanding of its elements:\n/* \nThis hello world program demonstrates the C++ concepts\nof commenting, using libraries, and using output.\n*/\n\n#include &lt;iostream&gt;               // ①\nusing namespace std;              // ② \n\nint main() {                       // ③   \n    cout &lt;&lt; \"Hello World!\\n\";     // ④ \n    return 0;                     // ⑤\n}\n\nDirective for including iostream header.\nMakes std:: implicit.\nmain() must exist & return an int.\nCharacter stream, print to std output device.\n0 indicates program ended correctly."
  },
  {
    "objectID": "topics/cpp/03.html",
    "href": "topics/cpp/03.html",
    "title": "Control Structures",
    "section": "",
    "text": "from utils import runcpp"
  },
  {
    "objectID": "topics/cpp/03.html#conditionals",
    "href": "topics/cpp/03.html#conditionals",
    "title": "Control Structures",
    "section": "Conditionals",
    "text": "Conditionals\nC++ if-else is just like Python. It doesn’t have elif. But it does allow chaining else if to emulate the same behavior.\n\n%%runcpp --run=false\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int grade;\n    cin &gt;&gt; grade;\n\n    if (grade &lt; 60) {\n        cout &lt;&lt; 'F' &lt;&lt; endl;\n    }\n    else if (grade &lt; 70) {\n        cout &lt;&lt; 'D' &lt;&lt; endl;\n    }\n    else if (grade &lt; 80) {\n        cout &lt;&lt; 'C' &lt;&lt; endl;\n    }\n    else if (grade &lt; 90) {\n        cout &lt;&lt; 'B' &lt;&lt; endl;\n    }\n    else {\n        cout &lt;&lt; 'A' &lt;&lt; endl;\n    }\n    \n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n\n\n\n\n!echo 10 | ./code/tmp\n\n0x0\n1\n\n\n\n!echo 83 | ./code/tmp\n\n0x0\n1\n\n\nRemark. There also the switch keyword, which uses case and break."
  },
  {
    "objectID": "topics/cpp/03.html#while-loops",
    "href": "topics/cpp/03.html#while-loops",
    "title": "Control Structures",
    "section": "While loops",
    "text": "While loops\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int c = 0;\n    while (c &lt; 5) {\n        cout &lt;&lt; c &lt;&lt; \" Hello, world!\\n\";\n        c += 1;\n    }\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n0 Hello, world!\n1 Hello, world!\n2 Hello, world!\n3 Hello, world!\n4 Hello, world!\n\n\nChecking ints as condition:\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int x;\n    x = -1; if (x) {cout &lt;&lt; \"ok\" &lt;&lt; endl;} else {cout &lt;&lt; \"not\" &lt;&lt; endl;}\n    x =  0; if (x) {cout &lt;&lt; \"ok\" &lt;&lt; endl;} else {cout &lt;&lt; \"not\" &lt;&lt; endl;}\n    x = +1; if (x) {cout &lt;&lt; \"ok\" &lt;&lt; endl;} else {cout &lt;&lt; \"not\" &lt;&lt; endl;}\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\nok\nnot\nok"
  },
  {
    "objectID": "topics/cpp/03.html#for-loops",
    "href": "topics/cpp/03.html#for-loops",
    "title": "Control Structures",
    "section": "For loops",
    "text": "For loops\nSame while loop above as a for loop:\n\n%%runcpp\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    for (int c = 0; c &lt; 5; c++) {\n        cout &lt;&lt; c &lt;&lt; \" Hello, world!\" &lt;&lt; endl;\n    }\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n0 Hello, world!\n1 Hello, world!\n2 Hello, world!\n3 Hello, world!\n4 Hello, world!"
  },
  {
    "objectID": "topics/cpp/05.html",
    "href": "topics/cpp/05.html",
    "title": "Collection Data Types",
    "section": "",
    "text": "In addition to the primitive types, C++ also offers built-in collection types. A collection data type is a grouping of some number of items (possibly 1 or 0) that have some shared significance or need to be operated upon together.\nArrays, vectors, strings, sets, and hash tables are among these useful C++ collection types."
  },
  {
    "objectID": "topics/cpp/05.html#arrays",
    "href": "topics/cpp/05.html#arrays",
    "title": "Collection Data Types",
    "section": "Arrays",
    "text": "Arrays\nAn array data structure is an ordered arrangement of values located at equally spaced addresses in contiguous computer memory. The fact that array elements are stored in memory in contiguous memory locations making look-up via index very, very fast. In computing, a word is the unit of data used by a particular processor design, such as 32 or 64 bits. For example, an array of 100 integer variables, with indices 0 through 99, might be stored as 100 words at memory addresses 20000, 20004, 20008, … 20396. The element with index i would be located at the address 20000 + 4 × i.\nC++ arrays can be allocated in two different ways:\n\n\n\n\n\n\n\n\nAllocation\nDescription\nUse case\n\n\n\n\nStatic\nThe array size is fixed at compile-time and cannot change\nSpeed is essential or where hardware constraints exist (real-time or low-level processing)\n\n\nDynamic\nPointers are used in the allocation process so the size can change at run-time\nTypically used when more flexibility is required\n\n\n\nRemark. As a Python programmer, you can think of the array as the ancestor of the Python list, and you might remember that Python lists are actually implemented via an underlying array consisting of references.\nStatic arrays. This can be initialized by indicating both type and size (explicit or implicit):\ndouble a[10];\nstring s[] = {\"this\", \"is\", \"an\", \"array\", \"of\", \"strings\"};\nNote that as tradeoff for efficiency, C++ arrays dont offer the same protections as Python:\n\nfrom utils import runcpp\n\nHere we did out of bounds access:\n\n%%runcpp --exitcode=true\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    int A[] = {1, 2, 3, 4, 5};\n    cout &lt;&lt; A[4] &lt;&lt; \" \" &lt;&lt; &A[4] &lt;&lt; endl;\n    cout &lt;&lt; A[5] &lt;&lt; \" \" &lt;&lt; &A[5] &lt;&lt; endl;      // out of bounds access\n    return 0;\n}\n\ng++ -std=c++23 ./code/tmp.cpp -o ./code/tmp\n./code/tmp\n\n\n\n./code/tmp.cpp:7:13: warning: array index 5 is past the end of the array (that has type 'int[5]') [-Warray-bounds]\n    7 |     cout &lt;&lt; A[5] &lt;&lt; \" \" &lt;&lt; &A[5] &lt;&lt; endl;      // out of bounds access\n      |             ^ ~\n./code/tmp.cpp:5:5: note: array 'A' declared here\n    5 |     int A[] = {1, 2, 3, 4, 5};\n      |     ^\n1 warning generated.\n\n\n5 0x16b7ae930\n1 0x16b7ae934\n0\n\n\nIt’s nice that we get a warning from our compiler. But the exit code is still 0.\nRemark. Observe that the addresses are spaced 4 apart. These are bytes which are the basic addressable unit of memory in most modern computer architectures. A byte consist of 8 bits (0 or 1). Primary data tpes are stored in multiple bytes:\n\nchar: 1 byte = 8 bits\nint / float: 4 bytes = 32 bits\ndouble: 8 bytes = 64 bits"
  },
  {
    "objectID": "topics/dlsys/02.html",
    "href": "topics/dlsys/02.html",
    "title": "Softmax Regression",
    "section": "",
    "text": "Suppose you want to write a program that will classify handwritten drawing of digits into their appropriate category: 0, 1, 2, …, 9.\nYou could, think hard about the nature of digits, try to determine the logic of what indicates what kind of digit, and write a program to codify this logic. Or you could take advantage of the statistics of the data, e.g. pixel intensity in a 28 x 28 grid as discriminative features of each instance."
  },
  {
    "objectID": "topics/dlsys/02.html#ml-as-data-driven-programming",
    "href": "topics/dlsys/02.html#ml-as-data-driven-programming",
    "title": "Softmax Regression",
    "section": "ML as data-driven programming",
    "text": "ML as data-driven programming\nML approach: Collect a training set of images with known labels and feed these into a machine learning algorithm, which, if done well, will automatically produce a “program” that solves this task. The said program includes a large number of magic numbers, but it nonetheless performs a sequence of computations to determine the output class."
  },
  {
    "objectID": "topics/dlsys/02.html#three-ingredients-of-a-ml-algorithm",
    "href": "topics/dlsys/02.html#three-ingredients-of-a-ml-algorithm",
    "title": "Softmax Regression",
    "section": "Three ingredients of a ML algorithm",
    "text": "Three ingredients of a ML algorithm\nEvery machine learning algorithm consists of three different elements:\n\nHypothesis class (\\(\\mathcal{H}\\)). The “program structure”, parameterized via a set of parameters, that describes how we map inputs (e.g. images of digits) to outputs (e.g. class labels, or probabilities of different class labels). Formally, \\(\\mathcal{H} = \\{h_\\Theta \\mid \\Theta \\in \\mathbb{R}^d \\}\\) where \\(h_\\Theta(\\mathbf{x}) = \\hat{y}\\) or \\(\\hat{\\mathbf{p}}\\) for an input \\(\\mathbf{x}.\\)\nLoss function (\\(\\ell\\)). A function that specifies how “well” a given hypothesis (i.e. a choice of parameters) performs on the task of interest. Thus, we have the loss \\(\\mathcal{L}(h_\\Theta, \\mathcal{D})\\) where \\(\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N \\subset \\mathcal{X} \\times \\mathcal{Y}\\) is a finite subset of size \\(N\\) of the product of the input space \\(\\mathcal{X}\\) and target space \\(\\mathcal{Y}.\\) The loss is expressed in terms of a pointwise loss function \\(\\ell\\) such that: \\[\\mathcal{L}(h_\\Theta, \\mathcal{D}) = \\frac{1}{N}\\sum_{i=1}^N \\ell(h_\\Theta(\\mathbf{x}_i), y_i)\\] where \\(\\ell\\) outputs nonnegative real numbers and \\(\\ell \\to 0\\) whenever the predictions are accurate, otherwise \\(\\ell \\to \\infty\\) as the predictions become increasingly worse.\nAn optimization method. A procedure for determining a set of parameters that (approximately) minimize the training loss: \\[\n\\Theta^* \\approx {\\text{argmin}}_{\\Theta} \\; \\mathcal{L}(h_\\Theta, \\mathcal{D}).\n\\] Note that the training dataset is fixed while we vary the parameters to determine a suitable hypothesis. The exact procedure for finding the optimal parameters \\(\\Theta^*\\) depends on an optimization algorithm. For deep learning, this is typically SGD and its variants."
  },
  {
    "objectID": "topics/dlsys/02.html#multi-class-classification",
    "href": "topics/dlsys/02.html#multi-class-classification",
    "title": "Softmax Regression",
    "section": "Multi-class classification",
    "text": "Multi-class classification\nIn a multi-class classification setting, we have a training dataset \\(\\mathcal{D}\\) that consist of input-output pairs \\(\\mathcal{D} = \\mathcal{X} \\times \\mathcal{Y} = \\{(\\mathbf{x}_i, y_i) \\mid i = 1, \\ldots, N \\}\\) such that \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) where \\(d\\) is the input dimensionality and \\(y_i \\in [1, K] \\subset \\mathbb{Z}\\) where \\(K\\) is the number of classes. Here \\(N = | \\mathcal{D} |\\) is the size of the dataset.\nA hypothesis function \\(h\\) in this setting maps inputs \\(\\mathbf{x}\\) to \\(K\\)-dimensional vectors: \\(h \\colon \\; \\mathbb{R}^d \\to \\mathbb{R}^K.\\) The output \\(h_j(\\mathbf{x})\\) indicates some measure of “belief” in how much likely the label is to be class \\(j\\). That is, the most likely class for an input \\(\\mathbf{x}\\) is predicted as the coordinate \\(\\hat{j} = \\text{argmax}_j \\; h_j(\\mathbf{x})\\).\nExample. For MNIST, \\(d = 28 \\times 28 = 784\\), \\(K = 10\\) and \\(M = 60,000.\\)"
  },
  {
    "objectID": "topics/dlsys/02.html#linear-hypothesis-class",
    "href": "topics/dlsys/02.html#linear-hypothesis-class",
    "title": "Softmax Regression",
    "section": "Linear hypothesis class",
    "text": "Linear hypothesis class\nA linear hypothesis function has matrix multiplication as core operation:\n\\[h_\\Theta(\\mathbf{x}) = \\Theta^\\top \\mathbf{x}\\]\nfor parameters \\(\\Theta \\in \\mathbb{R}^{d \\times K}.\\) In practice, we usually write this using matrix-batch notation since we process inputs in parallel as a matrix:\n\\[h_\\Theta(\\mathbf{X}) = \\mathbf{X} \\Theta\\]\nwhere\n\\[\n\\begin{equation}\n\\mathbf{X}=\\left[\\begin{array}{c}\n-\\, \\mathbf{x}^{(1)\\top}- \\\\\n\\vdots \\\\\n-\\, \\mathbf{x}^{(M)\\top}-\n\\end{array}\\right] \\in \\mathbb{R}^{M \\times d}\n\\end{equation}.\n\\]\nThat is, the inputs are laid out as row vectors inside the matrix. This allows us to multiply the parameter matrix without transposing:\n\\[\nh_\\Theta(\\mathbf{X}) =  \\underbrace{\\mathbf{X}}_{\\mathbb{R}^{M \\times d}} \\; \\underbrace{\\Theta}_{\\mathbb{R}^{d \\times K}} \\in \\mathbb{R}^{M \\times K}.\n\\]\nThis makes sense, we have \\(M\\) vectors of length \\(K.\\) Note that geometrically each \\(\\Theta_j = \\Theta_{[:, j]} \\in \\mathbb{R}^d\\) defines a separating hyperplane for class \\(j \\in [K].\\) So a linear hypothesis class is able to learn to separate linearly separable data points in \\(\\mathbb{R}^d\\) using \\(K\\) separating hyperplanes by assigning a score \\(s_j = \\Theta_j^\\top \\mathbf{x} \\in \\mathbb{R}\\) based on its weighted distance from the hyperplane."
  },
  {
    "objectID": "topics/dlsys/02.html#loss-functions-softmax-and-cross-entropy",
    "href": "topics/dlsys/02.html#loss-functions-softmax-and-cross-entropy",
    "title": "Softmax Regression",
    "section": "Loss functions, softmax, and cross-entropy",
    "text": "Loss functions, softmax, and cross-entropy\nThe simplest loss is just the classification error: \\[\\ell_{\\text{err}} = 1 - [[\\operatorname{argmax}_i h_i(\\mathbf{x})= y]] \\in \\{0, 1\\}.\\]\nWe typically use this loss function to assess the quality of classifiers. Unfortunately, the error is a bad loss function to use for optimization, because it is not differentiable, i.e. we can smoothly adjust the parameters without seeing a change in \\(\\ell_{\\text{err}}\\) or that it changes abruptly.\nInstead, we look at the probabilities assigned by the model to each class. To do this, we have to convert the class scores to probabilities exponentiating and normalizing its entries (i.e. making \\(\\sum_j p = 1\\) s.t. \\(p_j \\geq 0\\)). Class scores \\(h_j(\\mathbf{x}) = \\Theta_j^\\top \\mathbf{x}\\) are exponentiated before normalizing:\n\\[\np_j = \\frac{\\exp(h_j(\\mathbf{x}))}{\\sum_l \\exp(h_l(\\mathbf{x}))} \\eqqcolon \\text{Softmax} (h(\\mathbf{x}))_j.\n\\]\nThen, the cross-entropy loss is given by the negative log of the probability of the true class \\(y\\):\n\\[\n\\begin{aligned}\n\\ell_{\\text{CE}}(h(\\mathbf{x}), y)\n&= -\\log \\text{Softmax} (h(\\mathbf{x}))_y \\\\\n&= -h_y(\\mathbf{x})+\\log \\sum_{j=1}^K \\exp \\left(h_j(\\mathbf{x})\\right).\n\\end{aligned}\n\\]\nRemark. Exponentiating means that scores does not scale linearly. In fact,\n\\[\np_j = \\frac{\\exp(\\Delta h_j(\\mathbf{x}))}{\\sum_l \\exp(\\Delta h_l(\\mathbf{x}))}\n\\]\nwhere \\(\\Delta h_l(\\mathbf{x}) = h_l(\\mathbf{x}) - \\max_{m} h_m(\\mathbf{x}).\\) This prevents both underflow and overflow with \\(\\log (1 + \\sum a_j)\\) where \\(0 &lt; a_j \\leq 1\\), but also shows that the individual scores scale exponentially with the diff from the largest score. Hence, this transformation is sometimes called soft-argmax.\n\n\nCode\n%config InlineBackend.figure_formats = ['svg'] \nimport torch\nimport matplotlib.pyplot as plt\n\nz = torch.tensor([1, 2, 3]).float()\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].bar(range(3), z.numpy(), label=\"z\")\nax[1].bar(range(3), z.exp().numpy(), color=\"C1\", label=\"exp(z)\")\nax[2].bar(range(3), z.exp().numpy() / z.exp().numpy().sum(), color=\"C2\", label=\"Softmax(z)\")\nax[0].legend(); ax[1].legend(); ax[2].legend();\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nFinally, taking the negative log of the true class suffices since we have the constraint \\(\\sum p_j = 1.\\) Moreover, \\(p_y = 1\\) implies \\(\\ell_{\\text{ce}} = -\\log 1 = 0\\) while \\(p_y = 0\\) implies \\(-\\log 0 = +\\infty.\\) Observe that the penalized most severely when the model is confidently wrong.\n\n\nCode\nimport numpy as np\neps = 1e-5\np = np.linspace(0 + eps, 1, 10000)\nplt.figure(figsize=(5, 4))\nplt.plot(p, -np.log(p), linewidth=2, label=\"-log(p)\")\nplt.grid(alpha=0.6, linestyle=\"dashed\")\nplt.xlabel(\"p\"); plt.ylabel(\"NLL\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nMeanwhile, looking at the gradient, the NLL has the nice property that it does not saturate as it approaches perfect prediction:\n\\[\\frac{\\partial\\ell_{\\text{NLL}}}{\\partial p_y} = -\\frac{1}{p_y}.\\]"
  },
  {
    "objectID": "topics/dlsys/02.html#optimization-problem",
    "href": "topics/dlsys/02.html#optimization-problem",
    "title": "Softmax Regression",
    "section": "Optimization problem",
    "text": "Optimization problem\nThe third ingredient of a machine learning algorithm is a method for solving the associated optimization problem, i.e. the problem of minimizing the average loss on the training set:\n\\[\n\\begin{equation}\n\\hat{\\Theta} = \\underset{\\Theta}{\\operatorname{min}} \\frac{1}{N} \\sum_{i=1}^N \\ell_{\\text{CE}} (h_\\Theta(\\mathbf{x}_i), y_i)\n\\end{equation}\n\\]\nHow do we find an optimal set of parameters \\(\\hat{\\Theta}\\)?\n\nGradient Descent\nFor a matrix-input, scalar-output function \\(f\\colon \\mathbb{R}^{d \\times k} \\to \\mathbb{R}\\) the gradient \\(\\nabla_\\Theta f(\\Theta)\\) is defined as the matrix of partial derivatives:\n\\[\n\\begin{equation}\n\\nabla_\\Theta f(\\Theta) =\\left[\\begin{array}{ccc}\n\\frac{\\partial f(\\Theta)}{\\partial \\Theta_{11}} & \\cdots & \\frac{\\partial f(\\Theta)}{\\partial \\Theta_{1 k}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\Theta)}{\\partial \\Theta_{d 1}} & \\cdots & \\frac{\\partial f(\\Theta)}{\\partial \\Theta_{d k}}\n\\end{array}\\right] \\in \\mathbb{R}^{d \\times k}\n\\end{equation}.\n\\]\nNOTE: \\(\\nabla_\\Theta f\\) always has the same shape as \\(\\Theta\\) when \\(f\\) is a scalar.\nFrom the multivariate Taylor expansion\n\\[\nf(\\Theta + \\Delta \\Theta) \\approx f(\\Theta) + \\nabla_\\Theta f(\\Theta) \\cdot \\Delta\\Theta + \\mathcal{O}(\\Delta\\Theta^2).\n\\]\nSo that the gradient locally points in the direction that most increases \\(f\\), i.e. to first order. Hence, to minimize \\(f\\), we iteratively update the weight by \\(-\\nabla_\\Theta f\\) at each point in the surface defined by \\(f\\):\n\\[\n\\Theta_{t + 1} = \\Theta_t - \\alpha \\cdot \\nabla_\\Theta f (\\Theta_t)\n\\]\nwhere \\(\\alpha &gt; 0\\) is called the learning rate. GD naturally is sensitive to the scale of the learning rate:\n\n\n\nStochastic Gradient Descent (SGD)\nIn practice, we don’t typically want to compute the gradient using all examples to make a single update to the parameter. This is costly in cases where \\(N \\gg 1\\) which is typical in deep learning. Instead, we take many gradient steps where each update is based on a randomly sampled subset mini-batch \\(\\mathcal{B} \\subset \\mathcal{D}\\) where \\(B = |\\mathcal{B}| \\ll N.\\)\nAlgorithm. (SGD) 1. Sample \\(\\mathcal{B} \\subset \\mathcal{D}\\) so that we get \\(\\mathbf{X}_\\mathcal{B} \\in \\mathbb{R}^{B \\times d}\\) and \\(\\mathbf{y} \\in [K]^B.\\) 2. Update parameters: \\[\\begin{aligned}\\Theta_{t + 1}\n= \\Theta_t - \\frac{\\alpha}{B} \\, \\sum_{b \\in I_\\mathcal{B}} \\nabla_\\Theta \\ell (h_{\\Theta_t}(\\mathbf{x}_b), y_b).\\end{aligned}\\]\n\\(\\square\\)\nIt follows that the sample dataset varies at each training step. Unlike the previous case where \\(\\mathcal{D}\\) is fixed. This mechanism of SGD reduces overfitting by implicit regularization of the gradient, i.e. adding noise in the training process.\n\n\nGradient of cross-entropy\nHow do we actually compute \\(\\mathcal{L}_{\\text{CE}}\\)? This can be done using the chain rule and tracking functional dependencies. Recall\n\\[\n\\ell_{\\text{CE}}(h_\\Theta(\\mathbf{x}), y) = -h_\\Theta(\\mathbf{x})_y + \\log \\sum_{j=1}^K \\exp \\left(h_\\Theta(\\mathbf{x})_j\\right).\n\\]\nLet’s start by deriving the gradient of the softmax loss itself. For a vector \\(\\mathbf{h} \\in \\mathbb{R}^K\\):\n\\[\n\\frac{\\partial \\ell_{\\text{CE}}}{\\partial h_j} = - \\delta_{yj} + \\frac{\\exp h_j}{\\sum_{l=1}^K \\exp h_l} = - \\delta_{yj} + p_j.\n\\]\nIn vector form, \\(\\nabla_{\\mathbf{h}} \\ell_{\\text{CE}} = \\mathbf{p} - \\mathbf{e}_y\\) where \\(\\mathbf{e}_y\\) is a one-hot vector with 1 on index \\(y.\\)\nNext, to calculate the derivative with respect to \\(\\Theta\\), we use the chain rule:\n\\[\n\\frac{\\partial \\ell_{\\text{CE}}}{\\partial \\Theta_{ul}} =  \\frac{\\partial \\ell_{\\text{CE}}}{\\partial h_j} \\frac{{\\partial h_j}}{\\partial \\Theta_{ul}} = \\underbrace{(p_l - \\delta_{yl})}_{K-\\text{dim}} \\; \\underbrace{\\vphantom{(}x_u}_{d-\\text{dim}}.\n\\]\nFor the dimensions to make sense, \\(\\frac{\\partial \\ell_{\\text{CE}}}{\\partial \\Theta} = \\mathbf{x}(\\mathbf{p} - \\mathbf{e}_y)^\\top\\) in matrix form. Recall that our vectors are column vectors. Here the product reverses since we traverse the dependence backwards from the loss.\nBatch form. The same process works for a batch of inputs, except that we have an additional batch index which we sum over since \\(\\ell\\) depends on all input instances. The contribution of each input is matched and aggregated using matrix multiplication:\n\\[\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial \\Theta} = \\frac{1}{B}\\,\\underbrace{\\vphantom{(}\\mathbf{X}^\\top}_{d \\times M} \\;\\; \\underbrace{(\\mathbf{P} - \\mathbf{E}_{\\mathbf{y}})}_{M \\times K}.\n\\]\nHere the transposes switched since \\(\\mathbf{X}\\) is constructed such that it has rows of \\(\\mathbf{x}^\\top,\\) hence we internally get a double transpose. Putting it all together, we can write the SGD update rule for softmax regression as follows:\n\\[\n\\Theta_{t + 1} = \\Theta_t - \\frac{\\alpha}{B} \\cdot \\mathbf{X}^\\top (\\mathbf{P} - \\mathbf{E}_{\\mathbf{y}}).\n\\]\nHere we have \\(\\frac{1}{B}\\) since \\(\\mathcal{L} = \\frac{1}{B}\\sum_b { \\ell}_b.\\) Also it makes sense to scale down since the sum grows with batch size \\(B.\\)"
  },
  {
    "objectID": "topics/dlsys/02.html#code-implementation",
    "href": "topics/dlsys/02.html#code-implementation",
    "title": "Softmax Regression",
    "section": "Code implementation",
    "text": "Code implementation\nWe will train a classification model based on the SGD update rule above:\n\nimport torch\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))  # mean and std for MNIST\n])\n\ntrain_dataset = datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\n\ntest_dataset = datasets.MNIST(\n    root='./data',\n    train=False,          # Specify test dataset\n    download=True,\n    transform=transform\n)\n\nDefining the linear model:\n\nimport torch.nn as nn\nmodel = nn.Linear(784, 10, bias=False)\n\nRemark. The linear model can be extended to have a bias vector \\(\\beta\\), so that \\(h_\\Theta = \\mathbf{X}\\Theta + \\beta\\) where \\(\\beta \\in \\mathbb{R}^K.\\) But it turns out there is a “bias trick” in deep learning where an additional dimension containing only \\(+1\\) is added so that the input becomes of shape \\((M, d + 1).\\) For simplicity, we stick with no bias.\n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nloss_train = []\nN_EPOCHS = 5\nB = 16\nALPHA = 0.01\n\n@torch.no_grad()\ndef sgd_step(x, y):\n    x = x.reshape(-1, 784)\n    h = model(x)\n\n    for theta in model.parameters():\n        p = F.softmax(h, dim=1)\n        e = F.one_hot(y, num_classes=10)\n        g = x.T @ (p - e)\n\n        theta -= ALPHA / B * g.T\n\n    loss = -p[torch.arange(B), y].log().mean()\n    return loss.item()\n\n\ndl = DataLoader(train_dataset, batch_size=B, shuffle=True)  # shuffle for SGD!\nfor _ in tqdm(range(N_EPOCHS)):\n    for x, y in dl:\n        loss = sgd_step(x, y)\n        loss_train.append(loss)\n\n100%|██████████| 5/5 [00:29&lt;00:00,  5.92s/it]\n\n\nRemark. Pytorch nn.Linear computes x @ θ.T + b hence we take g.T before updating the parameter θ.\n\n\nCode\nplt.plot(np.array(loss_train).reshape(-1, 10).mean(1), label=\"train\")\nplt.xlabel(\"step\")\nplt.ylabel(\"loss\")\nplt.grid(alpha=0.3, linestyle=\"dashed\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nLabel prediction sample:\n\ndv = DataLoader(test_dataset, batch_size=B, shuffle=False)\nx, y = next(iter(dv))\nout = model(x.reshape(B, -1)).argmax(1)\nacc = (out == y).float()\nprint(\"Batch acc:\", f\"{acc.sum().int()}/{B}\", f\"({acc.mean().item() * 100}%)\")\n\nBatch acc: 15/16 (93.75%)\n\n\n\n\nCode\nfig, ax = plt.subplots(4, 4)\nfor i in range(B):\n    a, b = divmod(i, 4)\n    ax[a, b].imshow(x[i].reshape(28, 28), cmap=\"Greys\")\n    ax[a, b].set_title(f\"pred: {out[i]}\")\n    ax[a, b].axis(\"off\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nTest accuracy. The practical goal of training is not actually to minimize \\(\\mathcal{L}(h_\\Theta, \\mathcal{D}).\\) But to minimize the loss for samples outside of the training dataset. That is, the model should be accurate on test data. If the model does not overfit and the test distribution does not drift too far from the training distribution, then we should be good.\n\ntot = 0\nacc = 0\nfor x, y in dv:\n    out = model(x.reshape(B, -1)).argmax(1)\n    correct = (out == y).float()\n    tot += len(y)\n    acc += correct.sum()\n\nprint(f\"Test acc: {acc / tot * 100:.2f}%\")\n\nTest acc: 92.18%"
  },
  {
    "objectID": "topics/dlsys/02.html#appendix-model-complexity",
    "href": "topics/dlsys/02.html#appendix-model-complexity",
    "title": "Softmax Regression",
    "section": "Appendix: Model complexity",
    "text": "Appendix: Model complexity\nSince SGD relies on a stochastic process, the performance of the resulting trained model varies. How are we sure that we aren’t just lucky for this particular run / random seed? What is the variance of the trained model performance over multiple runs? Is there a way to control this? This issue is at the core of learning theory and precisely what the bias-variance tradeoff addresses.\nIn practice, the crucial parameter to control is model complexity. Here model capacity or complexity is a measure of how complicated a pattern or relationship a model architecture can express. Let \\(f\\) be the true function that underlies the task. If model capacity is sufficiently large, the model class \\(\\mathcal{F} = \\{f_{\\Theta} \\mid \\Theta \\in \\mathbb{R}^d \\}\\) contains an approximation \\(\\hat{f} \\in \\mathcal{F}\\) such that \\(\\| f - \\hat{f} \\| &lt; \\epsilon\\) for a small enough \\(\\epsilon &gt; 0.\\)\nThe capacity of a model class can be controlled, for example, by the number of learnable parameters in practical architectures. It can also be constrained directly by applying regularization or certain prior knowledge such as invariances. This biases the model towards certain solutions, so these constraints are sometimes referred to as inductive biases — such knowledge is bias in the sense that it makes some solutions more likely, and others less likely. The tradeoff is that the model are steered to biased solutions more efficiently."
  }
]