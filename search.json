[
  {
    "objectID": "topics/tooling/runpod.html",
    "href": "topics/tooling/runpod.html",
    "title": "Runpod Dev Environment Setup",
    "section": "",
    "text": "SSH implements public-key cryptography to establish trust between client and server systems. In this paradigm, we have the public key (.pub) that functions as an access verifier, installed on target systems (like Runpod nodes), and the private key serves as the unique authentication factor for mathematically proving identity.\nBelow, we create public keys which we install onto Runpod and GitHub systems. The corresponding private key for Runpod is stored locally, while that for GitHub is securely copied via scp. This gives every new pod read and write access to GitHub repositories.",
    "crumbs": [
      "Tooling",
      "Runpod Dev Environment Setup"
    ]
  },
  {
    "objectID": "topics/tooling/runpod.html#give-local-ssh-access-to-pods",
    "href": "topics/tooling/runpod.html#give-local-ssh-access-to-pods",
    "title": "Runpod Dev Environment Setup",
    "section": "Give local SSH access to pods",
    "text": "Give local SSH access to pods\n\nGenerate SSH keys for Runpod (no passphrase):\n\n\n\n(local)\n\nssh-keygen -t ed25519 -C \"runpod\" -f ~/.ssh/runpod -N \"\"\ncat ~/.ssh/runpod.pub\n\n\nAdd this to Settings&gt; SSH Public Keys in Runpod (separated by newline).",
    "crumbs": [
      "Tooling",
      "Runpod Dev Environment Setup"
    ]
  },
  {
    "objectID": "topics/tooling/runpod.html#give-pods-ssh-access-to-github",
    "href": "topics/tooling/runpod.html#give-pods-ssh-access-to-github",
    "title": "Runpod Dev Environment Setup",
    "section": "Give pods SSH access to GitHub",
    "text": "Give pods SSH access to GitHub\n\nGenerate SSH keys to access GitHub from each pod (no passphrase):\n\n\n\n(local)\n\nssh-keygen -t ed25519 -C \"runpod-github\" -f ~/.ssh/runpod_github -N \"\"\ncat ~/.ssh/runpod_github.pub\n\n\nAdd the public SSH key to your GitHub account under Settings&gt; SSH and GPG Keys:\n\n\n\nRun the following script. Then, copy your podâ€™s SSH over exposed TCP connection string1. Paste the connection string in the resulting input prompt. You should verify the auto-filled values for pod IP and port number before proceeding.\n\n\n\n(local)\n\necho -n \"\\nSSH over exposed TCP connection string: \\n\"\nread RUNPOD_SSH\n\nRUNPOD_IP=$(echo \"$RUNPOD_SSH\" | sed -E 's/.*@([0-9.]+).*/\\1/')\nRUNPOD_PORT=$(echo \"$RUNPOD_SSH\" | sed -E 's/.*-p ([0-9]+).*/\\1/')\necho \"\"\necho \"IP:   $RUNPOD_IP\"\necho \"Port: $RUNPOD_PORT\"\n\n\nRun the following script. This copies the GitHub private SSH key, adds it to the SSH agent, and logs in to the instance. You should now be connected to your pod! ðŸ«› Also, you can check that you have access to your repositories.\n\n\n\n(local)\n\n0ssh-keyscan -p $RUNPOD_PORT $RUNPOD_IP &gt;&gt; ~/.ssh/known_hosts\n\nscp -P $RUNPOD_PORT -i ~/.ssh/runpod \\\n    ~/.ssh/runpod_github root@$RUNPOD_IP:/root/.ssh/runpod_github\n\nssh -t -p $RUNPOD_PORT -i ~/.ssh/runpod root@$RUNPOD_IP '\n    # Set secure permissions for SSH directory and key\n1    chmod 700 ~/.ssh\n2    chmod 600 ~/.ssh/runpod_github\n\n    # Add GitHub to known_hosts and set permissions\n3    ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts\n4    chmod 644 ~/.ssh/known_hosts\n\n    # Start SSH agent and load the key\n5    eval \"$(ssh-agent -s)\" &gt; /dev/null;\n6    ssh-add ~/.ssh/runpod_github\n\n    # Launch interactive shell to keep the session alive\n7    exec bash -l\n'\n\n\n0\n\nAppend podâ€™s public SSH host key to local known_hosts to skip authenticity prompts.\n\n1\n\nPermissions: only owner can read, write, or execute directory.\n\n2\n\nPermissions: only owner can read or write on the private key file.\n\n3\n\nAppend GitHubâ€™s public SSH host key to pod known_hosts to avoid authenticity prompts.\n\n4\n\nPermissions: known_hosts owner can read/write, others have read-only access.\n\n5\n\nRun the SSH agent process in the background.\n\n6\n\nAdd GitHub private key to SSH agent.\n\n7\n\nOnce setup completes, exec bash -l starts a login shell (-l) that preserves the environmental variables of the last one. The -t flag ensures proper handling of the exec command.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe ssh-agent process persists in the background once started, holding your loaded keys in memory. What does not persist are the environment variables (SSH_AUTH_SOCK, SSH_AGENT_PID) that let your shell know how to communicate with that agent. When you open a new shell or SSH session, those env vars are not automatically set, so your shell canâ€™t find the running agent unless you restore them (e.g.Â doing ssh-add again). This is why we want all setup and SSH connection to happen in one continuous session.",
    "crumbs": [
      "Tooling",
      "Runpod Dev Environment Setup"
    ]
  },
  {
    "objectID": "topics/tooling/runpod.html#virtual-environment",
    "href": "topics/tooling/runpod.html#virtual-environment",
    "title": "Runpod Dev Environment Setup",
    "section": "Virtual environment",
    "text": "Virtual environment\nInside the pod, we can now clone repositories. For example:\n\n\n(root@runpod)\n\ngit clone git@github.com:particle1331/ai-notebooks.git\n\nIn this project, we use uv to build an environment synced using uv.lock. See ListingÂ 1 below. This creates a virtual env .venv that can be used as notebook kernel:\n\n\n(root@runpod)\n\nmake venv\n\nOr skip uv entirely and just use pip:\n\n\n(root@runpod)\n\nmake requirements\npip install -r requirements.txt\npip install -e .\n\n\n\n\n\n\n\nNote\n\n\n\nSome environments, like AzureML where compute instances have network-mounted or ephemeral filesystems, can cause uv failures. In the case of short-lived pods, you donâ€™t necessarily need to perfectly setup an environment or maintain a clean state. On the other hand, reproducibility benefits from a controlled, well-defined environment. As usual, the level of precision needed depends on the scope of the project.",
    "crumbs": [
      "Tooling",
      "Runpod Dev Environment Setup"
    ]
  },
  {
    "objectID": "topics/tooling/runpod.html#quarto-docs",
    "href": "topics/tooling/runpod.html#quarto-docs",
    "title": "Runpod Dev Environment Setup",
    "section": "Quarto docs",
    "text": "Quarto docs\nCheck here for the Quatro version that weâ€™re using. Adjust the following variable accordingly:\nexport QUARTO_VERSION=1.7.32  # may be outdated\nwget -q https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb\ndpkg -i quarto-${QUARTO_VERSION}-linux-amd64.deb && rm quarto-${QUARTO_VERSION}-linux-amd64.deb\nPreview then port-forward using vscode:\n\n\n(root@runpod)\n\nmake docs",
    "crumbs": [
      "Tooling",
      "Runpod Dev Environment Setup"
    ]
  },
  {
    "objectID": "topics/tooling/runpod.html#appendix-makefile",
    "href": "topics/tooling/runpod.html#appendix-makefile",
    "title": "Runpod Dev Environment Setup",
    "section": "Appendix: Makefile",
    "text": "Appendix: Makefile\n\n\n\nListingÂ 1: Makefile for the project.\n\n\n\n\nMakefile\n\n.PHONY: docs uv venv requirements\n\ndocs:\n    quarto preview\n\nuv:\n    pip install --upgrade pip\n    pip install uv\n\nrequirements: uv\n    uv pip compile pyproject.toml &gt; requirements.txt\n\nvenv: uv\n    uv venv --python 3.13\n    uv sync",
    "crumbs": [
      "Tooling",
      "Runpod Dev Environment Setup"
    ]
  },
  {
    "objectID": "topics/tooling/runpod.html#footnotes",
    "href": "topics/tooling/runpod.html#footnotes",
    "title": "Runpod Dev Environment Setup",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ne.g.Â ssh root@63.141.33.33 -p 22011 -i ~/.ssh/id_ed25519â†©ï¸Ž",
    "crumbs": [
      "Tooling",
      "Runpod Dev Environment Setup"
    ]
  },
  {
    "objectID": "topics/deep/index.html",
    "href": "topics/deep/index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep learning systems solved problems that were considered hard prior to 2010, e.g.Â obtaining superhuman / SOTA scores on tasks and challenges such as ImageNet, CASP, and Go (board game)1. Later, in the early 2020s, unprecedented progress in text & image generation were made with models like GPT-3 and Stable Diffusion:",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/index.html#course-overview",
    "href": "topics/deep/index.html#course-overview",
    "title": "Deep Learning",
    "section": "Course Overview",
    "text": "Course Overview\nThis course provides a practical introduction to the core ideas and techniques behind modern deep learning systems. We put focus on both conceptual understanding and practical skills for building and training neural networks in PyTorch and related frameworks.",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/index.html#learning-objectives",
    "href": "topics/deep/index.html#learning-objectives",
    "title": "Deep Learning",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this course, you will be able to:\n\nUnderstand the basic functioning of modern deep learning libraries\nExplain and implement concepts like autodifferentiation and gradient-based optimization\nImplement standard DL architectures (MLPs, ConvNets, RNNs, Seq2Seq, Transformers)\nUnderstand how hardware acceleration (on GPUs) works under the hood\nBe able to develop your own highly efficient code for modern deep learning",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/index.html#references",
    "href": "topics/deep/index.html#references",
    "title": "Deep Learning",
    "section": "References",
    "text": "References\n\nCMU 10-414/714: Deep Learning Systems (2022)",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/index.html#footnotes",
    "href": "topics/deep/index.html#footnotes",
    "title": "Deep Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGame tree complexity of 10^{360} at 250 moves over 150 move games.â†©ï¸Ž",
    "crumbs": [
      "Deep Learning"
    ]
  },
  {
    "objectID": "topics/deep/03.html",
    "href": "topics/deep/03.html",
    "title": "Automatic Differentiation",
    "section": "",
    "text": "For neural nets, any layer with m inputs and n weights can be written as\n\\mathbf{Z} = f_{\\Theta^1, \\ldots, \\Theta^n}(\\mathbf{Z}_{\\text{in}}^1, \\ldots, \\mathbf{Z}_{\\text{in}}^m).\nHence, for each layer / OP it sufficies to specify (1) input gradients {\\partial \\mathbf{Z}}/{\\partial{\\mathbf{Z}_{\\text{in}}^i}} for i = 1, \\ldots, m and (2) weight gradients {\\partial \\mathbf{Z}}/{\\partial{\\Theta}^k} for k= 1, \\ldots, n. So that these gradients can be accessed by any node {\\mathbf{Z}} that \\mathbf{U} is a parent of. Thus,\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{U}}\n=\n\\sum_{{\\mathbf{Z}} \\in C(\\mathbf{U})} \\pi\\left(\n    \\frac{\\partial \\mathcal{L}}{\\partial {{\\mathbf{Z}}}},  \n    \\frac{\\partial {\\mathbf{Z}}}{\\partial {\\mathbf{U}}}\n\\right)\nwhere C(\\mathbf{U}) = \\{ \\mathbf{Z} \\mid \\exists i, k \\; \\text{s.t.}\\; (\\mathbf{U} = {\\mathbf{Z}}_\\text{in}^i) \\vee (\\mathbf{U} = {\\Theta}^k) \\} are the child nodes of \\mathbf{U} and \\pi is an operator that abstracts the actual product (i.e.Â summing over an index) between the two tensors. Thatâ€™s easy enough for static graphs, but if we want to dynamically build the computational graph for arbitrary programs, then we have to do some abstraction.\nFor example, consider the binary OP \\mathbf{Z} = \\phi(\\mathbf{U}_1, \\mathbf{U}_2). Then, we have to determine \\xi_1 and \\xi_2 where:\n\\begin{aligned}\n\\xi_1(\\mathbf{G}) &= \\pi \\left(\\mathbf{G}, \\frac{\\partial \\phi(\\mathbf{U}_1, \\mathbf{U}_2)}{\\partial\\mathbf{U}_1} \\right) \\\\[1em]\n\\xi_2(\\mathbf{G}) &= \\pi \\left(\\mathbf{G}, \\frac{\\partial \\phi(\\mathbf{U}_1, \\mathbf{U}_2)}{\\partial\\mathbf{U}_2} \\right)\n\\end{aligned}\nwhich will be called during backward pass when we get to the node \\mathbf{Z}, where we set \\mathbf{G} = \\frac{\\partial \\mathcal L}{\\partial \\mathbf Z}. In general, whenever an OP is performed resulting in a node \\mathbf{Z}, we have to track \\xi_i for each dependency \\mathbf{U}_i that takes the gradient \\mathbf{G} = \\partial \\mathcal{L} / \\partial \\mathbf{Z} and calculates \\pi\\left(\n    \\mathbf{G},\n    {\\partial {\\mathbf{Z}}} / {\\partial {\\mathbf{U}_i}}\n\\right). This value is sent over to \\mathbf{U}_i where all such contributions are aggregated to form \\partial \\mathcal{L} / \\partial \\mathbf{U}_i.\nRemark. This locality allows neural network layers to be modular, i.e.Â composed arbitrarily, and new1 layers or ops to be easily integrated into the library. The formulas for local gradients are obtained by manual computation (or by using symbolic libraries). It turns out that the functions \\xi_i implement what is called the vector-Jacobian product (VJP). As a trick when deriving the formulas for a particular input node, we can think of:\nThe output is a contribution to the gradient of the input that is likewise shaped as a vector. Then, weâ€™ll just have to reshape back the Jacobians and the vectors to match the original shapes. Later, thinking of everything as vectors instead of tensors as a mental model will be helpful when implementing tensor ops.",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "topics/deep/03.html#tensors-as-nodes",
    "href": "topics/deep/03.html#tensors-as-nodes",
    "title": "Automatic Differentiation",
    "section": "Tensors as nodes",
    "text": "Tensors as nodes\nFor our purposes, each node in the computational graph is a tensor. This contains two primary attributes, data and grad. Moreover, it contains a list of dependencies or parents, i.e.Â the tensors that the current tensor depends on. The _backward implements functions such as \\xi_1(\\mathbf{G}) and \\xi_2(\\mathbf{G}) defined above and handles routing. Note that nodes may be shared by multiple other nodes in the graph, but we donâ€™t explicitly need to keep track of that.\n\nclass Tensor:\n    def __init__(self, data: np.array, requires_grad=False, parents=()):\n        self.data = data\n        self.grad = None\n        self.parents = parents\n        self.requires_grad = requires_grad\n\n    def _backward(self, parent) -&gt; np.array:\n        \"\"\"Calculate gradient contribution of self to parent.\"\"\"\n        raise NotImplementedError(\"Leaf node has no parents.\")\n\n    def __repr__(self):\n        return f\"Tensor(data={self.data}, requires_grad={self.requires_grad})\"\n    \n    def zero_grad(self):\n        self.grad = np.zeros(self.shape, dtype=np.float64)\n    \n    @property\n    def shape(self):\n        return self.data.shape\n    \n    ...",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "topics/deep/03.html#accumulating-the-gradients",
    "href": "topics/deep/03.html#accumulating-the-gradients",
    "title": "Automatic Differentiation",
    "section": "Accumulating the gradients",
    "text": "Accumulating the gradients\nSince the computational graph is directed, there exists a topological sorting of it. Practically speaking, we want to sort the graph starting from the final node (which, for simplicity, we assume to be scalar), to all other nodes in the graph based on their dependency. This ensures that gradients for every node has been fully aggregated before pushing its gradient to its dependents. This is the basis of the linear ordering, i.e.Â if u = f(v, ...), then i_u &lt; i_v. Here we call v a parent of u.\nBackward pass is implemented as follows:\nclass Tensor\n    ...\n\n    def backward(self):\n        self.grad = 1.0\n        for node in self.sorted_nodes():\n            for parent in node.parents:\n                parent.grad += node._backward(parent)\nInductively, by the time we get to a node, it has accumulated the gradients from all its children since node._backward(parent) sends out the contribution of the current node to its parent. And by construction, each child node occurs before any of its parent nodes, thus the full gradient of a child node is calculated before it is sent to its parent nodes. This relies on properly implementing topological sorting which we cover next.",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "topics/deep/03.html#topologically-sorting-the-graph",
    "href": "topics/deep/03.html#topologically-sorting-the-graph",
    "title": "Automatic Differentiation",
    "section": "Topologically sorting the graph",
    "text": "Topologically sorting the graph\nTo construct the topologically sorted list of nodes of a DAG starting from a terminal node, we use depth-first search. The following example is shown below. The following algorithm steps into dfs for each parent node until a leaf node is reached, which is pushed immediately to topo. Then, the algorithm steps out and the next parent node is processed. A node is only pushed when all its parents have been pushed (i.e.Â already in topo or finished looping through its parents), satisfying the ordering requirement, but in reverse. Hence, the reversal at the end.\n\nfrom collections import OrderedDict\n\nparents = {\n    \"a\": [],\n    \"b\": [],\n    \"x\": [],\n    \"c\": [\"a\", \"b\"],\n    \"d\": [\"x\", \"a\", \"c\"],\n    \"e\": [\"c\"],\n    \"f\": [\"d\"]\n}\n\ndef sorted_nodes(root):\n    \"\"\"Return topologically sorted nodes with self as root.\"\"\"\n    topo = []\n    \n    def dfs(node):\n        print(\"v\", node)\n        if node not in topo:\n            for parent in parents[node]:\n                dfs(parent)\n            \n            topo.append(node)\n            print(\"t\", topo)\n\n    dfs(root)\n    return reversed(topo)\n\nlist(sorted_nodes(\"f\"))\n\nv f\nv d\nv x\nt ['x']\nv a\nt ['x', 'a']\nv c\nv a\nv b\nt ['x', 'a', 'b']\nt ['x', 'a', 'b', 'c']\nt ['x', 'a', 'b', 'c', 'd']\nt ['x', 'a', 'b', 'c', 'd', 'f']\n\n\n['f', 'd', 'c', 'b', 'a', 'x']\n\n\n\n\n\n(a) Graph encoded in the parents dictionary above. Note e, which f has no dependence on, is excluded. Visited nodes (red) starts from the terminal node backwards into the graph. Then, each node is pushed once all its parents are pushed (starting from leaf nodes, yellow), preserving topological ordering. Here a is not pushed twice, even if both d and c depends on it, since a has already been visited after node d. (b) Topological sorting exposes a linear ordering of the compute nodes.",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "topics/deep/03.html#defining-tensor-operations",
    "href": "topics/deep/03.html#defining-tensor-operations",
    "title": "Automatic Differentiation",
    "section": "Defining tensor operations",
    "text": "Defining tensor operations\nRecall that all operations must be defined with specific local gradient computation for BP to work. In this section, we will implement a minimal autograd engine for creating computational graphs. In this section, we will implement operations such as ADD, MATMUL, MUL, POW, NEG, SUM, and SUB (derived from ADD and NEG). This suffices to train a dense neural net with mean-squared error loss for regression.\n\nimport random\nimport numpy as np\nrandom.seed(42)\nnp.random.seed(42)\n\n\nclass Tensor:\n    def __init__(self, data: np.array, requires_grad=False, parents=()):\n        self.data = data\n        self.grad = None\n        self.parents = parents\n        self.requires_grad = requires_grad\n\n    def _backward(self, parent) -&gt; np.array:\n        \"\"\"Calculate gradient contribution of self to parent.\"\"\"\n        raise NotImplementedError(\"Leaf node has no parents.\")\n\n    def __repr__(self):\n        return f\"Tensor(data={self.data}, requires_grad={self.requires_grad})\"\n    \n    def zero_grad(self):\n        self.grad = np.zeros(self.shape, dtype=np.float64)\n    \n    @property\n    def shape(self):\n        return self.data.shape\n\n    def sorted_nodes(self):\n        \"\"\"Return topologically sorted nodes with self as root.\"\"\"\n        topo = []\n\n        def dfs(node):\n            if node not in topo and node.requires_grad:\n                for parent in node.parents:\n                    dfs(parent)\n                    \n                topo.append(node)\n\n        dfs(self)\n        return reversed(topo)\n\n    def backward(self, grad=None):\n        \"\"\"Propagate gradients backward to all parent nodes.\"\"\"\n        self.set_grad(grad)\n        \n        for node in self.sorted_nodes():\n            for parent in node.parents:\n                parent.zero_grad() if parent.grad is None else \"\"\n                parent.grad += node._backward(parent)\n\n    def set_grad(self, grad):\n        if grad is not None:\n            self.grad = grad\n        elif self.shape == ():\n            self.grad = np.array(1.0, dtype=np.float64)\n        else:\n            raise RuntimeError(\"Gradients must be explicitly provided for non-scalar tensors.\")\n\n\nAddition\nNext, we define the supported operations or tensor ops. Observe that only a handful defined above are needed to implement a fully-connected neural net with MSE loss. For sum, c_j = a_j + b_j so the local gradient \\partial c_j / \\partial a_j = 1 and similarly for b_j. The only source of complexity is that binary operations between arrays involve broadcasting. Recall if a has shape (1, 5) and b has shape (3, 2, 5) then, a first becomes ({\\color{50fa7a}1}, {\\color{8be8fc}1}, 5) (left-added dims) and the last axis is effectively repeated to become ({\\color{50fa7a}3}, {\\color{8be8fc}2}, 5). That is, we always add extra dimensions from the left and expand existing axes where the dimension is 1 and the other is \\geq 1.\n\nfrom typing import Union\nimport numpy as np\n\n\nclass AddTensor(Tensor):\n    def __init__(self, a: Tensor, b: Tensor):\n        super().__init__(\n            a.data + b.data, \n1            requires_grad=a.requires_grad or b.requires_grad,\n            parents=(a, b)\n        )\n    \n    def _backward(self, parent) -&gt; np.array:\n        out = self.data\n        diff_ndim = len(out.shape) - len(parent.data.shape)\n\n        # sum out all left-added dims\n2        grad = self.grad\n3        for _ in range(diff_ndim):\n            grad = grad.sum(axis=0)\n\n        # sum out expanded dims (but not added)\n4        for i, dim in enumerate(parent.data.shape):\n            if dim == 1:\n                grad = grad.sum(axis=i, keepdims=True)\n        \n        return grad\n    \n\n# qol: Tensor(a) + 3.0 is nice to have...\nTensorable = Union[Tensor, float, int, np.ndarray]\n\n5def to_tensor(x: Tensorable) -&gt; Tensor:\n    if not isinstance(x, Tensor):\n        x = Tensor(np.array(x), requires_grad=False) \n    return x\n\n# dynamic overloading!\n6Tensor.__add__ = lambda self, other: AddTensor(self, to_tensor(other))\n\n# 3.0 + Tensor(a) \n7Tensor.__radd__ = lambda self, other: AddTensor(to_tensor(other), self)\n\n\n1\n\nChild nodes should also require grad for gradients to reach the parent nodes.\n\n2\n\nFirst, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}^\\prime} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{C}} where \\mathbf{A}^\\prime has shape (3, 2, 5) while \\mathbf{A} has shape (1, 5).\n\n3\n\nSum out leftmost axis to get (3, 2, 5) \\rightarrow (2, 5).\n\n4\n\nWeâ€™re back to the original rank of \\mathbf{A}. Here we sum out axes that are originally of dimension 1. Thus, (2, 5) \\rightarrow (1, 5). The function returns \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}}. See discussion below.\n\n5\n\nEach float or int c must be explicitly wrapped as Tensor(np.array(c)) to operate with tensors. A practical solution is to apply the to_tensor function to each argument of the previously defined operations. Since NumPy operations handle both scalars and arrays seamlessly, we expect (read: are hopeful) for this to work.\n\n6\n\nThis is a nice hack which will allow us to progressively register new OPs.\n\n7\n\nResolves to calling __radd__(self, &lt;float&gt;) since +(float, Tensor) is undefined.\n\n\n\n\nHere diff_ndim calculates the padded dims on the left of a parent tensor. For example, if \\mathbf{C} = \\mathbf{A} + \\mathbf{B} and \\mathbf{B} has shape (3, 2, 5) while \\mathbf{A} has shape (1, 5), then we have padded shape (1, 1, 5). Next, we find the axes which are broadcasted and sum over those. First, we can calculate3 \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}^\\prime} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{C}} \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{A}^\\prime} with \\mathbf{A}^\\prime as the expanded version of the original tensor. This reduces to \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}^\\prime} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{C}} since \\mathbf{A}^\\prime and \\mathbf{C} has 1-1 dependence between their entries.\nThen, since values of the original matrix is shared, the gradients should be summed. Indeed, we can take the final gradient as \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}^\\prime}  \\frac{\\partial \\mathbf{A}^\\prime}{\\partial \\mathbf{A}}. This involves summing over the broadcasted axes. To see this, let \\mathbf{C} be (3, 2) but \\mathbf{A} is (2,), then we know that \\mathbf{A}^\\prime is (3, 2). Thus,\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}}{\\partial a_{1}}\n&=\n\\frac{\\partial\\mathcal{L}}{\\partial a_{11}} \\frac{\\partial a_{11}}{\\partial a_{1}} +\n\\frac{\\partial\\mathcal{L}}{\\partial a_{21}} \\frac{\\partial a_{21}}{\\partial a_{1}} +\n\\frac{\\partial\\mathcal{L}}{\\partial a_{31}} \\frac{\\partial a_{31}}{\\partial a_{1}} \\\\[1.0em]\n&=\n\\frac{\\partial\\mathcal{L}}{\\partial a_{11}} +\n\\frac{\\partial\\mathcal{L}}{\\partial a_{21}} +\n\\frac{\\partial\\mathcal{L}}{\\partial a_{31}}.\n\\end{aligned}\n\nThat is, we keep the original dims, but sum over the added axis 0. Finally, for axes that were not added but expanded we also have to sum over this but keep the dimension, reducing it to the original 1. The shape of the resulting gradient should then match the original (unbroadcasted) parent tensor.\n\na = np.random.randn(3, 1, 2, 3)     # (3, 1, 2, 3)\nb = np.array([[1.], [2.]])          #       (2, 1)\n\nA = Tensor(a, requires_grad=True)\nB = Tensor(b, requires_grad=True)\nC = A + B\n\nc_grad = np.random.randn(*C.shape)  # Simulate a gradient for testing\nC.backward(c_grad)\n\n# Test shapes\nassert C._backward(A).shape == A.shape\nassert C._backward(B).shape == B.shape\n\nActually the shapes already match since the gradients C._backward(A) and C._backward(B) were already sent over to increment A.grad and B.grad during the .backward() call. Note that scalars are also supported, though a bit less convenient, as arrays of shape (1,). For example, we initialize 1.0 as Tensor(np.array([1.0])). You can confirm that the gradient for sum of scalars are correct, or sum of a scalar and a vector, etc.\nComparing with PyTorch autograd:\n\nimport torch\n\nA_torch = torch.tensor(a, requires_grad=True)\nB_torch = torch.tensor(b, requires_grad=True)\nC_torch = A_torch + B_torch\nC_torch.backward(torch.tensor(c_grad))\n\ndef diff(*args):\n    error = []\n    for (x_torch, x) in args:\n        error.append((x_torch.grad - torch.tensor(x.grad)).abs().max())\n\n    print(f\"max error: {max(error).item():.1e}\")\n    assert max(error) &lt; 1e-8\n\ndiff((A_torch, A), (B_torch, B))\n\nmax error: 2.2e-16\n\n\nHow bout adding floats?\n\nA = Tensor(a, requires_grad=True)\nC = 3.0 + A     # testing __radd__\nC.backward(c_grad)\n\n(A.grad == c_grad).all()\n\nnp.True_\n\n\n\n\nMatmul\nFor MATMUL, we will use the same approach as for addition. Recall that we know the gradient for matrix multiplication \\mathbf{C} = \\mathbf{A} \\mathbf{B} from the previous notebooks. This is implemented in the code below:\n\nclass MatmulTensor(Tensor):\n    def __init__(self, a: Tensor, b: Tensor):\n        super().__init__(\n            a.data @ b.data, \n            requires_grad=a.requires_grad or b.requires_grad, \n            parents=(a, b)\n        )\n    \n    def _backward(self, parent) -&gt; np.array:\n        \"\"\"\n        (m, n) @ (n, p) = (m, p)\n        A.grad += C.grad @ B.T     # (m, p) x (p, n) = (m, n)\n        B.grad += A.T @ C.grad     # (n, m) x (m, p) = (n, p)\n        \"\"\"\n        idx = self.parents.index(parent)\n        parent = self.parents[idx]\n        coparent = self.parents[1 - idx]\n        \n        if idx == 0:\n            grad = self.grad @ coparent.data.T\n        else:\n            grad = coparent.data.T @ self.grad\n\n        return grad\n    \n\nTensor.__matmul__ = lambda self, other: MatmulTensor(self, other)\n\nExample:\n\na = np.random.randn(3, 2)\nb = np.random.randn(2, 4)\nA = Tensor(a, requires_grad=True)\nB = Tensor(b, requires_grad=True)\nC = A @ B\n\nc_grad = np.random.randn(*C.shape)  # Simulate a gradient for testing\nC.backward(c_grad)\n\nTesting:\n\nA_torch = torch.tensor(a, requires_grad=True)\nB_torch = torch.tensor(b, requires_grad=True)\nC_torch = A_torch @ B_torch\nC_torch.backward(torch.tensor(c_grad))\n\n# test\ndiff((A_torch, A), (B_torch, B))\n\nmax error: 0.0e+00\n\n\n\n\nMultiplication\nThis OP refers to elementwise multiplication \\mathbf{C} = \\mathbf{A} \\odot \\mathbf{B} between tensors of the compatible shape (i.e.Â one can be broadcasted onto the other). Hence, we have to support broadcasting in calculating the gradients. Finally, like MATMUL, there is a dependence on the other tensor: \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{C}} \\odot \\mathbf{B} assuming same shape, otherwise itâ€™s broadcasted4 (e.g.Â use \\mathbf{A}^\\prime or \\mathbf{B}^\\prime adjusted to the shape of \\mathbf{C}). Moreover, we are particularly interested in the case where one tensor is a scalar.\n\nclass MulTensor(Tensor):\n    def __init__(self, a: Tensor, b: Tensor):\n        super().__init__(\n            a.data * b.data, \n            requires_grad=a.requires_grad or b.requires_grad, \n            parents=(a, b)\n        )\n    \n    def _backward(self, parent) -&gt; np.array:\n        # shape of the other tensor is \"maximally correct\" by broadcasting\n        out = self.data\n        other_idx = 1 - self.parents.index(parent)\n        grad = self.grad * self.parents[other_idx].data\n\n        # sum out all left-added dims\n        diff_ndim = len(out.shape) - len(parent.data.shape)\n        for _ in range(diff_ndim):\n            grad = grad.sum(axis=0)\n\n        # sum out expanded dims (but not added)\n        for i, dim in enumerate(parent.data.shape):\n            if dim == 1:\n                grad = grad.sum(axis=i, keepdims=True)\n        \n        return grad\n    \n\nTensor.__mul__ = lambda self, other: MulTensor(self, to_tensor(other))\nTensor.__rmul__ = lambda self, other: MulTensor(to_tensor(other), self)\n\nBroadcasting:\n\na = np.random.randn(3, 1, 2, 3)     # (3, 1, 2, 3)\nb = np.array([[1.], [2.]])          #       (2, 1)\n\nA = Tensor(a, requires_grad=True)\nB = Tensor(b, requires_grad=True)\nC = A * B\n\nc_grad = np.random.randn(*C.shape)  # Simulate a gradient for testing\nC.backward(c_grad)\n\nTesting:\n\nA_torch = torch.tensor(a, requires_grad=True)\nB_torch = torch.tensor(b, requires_grad=True)\nC_torch = A_torch * B_torch\nC_torch.backward(torch.tensor(c_grad))\n\ndiff((A_torch, A), (B_torch, B))\n\nmax error: 2.2e-16\n\n\nScalar multiplication:\n\nA = Tensor(a, requires_grad=True)\nC = 3.0 * A     # testing __rmul__\nC.backward(c_grad)\n(A.grad == 3 * c_grad).all()\n\nnp.True_\n\n\n\n\nPower\nThis is a unary operator unlike the above two which are binary.\n\nclass PowTensor(Tensor):\n    def __init__(self, a: Tensor, n: int):\n        super().__init__(\n            a.data ** n, \n            requires_grad=a.requires_grad, \n            parents=tuple([a])\n        )\n        self.n = n\n    \n    def _backward(self, parent) -&gt; float:\n        \"\"\"A.grad += C.grad x n * A ** (n - 1)\"\"\"\n        n = self.n\n        return self.grad * n * parent.data ** (n - 1)\n    \n\nTensor.__pow__ = lambda self, n: PowTensor(self, n)\n\nExample:\n\na = np.random.randn(3, 2)\nn = 3\nA = Tensor(a, requires_grad=True)\nC = A ** n\n\nc_grad = np.random.randn(*C.shape)  # Simulate a gradient for testing\nC.backward(c_grad)\n\nTesting:\n\nA_torch = torch.tensor(a, requires_grad=True)\nC_torch = A_torch ** n\nC_torch.backward(torch.tensor(c_grad))\n\n# test\ndiff((A_torch, A))\n\nmax error: 1.4e-17\n\n\n\n\nSum / Mean\nThis oneâ€™s easy, if c = \\sum a_i then \\frac{\\partial c}{\\partial a_j} = 1. For simplicity, we dont sum over a specific axis.\n\nclass SumTensor(Tensor):\n    def __init__(self, a: Tensor):\n        super().__init__(\n            a.data.sum(), \n            requires_grad=a.requires_grad, \n            parents=tuple([a])\n        )\n    \n    def _backward(self, parent) -&gt; float:\n        \"\"\"A.grad += C.grad * np.ones_like(A)\"\"\"\n        return self.grad * np.ones_like(parent.data)\n    \n\nTensor.sum = lambda self: SumTensor(self)\nTensor.mean = lambda self: SumTensor(self) * (1 / np.size(self.data))\n\nNote .mean() is derived from .sum() and MUL. So we just test the former:\n\na = np.random.randn(3, 2)\nA = Tensor(a, requires_grad=True)\nC = A.mean()\n\nc_grad = np.random.randn(*C.shape)  # Simulate a gradient for testing\nC.backward(c_grad)\n\nA_torch = torch.tensor(a, requires_grad=True)\nC_torch = A_torch.mean()\nC_torch.backward(torch.tensor(c_grad))\n\n# test\ndiff((A_torch, A))\n\nmax error: 1.7e-10\n\n\n\n\nNegative\nThis is also a unary operator. Clearly, \\frac{\\partial c_j}{\\partial a_i} = -\\delta_{ij}. This resembles a diagonal matrix, but the Jacobian shown below is rectangular. Recall, however, that the diagonal structure becomes rectangular after reshaping.\n\nclass NegTensor(Tensor):\n    def __init__(self, a: Tensor):\n        super().__init__(\n            -a.data, \n            requires_grad=a.requires_grad, \n            parents=tuple([a])\n        )\n    \n    def _backward(self, parent) -&gt; float:\n        \"\"\"A.grad += C.grad * -np.ones_like(A)\"\"\"\n        return -self.grad\n    \n\nTensor.__neg__ = lambda self: NegTensor(self)\n\nExample:\n\na = np.random.randn(3, 2)\nn = 3\nA = Tensor(a, requires_grad=True)\nC = -A\n\nc_grad = np.random.randn(*C.shape)  # Simulate a gradient for testing\nC.backward(c_grad)\n\nTesting:\n\nA_torch = torch.tensor(a, requires_grad=True)\nC_torch = -A_torch\nC_torch.backward(torch.tensor(c_grad))\n\n# test\ndiff((A_torch, A))\n\nmax error: 0.0e+00\n\n\n\n\nSubtraction\nThis is the first derived OP. Letâ€™s check if it works! :)\n\nTensor.__sub__ = lambda self, other: self + (-to_tensor(other))\nTensor.__rsub__ = lambda self, other: to_tensor(other) + (-self)\n\n# tests\na = np.random.randn(10, 3)\nb = np.random.randn(10, 1)          # broadcasting\nA = Tensor(a, requires_grad=True)\nB = Tensor(b, requires_grad=True)\nC = A - B\n\nc_grad = np.random.randn(*C.shape)\nC.backward(c_grad)\n\nSeems to work:\n\nA_torch = torch.tensor(a, requires_grad=True)\nB_torch = torch.tensor(b, requires_grad=True)\nC_torch = A_torch - B_torch\nC_torch.backward(torch.tensor(c_grad))\n\n# test\ndiff((A_torch, A), (B_torch, B))\n\nmax error: 4.4e-16\n\n\nScalars also OK:\n\nA = Tensor(a, requires_grad=True)\nC = 3.0 - A\nC.backward(c_grad)\n(A.grad == -c_grad).all()\n\nnp.True_\n\n\n\n\nReLU and Tanh\nHere we implement activations of tensors. Similar to NEG and POW, these are also unary operators, so that the output has the same shape as its input. The vector-Jacobian product should be similarly trivial since activation is performed element-wise. Letâ€™s see the implementation below.\n\nclass ReLUTensor(Tensor):\n    def __init__(self, a: Tensor):\n        super().__init__(\n            a.data * (a.data &gt; 0.0).astype(float), \n            requires_grad=a.requires_grad, \n            parents=tuple([a])\n        )\n    \n    def _backward(self, parent) -&gt; float:\n        return self.grad * (parent.data &gt; 0).astype(float)\n    \n\nclass TanhTensor(Tensor):\n    def __init__(self, a: Tensor):\n        super().__init__(\n            np.tanh(a.data), \n            requires_grad=a.requires_grad, \n            parents=tuple([a])\n        )\n    \n    def _backward(self, parent) -&gt; float:\n        return self.grad * (1 - self.data ** 2)\n    \n\nTensor.relu = lambda self: ReLUTensor(self)\nTensor.tanh = lambda self: TanhTensor(self)\n\nTesting:\n\na = np.random.randn(3, 2)\n\nA = Tensor(a, requires_grad=True)\nC = A.relu()\nD = A.tanh()\n\ng = np.random.randn(*A.shape)\nC.backward(g)\nD.backward(g)\n\n# test\nA_torch = torch.tensor(a, requires_grad=True)\nC_torch = A_torch.relu()\nD_torch = A_torch.tanh()\nC_torch.backward(torch.tensor(g))\nD_torch.backward(torch.tensor(g))\ndiff((A_torch, A))\n\nmax error: 1.1e-16\n\n\n\nC._backward(A) + D._backward(A)\n\narray([[-0.31426844,  0.96938398],\n       [ 0.89469013,  0.70512255],\n       [ 5.34170527,  0.12836244]])\n\n\n\nA_torch.grad\n\ntensor([[-0.3143,  0.9694],\n        [ 0.8947,  0.7051],\n        [ 5.3417,  0.1284]], dtype=torch.float64)\n\n\n\n\nðŸ§ª Integration test\nAbove we did unit tests. By combining operations and checking the resulting gradients, we also confirm that our implementation of backpropagation is also consistent with PyTorch. Hence, most likely correct.\n\nx = Tensor(np.random.randn(3, 12), requires_grad=True)\nb = Tensor(np.random.randn(12,),   requires_grad=True)\nw = Tensor(np.random.randn(4, 3),  requires_grad=True)\n\nz = 0.2 * x + 0.1 - x\ny = (w @ z).relu() + b.tanh()\nz = (-(y * y) ** 3).mean()\n\nz.backward()\n\nSame operations in PyTorch:\n\nxt = torch.tensor(x.data, requires_grad=True)\nbt = torch.tensor(b.data, requires_grad=True)\nwt = torch.tensor(w.data, requires_grad=True)\n\nz = 0.2 * xt + 0.1 - xt\ny = (wt @ z).relu() + bt.tanh()\nz = (-(y * y) ** 3).mean()\n\nz.backward()\ndiff((xt, x), (bt, b), (wt, w))\n\nmax error: 5.7e-14",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "topics/deep/03.html#model-training-from-scratch",
    "href": "topics/deep/03.html#model-training-from-scratch",
    "title": "Automatic Differentiation",
    "section": "Model training (from scratch!)",
    "text": "Model training (from scratch!)\nGenerating toy data for regression:\ny = \\sqrt[4]{t + \\mu_t} \\quad \\text{s. t.} \\quad \\mu_t \\in \\mathcal{N}(0, \\sigma=1)\n\nimport numpy as np\nRANDOM_SEED = 1\nnp.random.seed(RANDOM_SEED)\n\nN = 3000\nX = np.linspace(1, 8, N)\nY = (X + np.random.normal(size=N, scale=0.8)) ** 0.25\nX = X[~np.isnan(Y)]\nY = Y[~np.isnan(Y)]\nassert np.isnan(Y).sum() == 0\n\n\n\nCode\n%config InlineBackend.figure_format = \"svg\"\nimport matplotlib.pyplot as plt\n\nplt.scatter(X, Y, s=2, marker=\"o\", label=\"data\")\nplt.plot(X, X ** 0.25, color=\"black\", linewidth=3, label=\"$y = \\sqrt[4]{t}$\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nplt.grid(linestyle=\"dotted\", alpha=0.6)\nplt.legend(loc=\"lower right\");\n\n\n\n\n\n\n\n\n\nFor fun, we drop X as hidden variable and create a time-series dataset based on lag features for Y.\n\ndef create_lag_dataset(Y, valid_frac=0.15, window=10):\n    \"\"\"\n    Create a supervised dataset where inputs are lagged values of Y.\n    The last `valid_frac` fraction of the series is used for validation.\n    \"\"\"\n    Y_lags = np.lib.stride_tricks.sliding_window_view(Y, window_shape=window + 1)\n    X_all = Y_lags[:, :-1]  # previous 'window' values\n    y_all = Y_lags[:, -1]   # next value\n    \n    split_point = int(len(X_all) * (1 - valid_frac))\n    \n    X_train = X_all[:split_point]\n    y_train = y_all[:split_point]\n    X_valid = X_all[split_point:]\n    y_valid = y_all[split_point:]\n    \n    return X_train, y_train, X_valid, y_valid\n\n\n# only use observed target signal\nX_train, y_train, X_valid, y_valid = create_lag_dataset(Y, valid_frac=0.15, window=10)\nprint(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n\n(2533, 10) (2533,) (447, 10) (447,)\n\n\n\nModules\n\nParams = list[tuple[str, Tensor]]\n\nclass Module:\n    def forward(self, *args, **kwargs):\n        raise NotImplementedError(\".forward() must be implemented by subclasses.\")\n\n    def parameters(self) -&gt; Params:\n        raise NotImplementedError(\".parameters() must be implemented by subclasses.\")\n    \n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\nclass Linear(Module):\n    def __init__(self, input_size: int, output_size: int):\n        std = np.sqrt(2 / (input_size + 0.5 * output_size))\n        w = np.random.normal(size=(input_size, output_size)) * std\n        b = np.zeros((1, output_size))\n        self.W = Tensor(w, requires_grad=True)\n        self.B = Tensor(b, requires_grad=True)\n\n    def forward(self, x):\n        return x @ self.W + self.B\n    \n    def __call__(self, x):\n        return self.forward(x)\n    \n    def parameters(self):\n        return [(\"weight\", self.W), (\"bias\", self.B)]\n\n\n\n\n\n\n\nNote\n\n\n\nInitilization for the weights below is a mix of Xavier initialization (for Tanh) and Kaiming initialization (for ReLU). These are discussed further in a future notebook. Or see this article for an introduction. The following value for the standard deviation \\sigma is sort of an average between Kaiming with gain \\sqrt{2} and Xavier initialization for the normal distribution. See torch.nn.init docs. That is, \n\\sigma = \\sqrt{\\frac{2}{\\alpha \\cdot \\text{fan\\_in} + (1 - \\alpha) \\cdot (\\text{fan\\_in} + \\text{fan\\_out})}}\n\nwhere \\alpha = 0 for Xavier initialization and \\alpha = 1 for Kaiming. Setting \\alpha = 0.5 we get the above formula for std.\n\n\nActivation modules are simply wrappers around the corresponding OP:\n\nclass Tanh(Module):\n    def forward(self, x):\n        return x.tanh()\n    \n    def parameters(self):\n        return []\n\n\nclass ReLU(Module):\n    def forward(self, x):\n        return x.relu()\n    \n    def parameters(self):\n        return []\n\nThe following module stacks together layers and collects all parameters in a single list.\n\nclass Sequential(Module):\n    def __init__(self, *layers):\n        self.layers = layers\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n        \n    def __getitem__(self, i: int):\n        return self.layers[i]\n    \n    def parameters(self):\n        params = []\n        for i, layer in enumerate(self.layers):\n            for n, p in layer.parameters():\n                params.append((f\"{i}.{layer.__class__.__name__}.\" + n, p))\n        return params\n\n\nmodel_fn = lambda Act, h: Sequential(\n    Linear(input_size=10, output_size=h), Act(),\n    Linear(input_size=h,  output_size=h), Act(),\n    Linear(input_size=h,  output_size=1)\n)\n\nSample input:\n\nx = Tensor(X_train[:4])\nmodel = model_fn(ReLU, h=32)\nmodel(x)\n\nTensor(data=[[-0.41785972]\n [-0.08244604]\n [-0.26055724]\n [-0.40023012]], requires_grad=True)\n\n\nIndexing the layers:\n\nprint(model[0])\nprint(model[1])\n[print(f\"{n:&lt;18} {p.shape}\") for n, p in model.parameters()];\n\n&lt;__main__.Linear object at 0x12c963e00&gt;\n&lt;__main__.ReLU object at 0x12c963b60&gt;\n0.Linear.weight    (10, 32)\n0.Linear.bias      (1, 32)\n2.Linear.weight    (32, 32)\n2.Linear.bias      (1, 32)\n4.Linear.weight    (32, 1)\n4.Linear.bias      (1, 1)\n\n\n\n\nTraining loop\n\nclass DataLoader:\n    def __init__(self, X, Y, batch_size, seed=0):\n        \"\"\"Get last value of each sequence as label.\"\"\"\n        self.batch_size = batch_size\n        self.dataset = list(zip(X, Y))\n        self.rng = random.Random(seed)  # not affect global seed\n    \n    def next(self):\n        \"\"\"Sample a random batch from the dataset.\"\"\"\n        batch = self.rng.sample(self.dataset, self.batch_size)\n        x, y = zip(*batch)\n        x, y = np.stack(x), np.stack(y)\n        x = Tensor(x, requires_grad=False)\n        y = Tensor(y.reshape(-1, 1), requires_grad=False)\n        return x, y\n\n    def __len__(self):\n        return len(self.dataset)\n\nSample input-output pair:\n\nbatch = DataLoader(X_train, y_train, batch_size=32).next()\nx, y = batch\nx.shape, y.shape\n\n((32, 10), (32, 1))\n\n\nOur loss is given by MSE with L2 regularization:\n\\mathcal{L} = \\frac{1}{B} \\sum_b (\\hat{y}_b - y_b)^2 + \\frac{\\alpha}{2} \\sum_k ({\\Theta^k})^2.\nThe operations here all are defined in our autograd engine. So we can implement this loss cleanly:\n\ndef mse_loss(out: Tensorable, y: Tensorable):\n    return ((out - y) ** 2).mean()\n\ndef weight_reg(model, alpha=0.1):\n    s = 0.0\n    for _, w in model.parameters():\n        s += (w ** 2).sum()\n    return 0.5 * alpha * s\n\ndef sgd_step(model, lr=1.0):\n    for _, p in model.parameters():\n        p.data -= lr * p.grad\n        p.grad = None           # reset grads\n\n# sanity check (mse loss)\nw = model[0].W\nassert w.grad is None\nloss = mse_loss(model(x), y)\nloss.backward()\nassert w.grad is not None\n\nw0 = np.linalg.norm(w.data)\nsgd_step(model, lr=0.01)\nprint(f\"Î”â€–wâ€–: {(np.linalg.norm(w.data) / w0 - 1) * 100:.2f}%\")\n\nÎ”â€–wâ€–: 0.01%\n\n\nNotice weight norm decreases:\n\n# sanity check (L2 regularization)\nw = model[0].W\nassert w.grad is None\nloss = weight_reg(model, alpha=0.1)\nloss.backward()\nassert w.grad is not None\n\n# expected decay rate: -lr * alpha\nw0 = np.linalg.norm(w.data)\nsgd_step(model, lr=1.0)\nprint(f\"Î”â€–wâ€–: {(np.linalg.norm(w.data) / w0 - 1) * 100:.2f}%\")\n\nÎ”â€–wâ€–: -10.00%\n\n\nModel training. Weâ€™re ready to train our model!\n\nfrom tqdm.notebook import tqdm\nnp.random.seed(11)\n\nmodel = model_fn(ReLU, h=32)\nnum_steps = 5000\nlr = 3e-5\nhist = []\ntrain_loader = DataLoader(X_train, y_train, batch_size=32)\nfor _ in tqdm(range(num_steps)):\n    x, y = train_loader.next()\n    mse = mse_loss(model(x), y)\n    reg = weight_reg(model, alpha=1e-6)\n    \n    loss = mse + reg\n    loss.backward()\n    sgd_step(model, lr=lr)     # no need to zero grad\n\n    hist.append(mse.data)\n\n\n\n\n\n\nCode\nplt.plot(hist, label=r\"train\")\nplt.ylabel(\"MSE\")\nplt.xlabel(\"steps\")\nplt.grid(alpha=0.6, linestyle=\"dotted\")\nplt.legend();\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef moving_average(arr, window):\n    \"\"\" \n    ex: arr = np.array(1, 4) & window = 3\n    s &lt;- array([[nan, nan,  1.],\n                [nan,  1.,  2.],\n                [ 1.,  2.,  3.]])\n    \"\"\"\n    s = np.lib.stride_tricks.sliding_window_view(\n        np.concatenate([[np.nan] * (window - 1), arr]), \n        window\n    )\n    return np.nanmean(s, 1)\n\n\ny_train_pred = model(Tensor(X_train)).data.reshape(-1)\ny_valid_pred = model(Tensor(X_valid)).data.reshape(-1)\nt_train = np.arange(len(y_train))\nt_valid = np.arange(len(y_train), len(y_train) + len(y_valid))\n\nprint(f\"mse (train): {mse_loss(y_train, y_train_pred): .4f}\")\nprint(f\"mse (valid): {mse_loss(y_valid, y_valid_pred): .4f}\")\n\nw = 50\nplt.figure(figsize=(8, 5))\nx = X[10:]\ny = moving_average(np.concat([y_train, y_valid]), w)\nz = moving_average(np.concat([y_train_pred, y_valid_pred]), w)\n\nplt.plot(np.arange(len(x)), x ** 0.25, linewidth=2.0, color=\"red\", alpha=1.0, label=\"signal\")\nplt.plot(\n    np.arange(len(y_train_pred) + len(y_valid_pred)), \n    z, \n    color=\"k\",\n    linewidth=2,\n    label=r\"model ($\\text{MA}_{50}$)\"\n)\n\nsplit_point = len(t_train)\nplt.scatter(t_train, y_train, alpha=0.6, marker=\"o\", s=6, facecolors=\"none\", edgecolors=\"C0\", label=\"data\")\nplt.scatter(t_valid, y_valid, alpha=0.6, marker=\"o\", s=6, facecolors=\"none\", edgecolors=\"C0\")\n\nplt.axvline(len(t_train), linestyle=\"dashed\", color=\"k\")\nplt.grid(linestyle=\"dotted\", alpha=0.6)\nplt.xlabel(r\"$t$\")\nplt.ylabel(r\"$y$\")\nplt.legend(loc=\"lower right\");\n\n\nmse (train):  0.0176\nmse (valid):  0.0030",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "topics/deep/03.html#bonus-autoencoder",
    "href": "topics/deep/03.html#bonus-autoencoder",
    "title": "Automatic Differentiation",
    "section": "Bonus: Autoencoder",
    "text": "Bonus: Autoencoder\nThe objective is to minimize the regularized reconstruction loss:\n\\mathcal{L} = \\frac{1}{Bd} \\sum_b \\sum_j (\\hat{\\mathbf{x}}_{bj} - \\mathbf{x}_{bj})^2 + \\frac{\\alpha}{2} \\sum_k ({\\Theta^k})^2\nwhich forces \\hat{\\mathbf{x}} \\approx \\mathbf{x} where\n\\hat{\\mathbf{x}} = \\text{Decoder}_{\\phi}(\\text{Encoder}_{\\theta}(\\mathbf{x}))\nis the reconstruction of \\mathbf{x}, and d is the input dimensionality (d = 784 for MNIST). The encoder-decoder architecture with a bottleneck forces the model to learn a low-dimensional representation of the input. This is because the decoder has to be able to reconstruct the input from a lower dimension which we interpret as the latent dimensionality of the dataset.\n\n\n\nsource\n\n\nModel. Modeling a latent dimension of 16 \\ll 784.\n\nclass Autoencoder(Module):\n    def __init__(self):\n        self.encoder = Sequential(\n            Linear(784, 128), ReLU(),\n            Linear(128,  64), ReLU(),\n            Linear( 64,  16)\n        )\n\n        self.decoder = Sequential(\n            Linear( 16,  64), ReLU(),\n            Linear( 64, 128), ReLU(),\n            Linear(128, 784),\n            Tanh()   # pixel intensity [-1, 1]\n        )\n\n    def forward(self, x):\n        z = self.encoder(x)\n        o = self.decoder(z)\n        return o\n    \n    def __call__(self, x):\n        return self.forward(x)\n    \n    def parameters(self):\n        encoder_params = [(\"encoder.\" + n, p) for n, p in self.encoder.parameters()]\n        decoder_params = [(\"decoder.\" + n, p) for n, p in self.decoder.parameters()]\n        return encoder_params + decoder_params\n\nAutoencoder parameters:\n\nautoenc = Autoencoder()\n[print(f\"{n:&lt;25} {p.shape}\") for n, p in autoenc.parameters()];\n\nencoder.0.Linear.weight   (784, 128)\nencoder.0.Linear.bias     (1, 128)\nencoder.2.Linear.weight   (128, 64)\nencoder.2.Linear.bias     (1, 64)\nencoder.4.Linear.weight   (64, 16)\nencoder.4.Linear.bias     (1, 16)\ndecoder.0.Linear.weight   (16, 64)\ndecoder.0.Linear.bias     (1, 64)\ndecoder.2.Linear.weight   (64, 128)\ndecoder.2.Linear.bias     (1, 128)\ndecoder.4.Linear.weight   (128, 784)\ndecoder.4.Linear.bias     (1, 784)\n\n\n\nTraining\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\ntorch.manual_seed(0)    # data loaders\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))    # to [-1, 1] since out node is Tanh!\n])\nmnist_train = datasets.MNIST(root=\"./data\", download=True, transform=transform, train=True)\nmnist_valid = datasets.MNIST(root=\"./data\", download=True, transform=transform, train=False)\n\ntrain_loader = DataLoader(mnist_train, batch_size=16, shuffle=True)\nvalid_loader = DataLoader(mnist_valid, batch_size=10000)\n\nX_valid, y_valid = next(iter(valid_loader))\nX_valid = Tensor(X_valid.reshape(-1, 784).numpy())\ny_valid = Tensor(y_valid.numpy())\n\n\n\n\n\n\n\nImportant\n\n\n\nPixel intensity is normalized to [-1, 1] since our autoencoder has Tanh as output activation.\n\n\n\nfrom tqdm.notebook import tqdm\nnp.random.seed(1)\n\nlr = 0.03\nalpha = 1e-8\nhist = []\nnum_epochs = 15\nautoenc = Autoencoder()\n\nfor e in tqdm(range(num_epochs)):\n    for x, _ in train_loader:\n        x = Tensor(x.numpy().reshape(-1, 784))\n        mse = mse_loss(autoenc(x), x)\n        reg = weight_reg(autoenc, alpha=alpha)\n        loss = mse + reg\n        loss.backward()\n        sgd_step(autoenc, lr=lr)\n        \n        hist.append(mse.data)\n\n    train_loss = sum(hist[-50:]) / 50   # last 50 steps\n    valid_loss = mse_loss(autoenc(X_valid), X_valid).data\n    print(f\"epoch [{e+1:&gt;02d}/{num_epochs}]:    loss: {train_loss:&gt;6.5f}    val_loss: {valid_loss:.5f}\")\n\n\n\n\nepoch [01/15]:    loss: 0.22326    val_loss: 0.22318\nepoch [02/15]:    loss: 0.17572    val_loss: 0.17088\nepoch [03/15]:    loss: 0.15336    val_loss: 0.15081\nepoch [04/15]:    loss: 0.13900    val_loss: 0.13758\nepoch [05/15]:    loss: 0.13106    val_loss: 0.12789\nepoch [06/15]:    loss: 0.12357    val_loss: 0.12028\nepoch [07/15]:    loss: 0.11640    val_loss: 0.11389\nepoch [08/15]:    loss: 0.11297    val_loss: 0.10961\nepoch [09/15]:    loss: 0.10659    val_loss: 0.10665\nepoch [10/15]:    loss: 0.10614    val_loss: 0.10313\nepoch [11/15]:    loss: 0.10224    val_loss: 0.10029\nepoch [12/15]:    loss: 0.10254    val_loss: 0.09778\nepoch [13/15]:    loss: 0.09752    val_loss: 0.09575\nepoch [14/15]:    loss: 0.09604    val_loss: 0.09374\nepoch [15/15]:    loss: 0.09670    val_loss: 0.09237\n\n\n\n\nCode\nplt.plot(hist, label=\"train step\", alpha=1.0)\nplt.ylabel(\"MSE\")\nplt.grid(linestyle=\"dotted\", alpha=0.6)\nplt.legend();\n\n\n\n\n\n\n\n\n\n\n\nCode\nidx = [list(y_valid.data).index(i) for i in range(10)]\n\nfig, ax = plt.subplots(2, 10, figsize=(12, 3))\nfor i in range(10):\n    j = idx[i]\n    x = X_valid.data[j]\n    ax[0, i].imshow(autoenc(Tensor(x)).data.reshape(28, 28), cmap=\"gray\")\n    ax[1, i].imshow(x.reshape(28, 28), cmap=\"gray\")\n    ax[0, i].axis(\"off\")\n    ax[1, i].axis(\"off\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nClustering\nTo get the bottleneck vectors, we use only the encoder part of the network:\n\nlatent_vectors = autoenc.encoder(X_valid).data\nlabels = y_valid.data\n\nt-SNE clustering to see whether the low-dimensional vectors make sense:\n\n\nCode\nfrom sklearn.manifold import TSNE\n\nprint(\"vectors:\", latent_vectors.shape)\ntsne = TSNE(n_components=2, perplexity=30, random_state=0, init=\"random\", learning_rate=\"auto\")\nlatents_tsne = tsne.fit_transform(latent_vectors)\n\n# Plot using true labels\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(latents_tsne[:, 0], latents_tsne[:, 1], c=labels, cmap=\"tab10\", s=20, edgecolors=\"k\", lw=0.75)\nplt.colorbar(scatter, ticks=range(10))\nplt.tight_layout()\nplt.axis(\"off\");\n\n\nvectors: (10000, 16)",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "topics/deep/03.html#footnotes",
    "href": "topics/deep/03.html#footnotes",
    "title": "Automatic Differentiation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf the custom layer can be expressed in terms of existing ops and layers, then this is not strictly necessary, although it may be desirable for efficiency reasons.â†©ï¸Ž\nOne can think of this as realization of the abstract operator \\pi above, except reshaped / linearized.â†©ï¸Ž\nHere tensor multiplication \\pi(\\mathbf{u}, \\mathbf{v}) is abbreviated as \\mathbf{u} \\mathbf{v}. These are not direct matrix products. But they can be formulated as a â€œvector-Jacobian productâ€ (VJP) after some reshaping, i.e.Â flattening \\mathbf{C} and \\mathbf{A}^\\prime where the Jacobian J is a dependency matrix with entries J_{rs} = \\partial \\bar{\\mathbf{C}}_r / \\partial \\bar{\\mathbf{A}}^\\prime_s. VJP actually seems to be the best way to formulate all of these, now that I think about it.â†©ï¸Ž\nThis is what is meant by â€œmaximally correctâ€ in the code below. The two parent tensors agree at the shape of the product \\mathbf{C} which is the maximal shape. The resulting gradient is then reduced to the respective shapes of each parent tensor by summing over the broadcasted axes.â†©ï¸Ž",
    "crumbs": [
      "Deep Learning",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "topics/deep/01.html",
    "href": "topics/deep/01.html",
    "title": "Softmax Regression",
    "section": "",
    "text": "Suppose you want to write a program that will classify handwritten drawing of digits into their appropriate category: 0, 1, 2, â€¦, 9. You could, think hard about the nature of digits, try to determine the logic of what indicates what kind of digit, and write a program to codify this logic. Or you could take advantage of the statistics of the data, e.g.Â pixel intensity in a 28 x 28 grid, as discriminative features of each instance. For this task we will use the MNIST dataset:",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#ml-as-data-driven-programming",
    "href": "topics/deep/01.html#ml-as-data-driven-programming",
    "title": "Softmax Regression",
    "section": "ML as data-driven programming",
    "text": "ML as data-driven programming\nMachine learning approach. Collect a training set of images with known labels and feed these into a machine learning algorithm, which, if done well, will automatically produce a â€œprogramâ€ that solves this task. The said program will consist of a large number of magic numbers, but it nonetheless performs a sequence of computations to determine the output class:\n\n^The latter input is different from training data. In practice, we want the models to perform well on unlabeled data.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#three-elements-of-a-learning-algorithm",
    "href": "topics/deep/01.html#three-elements-of-a-learning-algorithm",
    "title": "Softmax Regression",
    "section": "Three elements of a learning algorithm",
    "text": "Three elements of a learning algorithm\nEvery machine learning algorithm has the ff.Â elements:\n\nElements of a learning algorithm\n\n\n\n\n\n\n\nElement\nObject\nDescription\n\n\n\n\nHypothesis class\n\\mathcal{H}\nThis defines the â€œprogram structureâ€. In deep learning, the hypothesis class is parameterized via a set of parameters \\Theta. The parameters describe how we map inputs (images of digits) to outputs (labels, or probabilities for each class). Formally, \\mathcal{H} = \\{h_\\Theta \\mid \\Theta \\in \\mathbb{R}^d \\} where h_\\Theta(\\mathbf{x}) = \\hat{y} or \\hat{\\mathbf{p}} for an input \\mathbf{x}.\n\n\nLoss function\n\\mathcal{L}, \\ell\nA function that specifies how â€œwellâ€ a given hypothesis h_\\Theta (i.e.Â a choice of parameters) performs on the task of interest. The loss is expressed in terms of a pointwise loss function \\ell such that: \\mathcal{L}_{\\mathcal{D}}(\\Theta) = \\frac{1}{N}\\sum_{i=1}^N \\ell(h_\\Theta(\\mathbf{x}_i), y_i) where \\ell \\geq 0 and \\ell \\to 0 whenever the predictions are accurate, and \\ell \\to \\infty as the predictions become increasingly worse.\n\n\nOptimizer\ne.g.Â  torch.optim.*,  LRScheduler\nProcedure for determining a set of parameters that (approximately) minimizes the training loss. More precisely, the optimizer handles state and the calculation required to find optimal parameters \\Theta^* \\approx {\\text{argmin}}_{\\Theta} \\; \\mathcal{L}_\\mathcal{D}(\\Theta). For deep learning, this is typically an iterative approach via SGD and its variants.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#multi-class-classification",
    "href": "topics/deep/01.html#multi-class-classification",
    "title": "Softmax Regression",
    "section": "Multi-class classification",
    "text": "Multi-class classification\nFor multi-class classification, the training dataset \\mathcal{D} consists of input-output pairs \\mathcal{D} = (\\mathbf{x}_i, y_i)_{i=1}^N where \\mathbf{x}_i \\in \\mathbb{R}^d with d is the input dimensionality and y_i \\in [1, K] \\subset \\mathbb{Z} where K is the number of classes. Here N = | \\mathcal{D} | is the size of the dataset. In this setting, a hypothesis function h maps input vectors \\mathbf{x} to K-dimensional vectors: h \\colon \\; \\mathbb{R}^d \\to \\mathbb{R}^K. The output h_j(\\mathbf{x}) indicates some measure of â€œbeliefâ€ in how much likely the label is to be class j. That is, the most likely class for an input \\mathbf{x} is predicted as the coordinate \\hat{j} = \\text{argmax}_j \\; h_j(\\mathbf{x}).\nExample. For MNIST, d = 28 \\times 28 = 784, K = 10 and M = 60,000.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#linear-hypothesis-class",
    "href": "topics/deep/01.html#linear-hypothesis-class",
    "title": "Softmax Regression",
    "section": "Linear hypothesis class",
    "text": "Linear hypothesis class\nA linear hypothesis function has matrix multiplication as core operation: h_\\Theta(\\mathbf{x}) = \\Theta^\\top \\mathbf{x} for parameters \\Theta \\in \\mathbb{R}^{d \\times K}. In practice, we usually write this using matrix-batch notation since we process inputs in parallel as a matrix:\n\nh_\\Theta(\\mathbf{X}) =  \\underbrace{\\mathbf{X}}_{\\mathbb{R}^{M \\times d}} \\; \\underbrace{\\Theta}_{\\mathbb{R}^{d \\times K}} \\in \\mathbb{R}^{M \\times K}\n\nwhere the inputs are laid out as row vectors inside the matrix:\n\n\\begin{equation}\n\\mathbf{X}=\\left[\\begin{array}{c}\n-\\, \\mathbf{x}^{(1)\\top}- \\\\\n\\vdots \\\\\n-\\, \\mathbf{x}^{(M)\\top}-\n\\end{array}\\right] \\in \\mathbb{R}^{M \\times d}\n\\end{equation}.\n\nRemark. Geometrically each \\Theta_j = \\Theta_{[:, j]} \\in \\mathbb{R}^d defines a separating hyperplane for class j \\in [K]. So a linear hypothesis class is able to learn to separate linearly separable data points in \\mathbb{R}^d using K separating hyperplanes by assigning a score s_j = \\Theta_j^\\top \\mathbf{x} \\in \\mathbb{R} proportional to its weighted distance from the hyperplane, and the â€œweightâ€ of that hyperplane \\lVert\\Theta_j\\rVert.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#softmax-and-cross-entropy",
    "href": "topics/deep/01.html#softmax-and-cross-entropy",
    "title": "Softmax Regression",
    "section": "Softmax and cross-entropy",
    "text": "Softmax and cross-entropy\nThe simplest loss is just the zero-one loss: \\ell = 0 if \\operatorname{argmax}_i h_i(\\mathbf{x}) = y, otherwise \\ell = 1. This is just the classification error. Unfortunately, the error is unsuitable for optimization, simply because it is not differentiable. That is, we can smoothly adjust the parameters without seeing a change in \\ell, or it transitions abruptly from 0 to 1.\nInstead, we look at the probabilities assigned by the model to each class. To do this, we have to convert the class scores to probabilities exponentiating and normalizing its entries (i.e.Â making \\sum_j p = 1 s.t. p_j \\geq 0). Class scores h_j(\\mathbf{x}) = \\Theta_j^\\top \\mathbf{x} are exponentiated before normalizing:\n\np_j = \\frac{\\exp(h_j(\\mathbf{x}))}{\\sum_l \\exp(h_l(\\mathbf{x}))} \\eqqcolon \\text{Softmax} (h(\\mathbf{x}))_j.\n\nWe can let \\hat{\\mathbf{y}} be equal to \\mathbf{p} = \\text{Softmax} (h(\\mathbf{x})) since the softmax approximates the one-hot vector \\mathbf{y} of the target label y (see remark below). Then, what is left is to define a loss function that captures the difference between the model probability vector and the true one. For this, we use the cross-entropy loss given by the negative log of the probability of the true class y:\n\n\\boxed{\n    \\begin{aligned}\n    \\ell_{\\text{CE}}(h(\\mathbf{x}), y)\n    &= -\\log \\hat{\\mathbf{y}} \\odot \\mathbf{y}\\\\[0.8em]\n    &= -\\log \\text{Softmax} (h(\\mathbf{x}))_y \\\\\n    &= -h_y(\\mathbf{x})+\\log \\sum_{j=1}^K \\exp \\left(h_j(\\mathbf{x})\\right).\n    \\end{aligned}\n}\n\nHere p_y = 1 implies \\ell = -\\log 1 = 0 while p_y = 0 implies -\\log 0 = +\\infty. Hence, the model is penalized the more it becomes unconfident in the true class. Note taking the negative log of the true class suffices to penalize high score in the other classes since we have the constraint \\sum p_j = 1. Also, looking at the gradient, the logarithm has the nice property that it does not saturate as it approaches perfect prediction, and it explodes with really bad predictions:\n\\frac{\\partial\\ell}{\\partial p_y} = -\\frac{1}{p_y}.\n\n\nCode\n%config InlineBackend.figure_formats = ['svg'] \nimport matplotlib.pyplot as plt\nimport numpy as np\n\neps = 1e-5\np = np.linspace(0 + eps, 1, 10000)\nplt.figure(figsize=(5, 4))\nplt.plot(p, -np.log(p), linewidth=2, label=\"-log(p)\")\nplt.grid(alpha=0.6, linestyle=\"dashed\")\nplt.xlabel(\"p\"); plt.ylabel(\"loss\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nTo further visualize the cross-entropy loss, letâ€™s incorporate this graph in the evaluation process. The baseline loss is given by -\\log 0.1 \\approx 2.30 assuming uniform prediction across 10 classes. Since the loss decrease more going from p= 0.1 to 0.2 compared to p = 0.5 to 0.6 for the true class, then the optimizer will focus on examples with bad predictions during training. This is desirable behavior.\n\n\n\n\n\n\n\n\n\nRemark. The softmax can be made to be numerically stable. To see this, notice that\n\np_j = \\frac{\\exp(\\Delta h_j(\\mathbf{x}))}{\\sum_l \\exp(\\Delta h_l(\\mathbf{x}))}\n\nwhere \\Delta h_l(\\mathbf{x}) = h_l(\\mathbf{x}) - \\max_{m} h_m(\\mathbf{x}). Thus log-softmax becomes \\log (1 + \\sum a_j) where 0 &lt; a_j \\leq 1, preventing both underflow and overflow. Moreover, it shows that the individual scores scale exponentially with the difference from the largest score. Hence, this transformation is sometimes called soft-argmax since it tends to pick out the largest entry.\n\n\nCode\nimport torch\nz = torch.tensor([1, 2, 3]).float()\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].bar(range(3), z.numpy(), label=\"z\")\nax[1].bar(range(3), z.exp().numpy(), color=\"C1\", label=\"exp(z)\")\nax[2].bar(range(3), z.exp().numpy() / z.exp().numpy().sum(), color=\"C2\", label=\"Softmax(z)\")\nax[0].legend(); ax[1].legend(); ax[2].legend();\nfig.tight_layout()",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#optimization-problem",
    "href": "topics/deep/01.html#optimization-problem",
    "title": "Softmax Regression",
    "section": "Optimization problem",
    "text": "Optimization problem\nThe third ingredient of a learning algorithm is a method for solving the associated optimization problem, i.e.Â the problem of minimizing the average loss on the training set:\n\n\\hat{\\Theta} = \\underset{\\Theta}{\\operatorname{min}} \\frac{1}{N} \\sum_{i=1}^N \\ell_{\\text{CE}} (h_\\Theta(\\mathbf{x}_i), y_i)\n\nHow do we find an optimal set of parameters \\hat{\\Theta}? It turns out that an iterative approach is the most practical.\n\nGradient Descent\nFor a matrix-input, scalar-output function f\\colon \\mathbb{R}^{d \\times k} \\to \\mathbb{R} the gradient \\nabla_\\Theta f(\\Theta) is defined as the matrix of partial derivatives:\n\n\\nabla_\\Theta f(\\Theta) =\\left[\\begin{array}{ccc}\n\\frac{\\partial f(\\Theta)}{\\partial \\Theta_{11}} & \\cdots & \\frac{\\partial f(\\Theta)}{\\partial \\Theta_{1 k}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f(\\Theta)}{\\partial \\Theta_{d 1}} & \\cdots & \\frac{\\partial f(\\Theta)}{\\partial \\Theta_{d k}}\n\\end{array}\\right] \\in \\mathbb{R}^{d \\times k}\n\nNOTE: \\nabla_\\Theta f always has the same shape as \\Theta when f is a scalar.\nFrom the multivariate Taylor expansion,\n\nf(\\Theta + \\Delta \\Theta) \\approx f(\\Theta) + \\nabla_\\Theta f(\\Theta) \\cdot \\Delta\\Theta + \\mathcal{O}(\\Delta\\Theta^2).\n\nSo that the gradient locally points in the direction that most increases f, i.e.Â to first order. Hence, to minimize f, we iteratively update the weight by -\\nabla_\\Theta f at each point in the surface defined by f:\n\n\\Theta_{t + 1} = \\Theta_t - \\alpha \\cdot \\nabla_\\Theta f (\\Theta_t)\n\nwhere \\alpha &gt; 0 is called the learning rate. GD is naturally sensitive to the scale of the learning rate and the initial point \\Theta_0.\n\n\n\nStochastic Gradient Descent (SGD)\nIn practice, computing the gradient over the entire dataset for a single parameter update is often prohibitively expensive, especially when N is large (typical in deep learning). Instead, we perform many gradient steps, each using a randomly sampled subset \\mathcal{B} \\subset \\mathcal{D} called a mini-batch, where the batch size B = |\\mathcal{B}| \\ll N. At each training step:\n\nRandomly sample \\mathcal{B} \\subset \\mathcal{D} so that we get \\mathbf{X}_\\mathcal{B} \\in \\mathbb{R}^{B \\times d} and \\mathbf{Y}_{\\mathcal{B}} \\in [K]^B.\nUpdate parameters: \n\\begin{aligned}\\Theta_{t + 1}\n= \\Theta_t -\n\\frac{\\alpha}{B} \\,\n\\sum_{b \\in I_\\mathcal{B}}\n\\nabla_\\Theta \\ell\n(h_{\\Theta_t}(\\mathbf{x}_b), y_b).\n\\end{aligned}\n\n\nIt follows that the sample dataset varies at each training step. Unlike the previous case where \\mathcal{D} is fixed. This mechanism of SGD reduces overfitting by implicit regularization of the gradient, i.e.Â adding noise in the training process.\n\n\n\n\n\n\n\n\n\n\n\nGradient of cross-entropy\nHow do we actually compute \\mathcal{L}_{\\text{CE}}? This can be done using the chain rule and tracking functional dependencies. Recall:\n\n\\ell_{\\text{CE}}(h_\\Theta(\\mathbf{x}), y) = -h_\\Theta(\\mathbf{x})_y + \\log \\sum_{j=1}^K \\exp \\left(h_\\Theta(\\mathbf{x})_j\\right).\n\nLetâ€™s start by deriving the gradient of the softmax loss itself. For a vector \\mathbf{h} \\in \\mathbb{R}^K:\n\n\\frac{\\partial \\ell_{\\text{CE}}}{\\partial h_j} = - \\delta_{yj} + \\frac{\\exp h_j}{\\sum_{l=1}^K \\exp h_l} = - \\delta_{yj} + p_j.\n\nIn vector form, \\nabla_{\\mathbf{h}} \\ell_{\\text{CE}} = \\mathbf{p} - \\mathbf{y} where \\mathbf{y} is a one-hot vector with 1 on index y. Next, to calculate the derivative with respect to \\Theta, we use the chain rule:\n\n\\frac{\\partial \\ell_{\\text{CE}}}{\\partial \\Theta_{ul}} =  \\frac{\\partial \\ell_{\\text{CE}}}{\\partial h_j} \\frac{{\\partial h_j}}{\\partial \\Theta_{ul}} = \\underbrace{(p_l - \\delta_{yl})}_{K-\\text{dim}} \\; \\underbrace{\\vphantom{(}x_u}_{d-\\text{dim}}.\n\nFor the dimensions to make sense, \\frac{\\partial \\ell_{\\text{CE}}}{\\partial \\Theta} = \\mathbf{x}(\\hat{\\mathbf{y}} - \\mathbf{y})^\\top in matrix form. Recall that our vectors are column vectors and \\hat{\\mathbf{y}} = \\mathbf{p}. Here the product reverses since we traverse the dependence backwards from the loss.\nBatch form. The same process works for a batch of inputs, except that we have an additional batch index which we sum over since \\ell depends on all input instances. The contribution of each input is matched and aggregated using matrix multiplication:\n\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial \\Theta} = \\frac{1}{B}\\,\\underbrace{\\vphantom{(}\\mathbf{X}^\\top}_{d \\times M} \\;\\; \\underbrace{(\\hat{\\mathbf{Y}} - \\mathbf{Y})}_{M \\times K}.\n\nHere the transposes switched since \\mathbf{X} is constructed such that it has rows of \\mathbf{x}^\\top, hence we internally get a double transpose. Putting it all together, we can write the SGD update rule for softmax regression as follows:\n\n\\Theta_{t + 1} = \\Theta_t - \\frac{\\alpha}{B} \\; \\mathbf{X}^\\top (\\hat{\\mathbf{Y}} - \\mathbf{Y}).\n\nHere we have \\frac{1}{B} since \\mathcal{L} = \\frac{1}{B}\\sum_b { \\ell}_b. Also it makes sense to scale down since the sum grows with batch size B. Finally, notice that the step size becomes smaller as \\| \\hat{\\mathbf{Y}} - \\mathbf{Y} \\| \\to 0. To recap, we have the following equations:\n\n\\boxed{\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial \\mathbf{H}} &= \\frac{1}{B}  (\\hat{\\mathbf{Y}} - \\mathbf{Y}) \\\\[0.75em]\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial \\Theta} &= \\frac{1}{B} \\mathbf{X}^\\top  (\\hat{\\mathbf{Y}} - \\mathbf{Y}) \\\\[0.60em]\n\\Theta_{t + 1} &= \\Theta_t - \\frac{\\alpha}{B} \\, \\mathbf{X}^\\top (\\hat{\\mathbf{Y}} - \\mathbf{Y})\n\\end{aligned}\n}",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#code-implementation",
    "href": "topics/deep/01.html#code-implementation",
    "title": "Softmax Regression",
    "section": "Code implementation",
    "text": "Code implementation\nSince shuffling a large dataset at each training step is expensive, in practice, we shuffle it once and then iteratively draw B-sized slices (batches) at each stepâ€”effectively sampling without replacement. A full pass through \\mathcal{D} under this process is called an epoch. After each epoch, we reshuffle the dataset for the next one. Consequently, the total number of SGD steps scales as \\mathcal{O}(Ne / B), where e is the number of epochs.\nIn PyTorch, this procedure is handled by the DataLoader object:\n\nfrom torch.utils.data import DataLoader\n\nNUM_EPOCHS = 2\nx = torch.arange(10)\ntrain_loader = DataLoader(x, batch_size=3, shuffle=True, drop_last=True)\n\nfor e in range(NUM_EPOCHS):\n    for batch in train_loader:\n        print(batch.numpy())\n    print()\n\n[9 2 7]\n[6 1 0]\n[5 4 3]\n\n[0 3 2]\n[4 1 9]\n[7 6 5]\n\n\n\nWe will train a classification model based on the SGD update rule above:\n\nimport torch\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n])\n\ntrain_dataset = datasets.MNIST('./data', train=True,  transform=transform)\nvalid_dataset = datasets.MNIST('./data', train=False, transform=transform)\n\nDefining the linear model:\n\nimport torch.nn as nn\nmodel = nn.Linear(784, 10, bias=False)\n\nRemark. The linear model can be extended to have a bias vector \\beta, so that h_\\Theta = \\mathbf{X}\\Theta + \\beta where \\beta \\in \\mathbb{R}^K. But it turns out there is a â€œbias trickâ€ in deep learning where an additional dimension containing only +1 is added so that the input becomes of shape (N, d + 1). For simplicity, we stick with no bias.\n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nloss_train = []\nNUM_EPOCHS = 5\nB = 16\nALPHA = 0.01\n\n@torch.no_grad()\ndef train_step(x, y):\n    x = x.reshape(-1, 784)\n    h = model(x)\n\n    for theta in model.parameters():\n        p = F.softmax(h, dim=1)\n        e = F.one_hot(y, num_classes=10)\n        g = x.T @ (p - e) / B\n\n        theta -= ALPHA * g.T\n\n    loss = -p[torch.arange(B), y].log().mean()\n    return loss.item()\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)  # SGD\n\nfor _ in tqdm(range(NUM_EPOCHS)):\n    for x, y in train_loader:\n        loss = train_step(x, y)\n        loss_train.append(loss)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:20&lt;00:00,  4.02s/it]\n\n\nRemark. Pytorch nn.Linear computes x @ Î¸.T + b. Hence, we take g.T before updating the parameter Î¸.\n\n\nCode\nplt.plot(np.array(loss_train).reshape(-1, 10).mean(1), label=\"train\")\nplt.xlabel(\"step\")\nplt.ylabel(\"loss\")\nplt.grid(alpha=0.3, linestyle=\"dashed\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nSample label predictions:\n\ndv = DataLoader(valid_dataset, batch_size=B, shuffle=False)\nx, y = next(iter(dv))\nout = model(x.reshape(B, -1)).argmax(1)\nacc = (out == y).float()\nprint(\"Batch acc:\", f\"{acc.sum().int()}/{B}\", f\"({acc.mean().item() * 100}%)\")\n\nBatch acc: 15/16 (93.75%)\n\n\n\n\nCode\nfig, ax = plt.subplots(4, 4)\nfor i in range(B):\n    a, b = divmod(i, 4)\n    ax[a, b].imshow(x[i].reshape(28, 28), cmap=\"Greys\")\n    color = \"black\" if out[i] == y[i] else \"red\"\n    ax[a, b].set_title(f\"{out[i]}\", color=color)\n    ax[a, b].axis(\"off\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nEvals. The practical goal of training is not actually to minimize \\mathcal{L}_\\mathcal{D}(\\Theta). But to minimize the loss for samples outside of the training dataset. That is, the model should be accurate on test data. If the model does not overfit and the test distribution does not drift too far from the training distribution, then we should be good.\n\ntot = 0\nacc = 0\nfor x, y in dv:\n    out = model(x.reshape(B, -1)).argmax(1)\n    correct = (out == y).float()\n    tot += len(y)\n    acc += correct.sum()\n\nprint(f\"Test acc: {acc / tot * 100:.2f}%\")\n\nTest acc: 92.31%",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#appendix-model-complexity",
    "href": "topics/deep/01.html#appendix-model-complexity",
    "title": "Softmax Regression",
    "section": "Appendix: Model complexity",
    "text": "Appendix: Model complexity\nSince SGD relies on a stochastic process, the performance of the resulting trained model varies. How are we sure that we arenâ€™t just lucky for this particular run / random seed? What is the variance of the trained model performance over multiple runs? Is there a way to control this? This issue is at the core of learning theory and precisely what the Bias-Variance Tradeoff addresses.\nIn practice, the crucial parameter to control is model complexity. Here model capacity or complexity is a measure of how complicated a pattern or relationship a model architecture can express. Let f be the true function that underlies the task. If model capacity is sufficiently large, the model class \\mathcal{F} = \\{f_{\\Theta} \\mid \\Theta \\in \\mathbb{R}^d \\} contains an approximation \\hat{f} \\in \\mathcal{F} such that \\| f - \\hat{f} \\| &lt; \\epsilon for a small enough \\epsilon &gt; 0.\nThe capacity of a model class can be controlled, for example, by the number of learnable parameters in practical architectures. It can also be constrained directly by applying regularization or certain prior knowledge such as invariances. This biases the model towards certain solutions, so these constraints are sometimes referred to as inductive biases â€” such knowledge is bias in the sense that it makes some solutions more likely, and others less likely. The tradeoff is that the model are steered to biased solutions more efficiently.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "topics/deep/01.html#appendix-maximum-likelihood-estimation-mle",
    "href": "topics/deep/01.html#appendix-maximum-likelihood-estimation-mle",
    "title": "Softmax Regression",
    "section": "Appendix: Maximum likelihood estimation (MLE)",
    "text": "Appendix: Maximum likelihood estimation (MLE)\nWe derive a loss function based on the principle of maximum likelihood estimation (MLE), i.e.Â finding optimal parameters \\hat{\\Theta} such that the dataset is assigned the highest probability under h_{\\hat{\\Theta}}. Consider a parametric model of the target denoted by p_{\\Theta}(y \\mid \\mathbf{x}). The likelihood of the i.i.d. sample \\mathcal{D} = (\\mathbf{x}_i, y_i)_{i=1}^N can be defined as\n\n\\begin{aligned}\n{L}(\\Theta)\n&= \\prod_{i=1}^N p_{\\Theta}(\\mathbf{x}_i, y_i) \\\\\n&= {\\prod_{i=1}^N {p_{\\Theta}(y_i \\mid \\mathbf{x}_i)}} \\cdot p_{\\Theta}(\\mathbf{x}_i).\n\\end{aligned}\n\nThis is proportional to the probability assigned by the parametric model with parameters \\Theta on the sample \\mathcal{D}. The i.i.d. assumption is important: maximizing the likelihood results in a model that focuses more on inputs that are better represented in the sample, i.e.Â more probable since they are sampled more. This makes sense, but should always be kept in mind, especially if we are concerned with getting accurate inferences for underrepresented segments of the dataset.\nGoing back, probabilities are small numbers in [0, 1], so applying the logarithm to convert the large product to a sum is a good idea. Moreover, it doesnâ€™t affect our optimization objective since \\log is monotonic. Then,\n\n\\begin{aligned}\n\\log {L}(\\Theta)\n&= \\sum_{i=1}^N \\log p_{\\Theta}(y_i \\mid \\mathbf{x}_i) + \\sum_{i=1}^N \\log p_{\\Theta}(\\mathbf{x}_i).\n\\end{aligned}\n\nMLE then maximizes the log-likelihood with respect to the parameters \\Theta to get a model that makes training data more probable. The latter term is independent of \\Theta, hence can be dropped. It is customary in machine learning to convert this to a minimization problem. Finally, we multiply by \\frac{1}{N} to scale the sum with the number of examples which doesnâ€™t affect the solution. The following then becomes our optimization problem:\n\\boxed{\n\\hat{\\Theta} = \\underset{\\Theta}{\\text{argmin}}\\, -\\frac{1}{N}\\sum_{i=1}^N \\log p_{\\Theta}(y_i \\mid \\mathbf{x}_i).\n}\n\nHence, MLE is equivalent to minimizing cross-entropy by writing\np_{\\Theta}(y \\mid \\mathbf{x}) = \\text{Softmax}(h_\\Theta(\\mathbf{x}))_y.",
    "crumbs": [
      "Deep Learning",
      "Softmax Regression"
    ]
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "AI notebooks",
    "section": "",
    "text": "Jupyter notebooks containing notes and implementation of AI models, algorithms, & applications.\n\n\n\nThe venv used to run the notebooks can be re-created easily using uv:\nmake venv\nYou can also install requirements via pip entirely skipping uv:\nmake requirements\npip install -r requirements.txt\npip install -e .\n\n\n\n\n\n\nTip\n\n\n\nSee here where we setup a remote environment from scratch.\n\n\n\n\n\n\nThe notebooks for each topic can be found in separate folders in the /topics directory:\n\n\n\nTopic\nFolder\nPrimary Reference(s)\n\n\n\n\nDeep Learning\n/deep\nCMU 10-414/714: Deep Learning Systems (Fall 2022)"
  },
  {
    "objectID": "README.html#venv",
    "href": "README.html#venv",
    "title": "AI notebooks",
    "section": "",
    "text": "The venv used to run the notebooks can be re-created easily using uv:\nmake venv\nYou can also install requirements via pip entirely skipping uv:\nmake requirements\npip install -r requirements.txt\npip install -e .\n\n\n\n\n\n\nTip\n\n\n\nSee here where we setup a remote environment from scratch."
  },
  {
    "objectID": "README.html#the-notebooks",
    "href": "README.html#the-notebooks",
    "title": "AI notebooks",
    "section": "",
    "text": "The notebooks for each topic can be found in separate folders in the /topics directory:\n\n\n\nTopic\nFolder\nPrimary Reference(s)\n\n\n\n\nDeep Learning\n/deep\nCMU 10-414/714: Deep Learning Systems (Fall 2022)"
  },
  {
    "objectID": "topics/deep/02.html",
    "href": "topics/deep/02.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Recall that a hypothesis function (i.e.Â a choice of architecture) h\\colon \\mathbb{R}^d \\to \\mathbb{R}^K maps input data to desired outputs. Initially, we used a linear hypothesis class h_\\Theta(\\mathbf{x}) = \\Theta^\\top \\mathbf{x} where \\Theta \\in \\mathbb{R}^{d \\times K}. This forms K linear functions of the input and predicts the class with the largest value. This turns out to be equivalent to partitioning the input space into K linear convex regions corresponding to each class.\nRemark. In \\mathbb{R}^2 with 3 classes, we have 2 inequality constraints \\theta_1^\\top \\mathbf{x} \\geq \\theta_2^\\top \\mathbf{x} and \\theta_1^\\top \\mathbf{x} \\geq \\theta_3^\\top \\mathbf{x}. This resuts in a convex polyhedron which is the intersection of two linear half-spaces. Similarly, for the other two. It can be shown that the interior of these subsets are disjoint and the union of the three cover all of \\mathbb{R}^2.\nQ. What about data that are not linearly separable? We want some way to separate these points via a nonlinear set of class boundaries.\nimport math\nimport numpy as np\n\nN = 100\nr_eps = 0.3\n\ndef circle_data(radius: float, r_eps=r_eps, num_points=N):\n    r0 = radius\n    t = 2 * math.pi * np.random.random(N)\n    r = r0 + r_eps * np.random.randn(N)\n    x, y = r * np.cos(t), r * np.sin(t)\n    return x, y\n\nx0, y0 = circle_data(3)\nx1, y1 = circle_data(5)\nx2, y2 = circle_data(1)\nCode\n%config InlineBackend.figure_formats = [\"svg\"] \nimport matplotlib.pyplot as plt\n\nplt.scatter(x0, y0, edgecolor=\"k\")\nplt.scatter(x1, y1, marker=\"*\", edgecolor=\"k\", s=100)\nplt.scatter(x2, y2, marker=\"s\", edgecolor=\"k\")\nplt.axis(\"equal\");",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#nonlinear-features",
    "href": "topics/deep/02.html#nonlinear-features",
    "title": "Neural Networks",
    "section": "Nonlinear features",
    "text": "Nonlinear features\nOne idea: Apply a linear classifier to some (potentially higher-dimensional) features of the data:\nh_\\Theta(\\mathbf{x}) = \\Theta^\\top \\phi(\\mathbf{x})\nwhere \\Theta \\in \\mathbb{R}^{h \\times K} and \\phi\\colon \\mathbb{R}^d \\to \\mathbb{R}^h is a feature transformation.\nExample. For the above dataset, we can define \\phi(x, y) = (x, y, x^2 + y^2) (i.e.Â r^2) which makes the dataset separable in \\mathbb{R}^3:\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n\nax.scatter(x0, y0, x0 ** 2 + y0 ** 2, edgecolor=\"k\")\nax.scatter(x1, y1, x1 ** 2 + y1 ** 2, marker=\"*\", edgecolor=\"k\", s=100)\nax.scatter(x2, y2, x2 ** 2 + y2 ** 2, marker=\"s\", edgecolor=\"k\");\n\n\n\n\n\n\n\n\nQ. How do we create the features?\n\nManual feature engineering (see above).\nIn a way that \\phi itself is learned from data (i.e.Â \\phi parametric).\n\nNote that \\phi linear doesnâ€™t work since we just get a linear classifier. If \\phi(\\mathbf{x}) = \\Phi^\\top \\mathbf{x} where \\Phi \\in \\mathbb{R}^{d \\times h}, then h_\\Theta(\\mathbf{x})\n= \\Theta^\\top \\phi(\\mathbf{x}) = \\Theta^\\top \\Phi^\\top \\mathbf{x} = (\\Phi \\Theta)^\\top \\mathbf{x}. Thus, \\phi must be nonlinear. It turns out that applying a univariate nonlinear function \\sigma\\colon \\mathbb{R} \\to \\mathbb{R} called an activation function1 suffices to create rich hypothesis classes (see SectionÂ 5). Hence, we set\n\\phi(\\mathbf{x}) = \\sigma(\\Phi^\\top \\mathbf{x}).\nRemark. More precisely, we require \\sigma to have a good range of values and almost everywhere differentiability to be usable. Observe that we can feature transform features, i.e.Â compose feature transformations. This is the main idea behind deep networks as we will see below.",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#neural-networks",
    "href": "topics/deep/02.html#neural-networks",
    "title": "Neural Networks",
    "section": "Neural networks",
    "text": "Neural networks\nA neural network2 refers to a particular type of hypothesis class, consisting of multiple, parameterized differentiable functions (a.k.a. â€œlayersâ€) composed together in any manner to form the output. Since neural networks involve composing a lot of functions (sometimes hundreds), it is usually referred to as deep neural networks, although there is really no requirement on depth beyond being not linear.\n\nTwo-layer neural network\nThe simplest form of neural network is basically just the nonlinear features presented earlier:\n\n\\begin{aligned}\nh_\\Theta(\\mathbf{X}) = \\sigma ( \\mathbf{X} \\mathbf{W_1}) \\mathbf{W}_2\n\\end{aligned}\n\nwhere \\Theta = \\{ \\mathbf{W}_1 \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_2 \\in \\mathbb{R}^{h \\times K} \\} are called the trainable parameters of the network and \\sigma\\colon \\mathbb{R} \\to \\mathbb{R} is the activation function that is applied elementwise to each vector. A commonly used one is ReLU defined as \\sigma(z) = \\max(0, z):\n\nz = np.linspace(-10, 10, 1000)\nplt.figure(figsize=(5, 2))\nplt.plot(z, np.clip(z, a_min=0, a_max=None), label=\"relu(z)\", linewidth=2, color=\"k\")\nplt.grid(alpha=0.6, linestyle=\"dotted\"); plt.ylim(-1, 11); plt.legend();\n\n\n\n\n\n\n\n\n\n\nFully-connected deep networks\nObserve that for the 2-layer network, we have |\\Theta| = 2. A more generic form is the L-layer neural network, sometimes called a multi-layer perceptron (MLP), feedforward network (FFN), or fully-connected network (FC) written in batch form as:\n\nh_\\Theta(\\mathbf{X}) = \\sigma(\\sigma(\\ldots \\sigma(\\mathbf{X}\\mathbf{W}_1)\\mathbf{W}_2 \\ldots) \\mathbf{W}_{L-1}) \\mathbf{W}_L\n\nor\n\n\\begin{aligned}\n\\mathbf{Z}_0 &= \\mathbf{X} \\\\\n\\mathbf{Z}_{i} &= \\sigma_i (\\mathbf{Z}_{i-1} \\mathbf{W}_i), \\quad \\forall i = 1, \\ldots, L \\\\\nh_\\Theta(\\mathbf{X}) &= \\mathbf{Z}_{L}\n\\end{aligned}\n\nwhere \\mathbf{Z}_i \\in \\mathbb{R}^{N \\times d_i} and \\mathbf{W}_i \\in \\mathbb{R}^{d_{i-1} \\times d_{i}}, d_0 = d and d_{L} = K, and \\sigma_i: \\mathbb{R} \\to \\mathbb{R} are applied elementwise. The weights of the network is given by \\Theta = \\{ \\mathbf{W}_1, \\ldots, \\mathbf{W}_L \\}, so that |\\Theta| = L. Also, we typically set \\sigma_L = \\text{Id} for the output layer.\nRemark. Again a bias term can be added. But in theoretical analysis we can just think there is an extra column containing ones to simplify the computation. The index i = 1, \\ldots, L corresponds to the number of layers applied to the input. In particular, the output layer is indexed L since we apply L transformations to the input indexed 0.\nExample. Shapes for a 3-layer neural network which classifies inputs in \\mathbb{R}^{16} into 10 classes:\n\n\n\n\ni\nd_{i-1}\nd_{i}\nPyTorch object\n\n\n\n\n1\n16\n32\nLinear(16, 32)\n\n\n2\n32\n64\nLinear(32, 64)\n\n\n3\n64\n10\nLinear(64, 10)",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#backpropagation",
    "href": "topics/deep/02.html#backpropagation",
    "title": "Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\nRecall that to train the linear function via softmax regression and SGD, we had to calculate the gradients across cross-entropy and the matrix product. For a deep neural network, we need to calculate the gradient for each \\mathbf{W}_i \\in \\Theta, i.e.Â across each layer i = 1, 2, \\ldots, L. Since the gradients are calculated from the loss to the inputs, this part of the computation is called backward pass. The algorithm for caching intermediate results and accumulating the gradients is collectively called backpropagation (BP) or simply â€œbackpropâ€.\nThe best way to understand and calculate backprop is to view neural nets as computational graphs with certain defined operations (e.g.Â entire layers, or lower-level tensor operations):\n\n\nCode\nfrom graphviz import Digraph\n\ndot = Digraph(format=\"png\")\ndot.attr(rankdir=\"LR\")\ndot.node_attr.update(shape=\"box\", style=\"rounded,filled\", fontsize=\"10\", fontname=\"Helvetica\")\ndot.node(\"X\", \"X\", fillcolor=\"lightgray\")\ndot.node(\"W1\", \"Wâ‚\", fillcolor=\"lightyellow\")\ndot.node(\"Z1\", \"Zâ‚ = Ïƒ(XWâ‚)\", fillcolor=\"lightgreen\")\ndot.node(\"W2\", \"Wâ‚‚\", fillcolor=\"lightyellow\")\ndot.node(\"Z2\", \"Zâ‚‚ = Zâ‚Wâ‚‚\", fillcolor=\"lightgreen\")\ndot.node(\"loss\", \"CE loss\", fillcolor=\"lightblue\")\ndot.node(\"Y\", \"Y\", fillcolor=\"lightgrey\")\n\ndot.edges([(\"X\", \"Z1\"), (\"W1\", \"Z1\"), (\"Z1\", \"Z2\"), (\"W2\", \"Z2\"), (\"Z2\", \"loss\"), (\"Y\", \"loss\")])\ndot\n\n\n\n\n\n\n\n\nFigureÂ 1: A computational graph of a two-layer neural network with softmax regression.\n\n\n\n\n\n\nGradients of a 2-layer NN\nConsider a two-layer neural network for softmax regression. Our goal is to find \\nabla_{\\mathbf{W}_i} \\mathcal{L}(h_\\Theta(\\mathbf{X}), \\mathbf{y}) for i = 1, 2. Letâ€™s write this as \\mathbf{Z}_1 = \\sigma(\\mathbf{X} \\mathbf{W}_1) and \\mathbf{Z}_2 = \\mathbf{Z_1}\\mathbf{W}_2. First, we branch towards the linear dependencies of \\mathbf{Z}_2:\n\n\\boxed{\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2}\n&= \\frac{1}{B} (\\hat{\\mathbf{Y}} - \\mathbf{Y}) \\\\[0.75em]\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_2}\n&=\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2}\n\\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{W}_2}\n=\n\\frac{1}{B}\\mathbf{Z}_1^\\top (\\hat{\\mathbf{Y}} - \\mathbf{Y}) \\\\[0.75em]\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_1} &=\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_2}\n\\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{Z}_1} =\n\\frac{1}{B}\n(\\hat{\\mathbf{Y}} - \\mathbf{Y})\n\\mathbf{W}_2^\\top.\n\\end{aligned}\n}\n\nFor the second equation, LHS has (d_1, K), while RHS has (d_1, B) \\times (B, K). OK. The last formula is obtained by considering a single instance: {\\partial z_2^{j}}/{\\partial z_1^{i}} = w^{ij}_2 = (w^\\top_2)^{ji}. This has the expected shape (B, K) \\times (K, d_1) = (B, d_1).\nNext, we calculate the gradient across \\sigma. Here we are taking the derivative of d_1 functions with respect to the weight tensor of shape (d, d_1). Let \\mathbf{U}_1 = \\mathbf{X}\\mathbf{W}_1, so that \\mathbf{Z}_1 = \\sigma(\\mathbf{U}_1). Then,\n\n\\begin{align*}\n\\frac{\\partial {z}_1^j}{\\partial w_1^{ik}}\n&= \\sum_l\n\\frac{\\partial z_1^j}{\\partial u_1^l}\n\\frac{\\partial u_1^l}{\\partial w_1^{ik}}  \\\\\n&= \\sum_l \\delta^{jl} \\sigma^\\prime(u_1^l) \\, x^i \\delta^{lk} = \\delta^{jk} \\sigma^\\prime(u_1^j) \\, x^i.\n\\end{align*}\n\\tag{1}\nHere \\delta refers to the Kronecker delta. The two Kronecker delta â€œcontractedâ€ into one. The effect of \\delta^{jk} is that \\sigma^\\prime(u^j) essentially multiplies element-wise to the incoming gradient. Moreover, we aggregate the contribution of the batch instances by multiplying \\mathbf{X}^\\top. Thus, in batch form:\n\n\\boxed{\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_1}\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_1} \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1} \\\\[0.7em]\n&= \\mathbf{X}^\\top \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}_1} \\odot \\sigma^\\prime(\\mathbf{X} \\mathbf{W}_1) \\right) \\\\[0.7em]\n&= \\frac{1}{B} \\mathbf{X}^\\top \\Big( [(\\hat{\\mathbf{Y}} - \\mathbf{Y})\n\\mathbf{W}_2^\\top ] \\odot \\sigma^\\prime(\\mathbf{X} \\mathbf{W}_1) \\Big).\n\\end{aligned}\n}\n\n\n\nGraph visualization\nBackward dependence can be inspected using the torchviz library. The graph shows the forward tensors being stored (\\color{orange}\\blacksquare) during forward pass to compute the gradients. The weights (\\color{cyan}\\blacksquare) are instances of leaf tensors, i.e.Â the outermost nodes with no parent nodes. Hence, backward iteration stops at the leaf nodes. It follows that calculating gradients require:\n\ntwice the memory of forward pass due to caching\nsame time complexity \\mathcal{O}(S) where S is the network size.\n\n\nfrom torchviz import make_dot\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nmodel = nn.Sequential(\n    nn.Linear(16, 32, bias=False),\n    nn.ReLU(),\n    nn.Linear(32, 10, bias=False)\n)\n\nB = 8\nx = torch.randn(B, 16)\nz = model(x)\ny = torch.randint(0, 10, size=(B,))\nloss = F.cross_entropy(z, y)\n\nmake_dot(loss.mean(), params=dict(model.named_parameters()), show_saved=True)\n\n\n\n\n\n\n\n\n\n\nGeneral formula for L-layers\nFor an L-layer network, we can traverse along the \\mathbf{Z}_iâ€™s to find \\partial{\\mathcal{L}} / \\partial{\\mathbf{Z_i}} by taking the product with the local gradients \\partial{\\mathbf{Z}_{i}}/\\partial{\\mathbf{Z}_{i-1}}. This can be thought of as traversing the main trunk of the network (FigureÂ 1). Meanwhile, the gradient for the weights (leaf tensors) can be calculated using the incoming gradients and \\partial{\\mathbf{Z}_{i}}/\\partial{\\mathbf{W}_{i}} (see EquationÂ 1). Calculating \\partial{\\mathbf{Z}_{i}}/\\partial{\\mathbf{Z}_{i-1}} can be done similarly as follows. Observe that indices check out:\n\n\\begin{aligned}\n\\frac{\\partial {z}_i^j}{\\partial {z}_{i-1}^k}\n&= \\sum_l\n\\frac{\\partial {z}_i^j}{\\partial u_i^l}\n\\frac{\\partial u_i^l}{\\partial {z}_{i-1}^k}  \\\\\n&= \\sum_l \\delta^{jl} \\sigma^\\prime(u_i^l) \\, w_{i}^{kl} = \\sigma^\\prime(u_i^j) \\, ({w^\\top})^{jk}.\n\\end{aligned}\n\nLet \\mathbf{G}_{i} = \\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathbf{Z}_{i}}} and \\boldsymbol{\\varsigma}_i = \\sigma_i^\\prime(\\mathbf{Z}_{i-1} \\mathbf{W}_i) both of shape (B, d_{i}). For i = L, \\ldots, 1:\n\n\\boxed{\n\\begin{aligned}\n\\mathbf{G}_{L} &= \\frac{1}{B}(\\hat{\\mathbf{Y}} - \\mathbf{Y}) \\quad \\quad \\;\\text{(cross-entropy)} \\\\\n\\mathbf{G}_{i-1} &=\n\\mathbf{G}_{i}\n\\frac{\\partial \\mathbf{Z}_{i}}{\\partial \\mathbf{Z}_{i-1}}\n= [\\mathbf{G}_{i} \\odot \\boldsymbol{\\varsigma}_i ]\\mathbf{W}_i^\\top \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{i}}\n&= \\mathbf{G}_{i}\n\\frac{\\partial \\mathbf{Z}_{i}}{\\partial \\mathbf{W}_{i}}\n= \\mathbf{Z}_{i-1}^\\top[\\mathbf{G}_{i} \\odot \\boldsymbol{\\varsigma}_i]\n\\end{aligned}\n}\n\nThis looks very compact. You can verify that the shapes are correct and that it is consistent with the equations for a two-layer network with \\sigma_2 = \\text{Id}. The formulas essentially describe the flow of gradients from the loss to the input. However, each update relies only on the gradients from the next layer.\nRemark. The gradients are modulated by \\sigma_i^\\prime and then multiplied by the weights. This is analogous to forward pass, but involves only basic operations (\\odot, MATMUL). Also, the equations have the form loss_grad x local_grad where the loss gradient \\mathbf{G}_{i} from the next layer is, in a sense, â€œglobalâ€ in contrast to the â€œlocalâ€ gradients between adjacent nodes. This pattern is quite general as we will see in the next chapter.",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#appendix-gradient-check-using-pytorch",
    "href": "topics/deep/02.html#appendix-gradient-check-using-pytorch",
    "title": "Neural Networks",
    "section": "Appendix: Gradient check using PyTorch",
    "text": "Appendix: Gradient check using PyTorch\nCalculating manually the gradients of a 3-layer neural network:\n\n# network parameters\nw1 = torch.randn(16, 64, requires_grad=True)\nw2 = torch.randn(64, 32, requires_grad=True)\nw3 = torch.randn(32, 10, requires_grad=True)\n\n# forward pass\nB  = 8\nx  = torch.randn(B, 16)\nz1 = torch.relu(x @ w1)\nz2 = torch.relu(z1 @ w2)\nz3 = z2 @ w3\n\nfor u in [w1, w2, w3, z1, z2, z3]:\n    u.retain_grad()\n\nh = z3\ny = torch.randint(0, 10, size=(B,))\nloss = F.cross_entropy(h, y)\nloss.backward()\n\nBackward pass:\n\n# walking down the trunk of the network\ng3 = (F.softmax(h, dim=1) - F.one_hot(y, num_classes=10)) / B\ng2 = g3 @ w3.T\ng1 = (g2 * ((z1 @ w2) &gt; 0).int()) @ w2.T\n\n# branching to the leaf tensors (weights)\nz0 = x\ndw3 = z2.T @ g3\ndw2 = z1.T @ (g2 * ((z1 @ w2) &gt; 0).int())\ndw1 = z0.T @ (g1 * ((z0 @ w1) &gt; 0).int())\n\nNote that shapes are equal, otherwise we canâ€™t subtract:\n\ndef compare(name, dt, t):\n    exact  = torch.all(dt == t.grad).item()\n    approx = torch.allclose(dt, t.grad, rtol=1e-5)\n    max_diff = (dt - t.grad).abs().max().item()\n    print(f'{name:&lt;3s} | exact: {str(exact):5s} | approx: {str(approx):5s} | max_diff: {max_diff:.2e}')\n    return max_diff\n\nerrors = []\nerrors.append(compare(\"z3\", g3, z3))\nerrors.append(compare(\"z2\", g2, z2))\nerrors.append(compare(\"z1\", g1, z1))\nerrors.append(compare(\"w1\", dw1, w1))\nerrors.append(compare(\"w2\", dw2, w2))\nerrors.append(compare(\"w3\", dw3, w3))\nassert max(errors) &lt; 1e-6, \"Gradients do not match!\"\n\nz3  | exact: False | approx: True  | max_diff: 3.64e-12\nz2  | exact: True  | approx: True  | max_diff: 0.00e+00\nz1  | exact: True  | approx: True  | max_diff: 0.00e+00\nw1  | exact: True  | approx: True  | max_diff: 0.00e+00\nw2  | exact: True  | approx: True  | max_diff: 0.00e+00\nw3  | exact: False | approx: True  | max_diff: 9.09e-13",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#sec-univapprox",
    "href": "topics/deep/02.html#sec-univapprox",
    "title": "Neural Networks",
    "section": "Appendix: Universal approximation",
    "text": "Appendix: Universal approximation\n[Cybenko 1989]. It turns out that any continuous map f\\colon \\Omega \\subset \\mathbb{R}^d \\to \\mathbb{R}^m defined on a compact set \\Omega can be approximated by a 1-layer (wide) fully-connected network. Continuity on a compact domain is a reasonable assumption about a ground truth function that we assume exists.\nDemo. Approximating a one-dimensional curve with a ReLU network:\n\nimport torch\n\n# Ground truth\nx = torch.linspace(-2 * torch.pi, 2 * torch.pi, 1000)\ny = torch.sin(x) + 0.3 * x\n\n# Get sorted sample. Shifted for demo\nB = sorted(torch.randint(30, 970, size=(24,)))\nxs = x[B,]\nys = y[B,]\n\n# ReLU approximation\nz = torch.zeros(1000,) + ys[0]\nfor i in range(len(xs) - 1):\n    if torch.isclose(xs[i + 1], xs[i]):\n        m = torch.tensor(0.0)\n    else:\n        m = (ys[i+1] - ys[i]) / (xs[i+1] - xs[i])\n    z += m * (torch.relu(x - xs[i]) - torch.relu(x - xs[i+1]))\n\nNOTE: This only works for target f with compact domainÂ [a,Â b] consistent with the theorem.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(7, 3))\nax[0].scatter(xs, ys, facecolor=\"none\", s=12, edgecolor=\"k\", zorder=3, label=\"data\")\nax[0].plot(x, y, color=\"C1\", label=\"f\")\nax[0].set_xlabel(\"x\")\nax[0].set_ylabel(\"y\")\nax[0].legend(loc=\"upper left\")\n\nax[1].scatter(xs, ys, facecolor=\"none\", s=12, edgecolor=\"k\", zorder=4, label=\"data\")\nax[1].plot(x, z, color=\"C0\", label=f\"relu approx. (B={len(B)})\", zorder=3)\nax[1].plot(x, y, color=\"C1\")\nax[1].set_xlabel(\"x\")\nax[1].set_ylabel(\"y\")\nax[1].legend(loc=\"lower right\", fontsize=7.5)\nfig.tight_layout();",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#appendix-linearly-separable-features",
    "href": "topics/deep/02.html#appendix-linearly-separable-features",
    "title": "Neural Networks",
    "section": "Appendix: Linearly separable features",
    "text": "Appendix: Linearly separable features\nAn ideal classifier is that where the sequence of layers transform the input \\mathbf{x}_i \\in \\mathbb{R}^{d} into data points F_{\\Phi}(\\mathbf{x}_i) \\in \\mathbb{R}^{d_{L-1}} that is linearly separable. That is, we essentially extend linear classification to input that is not linearly separable. This explains why the final layer has no activation, i.e.Â we can think of the network f as\nf_{(\\Theta, \\Phi)} = h_{\\Theta} \\circ F_{\\Phi}\nwhere h_\\Theta \\colon \\mathbb{R}^{d_{L-1}} \\to \\mathbb{R}^K is a linear hypothesis, while the earlier L-1 layers are feature extractors that compose F_{\\Phi}.\nDemo. Generating a dataset in \\mathbb{R}^2. Our goal is to separate this with a plane in \\mathbb{R}^3.\n\nimport torch\ntorch.manual_seed(2)\n\ndef generate_data(M: int):\n    noise = lambda e: torch.randn(M, 2) * e\n    t = 2 * torch.pi * torch.rand(M, 1)\n    s = 2 * torch.pi * torch.rand(M, 1)\n\n    x0 = torch.cat([0.3 * torch.cos(s), 0.3 * torch.sin(s)], dim=1) + noise(0.2)\n    x1 = torch.cat([3.0 * torch.cos(t), 3.0 * torch.sin(t)], dim=1) + noise(0.3)\n    y0 = (torch.ones(M,) * 0).long()\n    y1 = (torch.ones(M,) * 1).long()\n\n    return x0, y0, x1, y1\n\n\nx0, y0, x1, y1 = generate_data(1500)\n\nLetâ€™s do a simple 2-layer neural net where the feature extractor maps to \\mathbb{R}^3:\n\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Linear(2, 3), nn.Tanh(),\n    nn.Linear(3, 2)\n)\n\nModel training:\n\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\noptim = torch.optim.SGD(model.parameters(), lr=0.01)\n\nx = torch.cat([x0, x1])\ny = torch.cat([y0, y1])\nhistory = {\"accs\": [], \"loss\": []}\nfor step in tqdm(range(15000)):\n    s = model(x)\n    loss = F.cross_entropy(s, y)\n    loss.backward()\n    optim.step()\n    optim.zero_grad()\n    history[\"loss\"].append(loss.item())\n    history[\"accs\"].append(100 * (y == torch.argmax(s, dim=1)).float().mean())\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [00:12&lt;00:00, 1191.20it/s]\n\n\n\n\nCode\nfig, ax1 = plt.subplots(figsize=(8, 3))\nax2 = ax1.twinx()\n\nax1.plot(history[\"loss\"], color=\"blue\", linewidth=2)\nax2.plot(history[\"accs\"], color=\"red\",  linewidth=2)\nax1.set_xlabel(\"step\")\nax1.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(3, 3))\nax1.grid(axis=\"both\", linestyle=\"dotted\", alpha=0.8)\n\nax1.set_ylabel(\"Batch loss\")\nax2.set_ylabel(\"Batch accs (%)\")\nax1.yaxis.label.set_color(\"blue\")\nax2.yaxis.label.set_color(\"red\");\n\n\n\n\n\n\n\n\n\nObserve that accuracy trend does not exactly match the steadily decreasing loss. This is expected since accuracy considers hard labels, while the loss is calculated with respect to soft probability distributions.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# transformations\nwith torch.no_grad():\n    linear_0 = model[0](x0)\n    linear_1 = model[0](x1)\n    linear_act_0 = model[1](model[0](x0))\n    linear_act_1 = model[1](model[0](x1))\n\n    # separating hyperplane (see above discussion, i.e. w &lt;- w1 - w0  == logistic reg)\n    h = 1\n    w, b = model[2].parameters()\n    w, b = (w[h] - w[h-1]), (b[h] - b[h-1])\n\n# plot\nfig = plt.figure(figsize=(8, 3))\nax0 = fig.add_subplot(131)\nax1 = fig.add_subplot(132, projection=\"3d\")\nax2 = fig.add_subplot(133, projection=\"3d\")\n\nax0.grid(alpha=0.8, linestyle=\"dotted\")\nax0.set_axisbelow(True)\nax0.scatter(x0[:, 0], x0[:, 1], s=2.0, label=0, color=\"C0\")\nax0.scatter(x1[:, 0], x1[:, 1], s=2.0, label=1, color=\"C1\")\nax0.set_xlabel(\"$x_1$\")\nax0.set_ylabel(\"$x_2$\")\nax0.set_xlim(-1.5, 1.5)\nax0.set_ylim(-1.5, 1.5)\nax0.set_title(\"(a) input\")\nax0.legend()\nax0.set_facecolor(\"whitesmoke\")\nax0.axis(\"equal\")\n\nax1.scatter(linear_0[:, 0], linear_0[:, 1], linear_0[:, 2], s=3, label=0, color=\"C0\", alpha=0.8)\nax1.scatter(linear_1[:, 0], linear_1[:, 1], linear_1[:, 2], s=3, label=1, color=\"C1\", alpha=0.8)\nax1.set_xlabel(\"$x_1$\")\nax1.set_ylabel(\"$x_2$\")\nax1.set_zlabel(\"$x_3$\")\nax1.set_title(\"(b) linear\")\n\nax2.scatter(linear_act_0[:, 0], linear_act_0[:, 1], linear_act_0[:, 2], s=3, label=0, color=\"C0\")\nax2.scatter(linear_act_1[:, 0], linear_act_1[:, 1], linear_act_1[:, 2], s=3, label=1, color=\"C1\")\nax2.set_xlabel(\"$x_1$\")\nax2.set_ylabel(\"$x_2$\")\nax2.set_zlabel(\"$x_3$\")\nax2.set_title(\"(c) linear + tanh\")\n\n# Generate grid of points\nx_min = min(linear_act_1[:, 0].min(), linear_act_0[:, 0].min())\nx_max = max(linear_act_1[:, 0].max(), linear_act_0[:, 0].max())\ny_min = min(linear_act_1[:, 1].min(), linear_act_0[:, 1].min())\ny_max = max(linear_act_1[:, 1].max(), linear_act_0[:, 1].max())\na, b, c, d = w[0], w[1], w[2], b\nx = np.linspace(x_min, x_max, 50)\ny = np.linspace(y_min, y_max, 50)\nX, Y = np.meshgrid(x, y)\nZ = (-a * X - b * Y - d) / c\n\n# Plot the hyperplane for the positive class\nax2.plot_surface(X, Y, Z, alpha=0.5, color=f\"C{h}\")\nfig.tight_layout();\n\n\n\n\n\n\n\n\n\nRemark. The last linear layer (the â€œlogitsâ€) defines the separating hyperplane.\nPredicting on \\mathbb{R}^2:\n\n# Create a grid of points\nN = 300\nx = np.linspace(-6, 6, N)\ny = np.linspace(-5, 5, N)\nX, Y = np.meshgrid(x, y)\n\n# Calculate p[1] for each point in grid\ninp = torch.tensor(list(zip(X, Y)), dtype=torch.float32).permute(0, 2, 1).reshape(-1, 2)\nout = F.softmax(model(inp), dim=1)\nZ = out[:, 1].reshape(300, 300).detach().numpy()\n\nThe plot below is obtained by applying the model on each point in the grid. The coloring essentially represents the model on test data \\mathbf{x} \\in \\mathbb{R}^2. Notice that we essentially have a nonlinear decision boundary:\n\n\nCode\n# create a color plot\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Define custom colormap\ncolors = [\"C0\", \"C1\"]\nn_bins = 100\ncm = LinearSegmentedColormap.from_list(name=\"\", colors=colors, N=n_bins)\n\nfig = plt.figure(figsize=(6, 4))\nplt.pcolormesh(X, Y, Z, shading=\"auto\", cmap=cm, rasterized=True)\nplt.scatter(x0[:, 0], x0[:, 1], s=10.0, label=0, color=\"C0\", edgecolor=\"black\")\nplt.scatter(x1[:, 0], x1[:, 1], s=10.0, label=1, color=\"C1\", edgecolor=\"black\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.gca().set_aspect(\"equal\");",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/02.html#footnotes",
    "href": "topics/deep/02.html#footnotes",
    "title": "Neural Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe term â€œactivationâ€ has roots in neuroscience where it represents the action potential firing in the cell.â†©ï¸Ž\nThe term stems from biological inspiration, but at this point, literally any hypothesis function of the type above is referred to as a neural network.â†©ï¸Ž",
    "crumbs": [
      "Deep Learning",
      "Neural Networks"
    ]
  },
  {
    "objectID": "topics/deep/04.html",
    "href": "topics/deep/04.html",
    "title": "Gradient-Based Optimization",
    "section": "",
    "text": "Typically, we take B = 8, 32, 64, 128, 256.\nExpectation. For fixed {\\Theta}, the estimated gradient is unbiased in the sense that\n\n\\begin{aligned}\n\\mathbb{E}\\left[\\frac{1}{B} \\sum_{j=1}^{B} \\nabla \\mathcal{L}_{\\mathcal{B}, j}({\\Theta})\\right]\n&= \\frac{1}{B} \\sum_{j=1}^{B} \\mathbb{E}\\left[\\nabla \\mathcal{L}_{\\mathcal{B}, j}({\\Theta})\\right] \\\\\n&\\approx \\frac{1}{B} \\sum_{j=1}^{B} \\left( \\frac{1}{N} \\sum_{i=1}^N \\nabla \\mathcal{L}_i({\\Theta}) \\right)\n=\n\\nabla \\mathcal{L}({\\Theta}).\n\\end{aligned}\n\nThe second equality holds because shuffling the dataset at the start of an epoch makes the j-th element of the mini-batch \\mathcal{B} be any element of the dataset with uniform probability.\nVariance. For the sake of tractability, letâ€™s assume that each mini-batch \\mathcal{B} is sampled with replacement, so that all samples are independent. This is an approximation of the SGD algorithm used in practice. Then, the variance of the gradient with uncorrelated mini-batches decreases linearly with increasing batch size B:\n\n\\mathbb{V}\\left(\\frac{1}{B} \\sum_{j=1}^{B} \\nabla \\mathcal{L}_{\\mathcal{B}, j}\\left({\\Theta}\\right)\\right)=\\frac{1}{B} \\,\\mathbb{V}\\Bigg(\\nabla \\mathcal{L}_{\\mathcal{B}, 1}\\left({\\Theta}\\right)\\Bigg).\n\nExample. SGD (B = 16 and B = 128) vs GD with N = 1000 on a non-convex loss surface. Observe that the SGD steps with larger batch size is less noisy:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Deep Learning",
      "Gradient-Based Optimization"
    ]
  },
  {
    "objectID": "topics/tooling/index.html",
    "href": "topics/tooling/index.html",
    "title": "Tooling",
    "section": "",
    "text": "Quick reference for things like environment & infrastructure setup to specific utilities.",
    "crumbs": [
      "Tooling"
    ]
  },
  {
    "objectID": "topics/tooling/index.html#contents",
    "href": "topics/tooling/index.html#contents",
    "title": "Tooling",
    "section": "Contents",
    "text": "Contents\n\n\n\nTopic\nDescription\n\n\n\n\nRunpod Setup\nSetting up SSH access to dev environments in Runpod.io and pod access to GitHub.",
    "crumbs": [
      "Tooling"
    ]
  }
]